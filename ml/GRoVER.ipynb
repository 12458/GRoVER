{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WmaV2AUrpb0"
      },
      "source": [
        "# Basic retreival"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybZe9FYnxZr-"
      },
      "source": [
        "planning\n",
        "\n",
        "1. pdf loader\n",
        "2. vector store (FAISS)\n",
        "3. generate answer with sources\n",
        "\n",
        "\n",
        "Further:\n",
        "- inline citation\n",
        "- conversational\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpbDRNVMrwr_",
        "outputId": "7a5890a5-1266-44bb-eb9d-e07e8443a71c"
      },
      "outputs": [],
      "source": [
        "# load credentials\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF2xeHFT1W7O",
        "outputId": "29cb2670-aea8-4e09-9f12-a0d1ebab11f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of document chunks: [Document(page_content='Enabling Large Language Models to Generate Text with Citations\\nTianyu Gao Howard Yen Jiatong Yu Danqi Chen\\nDepartment of Computer Science & Princeton Language and Intelligence\\nPrinceton University\\n{tianyug,hyen,jiatongy,danqic}@cs.princeton.edu\\nAbstract\\nLarge language models (LLMs) have emerged\\nas a widely-used tool for information seeking,\\nbut their generated outputs are prone to halluci-\\nnation. In this work, our aim is to allow LLMs\\nto generate text with citations , improving their\\nfactual correctness and verifiability. Existing\\nwork mainly relies on commercial search en-\\ngines and human evaluation, making it chal-\\nlenging to reproduce and compare different\\nmodeling approaches. We propose ALCE , the\\nfirst benchmark for Automatic LLMs’ Citation\\nEvaluation. ALCE collects a diverse set of\\nquestions and retrieval corpora and requires\\nbuilding end-to-end systems to retrieve sup-\\nporting evidence and generate answers with\\ncitations. We develop automatic metrics along\\nthree dimensions—fluency, correctness, and ci-\\ntation quality—and demonstrate their strong\\ncorrelation with human judgements. Our exper-\\niments with state-of-the-art LLMs and novel\\nprompting strategies show that current systems\\nhave considerable room for improvement—For\\nexample, on the ELI5 dataset, even the best\\nmodels lack complete citation support 50% of\\nthe time. Our analyses further highlight promis-\\ning future directions, including developing bet-\\nter retrievers, advancing long-context LLMs,\\nand improving the ability to synthesize infor-\\nmation from multiple sources.1\\n1 Introduction\\nLarge language models (LLMs; Brown et al., 2020;\\nOpenAI, 2023) have gained increasing popularity\\nas a tool for information seeking. While they gener-\\nate engaging and coherent responses, their outputs\\nare prone to hallucination and often contain fac-\\ntually incorrect information (Ji et al., 2023). This\\nmakes it harder for users to trust and verify LLM-\\ngenerated outputs without any supporting evidence.\\nIn this work, we study a new generation\\nparadigm for LLMs, in which we require LLMs\\n1Our code and data are available at https://github.\\ncom/princeton-nlp/ALCE .\\nWhen did the US break away from England?\\nThe US took the first step towards gaining independence from GB when it declared independence on July 2, 1776 (although the event is now commemorated on July 4, 1776, the date when the Declaration of Independence was oﬀicially adopted by Congress) [1][2]. The Treaty of Paris was later signed on September 3, 1783, formally separating the United States from the British Empire [3].QuestionModel outputLLMCorpusRetrieve[1] Decolonization of the Americas:  … in-dependence from GB on July 2, 1776 …\\n[3] American Revolu-tion: … The Treaty of Paris was signed September 3, 1783 ...\\n[2] Decolonization: …  It was oﬀicially adop-ted by Congress on July 4, 1776 ……Figure 1: The task setup of ALCE. Given a question,\\nthe system generates text while providing citing pas-\\nsages from a large retrieval corpus. Each statement may\\ncontain multiple citations (e.g., [1][2] ).\\nto provide citations to one or a few text passages\\nfor any statement they generate (Figure 1). Incor-\\nporating citations brings several benefits: (1) users\\ncan easily verify LLMs’ claims with the provided\\ncitations; (2) LLMs can generate text that faithfully\\nfollows cited passages, which has the promise to\\nimprove correctness and alleviate hallucination.\\nMultiple commercial systems have adopted this\\nparadigm: Bing Chat2and perplexity.ai3respond to\\nuser questions in natural language with references\\nto Web pages. Nakano et al. (2021); Menick et al.\\n(2022) share a similar motivation, but they mainly\\nexperiment with commercial search engines and\\nclosed-source models, making their results difficult\\nto evaluate. Retrieval-augmented LMs (Borgeaud\\net al., 2022; Izacard et al., 2022) incorporate re-\\ntrieved passages during both training and infer-\\nence, but do not guarantee faithfulness to retrieved', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 0}), Document(page_content='et al., 2022; Izacard et al., 2022) incorporate re-\\ntrieved passages during both training and infer-\\nence, but do not guarantee faithfulness to retrieved\\npassages or explicitly provide citations. Addition-\\nally, previous studies mostly rely on human eval-\\nuation (Nakano et al., 2021; Menick et al., 2022;\\nLiu et al., 2023), which is expensive and difficult to\\nreproduce. We argue that the absence of automated\\nevaluation hinders the advances of such systems.\\n2https://www.bing.com/new\\n3https://www.perplexity.aiarXiv:2305.14627v2  [cs.CL]  31 Oct 2023', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 0}), Document(page_content='Dataset Corpus (#passages) Question type Example\\nASQA Wikipedia (21M) Factoid (ambiguous) Q: When did the US break away from England?\\nA: The US declared independence on July 2, 1776 [1][2] ... The\\nTreaty of Paris was later signed on September 3, 1783 [3].\\nQAMPARI Wikipedia (21M) Factoid (list) Q: Which films have Gong Li as a member of their cast?\\nA: The Story of Qiu Ju [1], Farewell My Concubine [2], The\\nMonkey King 2 [3], Mulan [3], Saturday Fiction [3] ...\\nELI5 Sphere (899M) Why/How/What Q: How do student loans affect getting a mortgage?\\nA: Student loans can affect the debt to income ratio [1], which is\\na key factor in determining the amount that ... [2][3]\\nTable 1: The three datasets used in our ALCE benchmark. These datasets cover a wide range of question types and\\nthe corresponding corpora span from Wikipedia to Web-scale document collection.\\nWe present ALCE , the first reproducible bench-\\nmark for automatically evaluating LLMs’ gener-\\nations with citations. ALCE assumes a natural-\\nlanguage question and a retrieval corpus, and re-\\nquires building end-to-end systems to retrieve rel-\\nevant passages from the corpus, generate a re-\\nsponse to the question, and cite corresponding sup-\\nporting passages. We compile three datasets that\\ncover different types of questions and corpora—\\nASQA (Stelmakh et al., 2022), QAMPARI (Ru-\\nbin et al., 2022), and ELI5 (Fan et al., 2019)—as\\nshown in Table 1. Different from previous bench-\\nmarks (Lee et al., 2019; Bohnet et al., 2022), ALCE\\nevaluates long-text generation, focusing on auto-\\nmatically evaluating citation quality, and allows\\nciting multiple passages for individual statements.\\nWe design automatic evaluation methods in three\\ndimensions: fluency ,correctness , and citation\\nquality . Specifically, we use MAUVE (Pillutla\\net al., 2021) to measure fluency, propose tailored\\ncorrectness metrics for each dataset, and adopt a\\nnatural language inference (NLI) model (Honovich\\net al., 2022) to measure citation quality. We show-\\ncase how the three dimensions together contribute\\nto a robust evaluation, preventing systems from ex-\\nploiting shortcuts. Additionally, we conduct human\\nevaluation and demonstrate a strong correlation\\nwith our automatic metrics.\\nWe experiment on multiple systems with state-\\nof-the-art LLMs and retrievers and also propose\\nnovel prompting strategies to synthesize retrieved\\ntext into text generation. Although all systems are\\ncapable of providing fluent and coherent responses,\\nthere remains substantial room for improvement in\\nterms of correctness and citation quality: For exam-\\nple, on the ELI5 dataset, around 50% generations\\nof our ChatGPT and GPT-4 baselines are not fully\\nsupported by the cited passages. Additionally, we\\nfind that (1) a closed-book model (generating an-\\nswers without accessing any retrieved documents)with post-hoc citing achieves good correctness but\\nmuch worse citation quality; (2) although interac-\\ntive retrieval approaches (Yao et al., 2023; Schick\\net al., 2023) offer more flexibility in when/what\\nto retrieve, they do not improve the performance\\non this challenging benchmark; (3) summarizing\\nthe retrieved passages in a shorter text improves\\ncorrectness but not citation quality; (4) reranking\\nmultiple generations boosts citation quality mea-\\nsured by human evaluation; (5) incorporating more\\nretrieved passages in context does not help Chat-\\nGPT but improves GPT-4 performance.\\nOur extensive analyses highlight three major\\nchallenges of building LLMs to generate text with\\ncitations: (1) the retrieval quality is crucial to the\\nfinal performance and has substantial room for im-\\nprovement; (2) LLMs’ limited context window re-\\nstricts the number of passages they can incorporate;\\n(3) current LLMs struggle to synthesize multiple\\ndocuments in context without being distracted by\\nirrelevant ones, although better instruction tuning\\nbrings significant improvement. These challenges\\npose promising research directions for developing', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 1}), Document(page_content='documents in context without being distracted by\\nirrelevant ones, although better instruction tuning\\nbrings significant improvement. These challenges\\npose promising research directions for developing\\nbetter systems integrating retrieval and LLMs.\\n2 Task Setup and Datasets\\nOur task is formalized as follows: Given a query\\nqand a corpus of text passages D, the system\\nis required to return an output S, which consists\\nofnstatements s1, ...,sn, and each statement si\\ncites a list of passages Ci={ci,1, ci,2, . . .}4, where\\nci,j∈ D. In this work, we segment LLMs’ output\\ninto statements by sentence boundaries.5While\\nLLMs may include sentences that do not require a\\ncitation, such as “ I’m happy to help ”, we observe\\nthat almost all sentences that LLMs output provide\\n4In practice, we allow at most 3 citations for each state-\\nment as more citations usually do not help.\\n5QAMPARI requires a list as the answer, and we choose\\neach entity in the generated list as a statement.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 1}), Document(page_content='valuable information and require citations, similar\\nto findings in Liu et al. (2023). In this work, cita-\\ntions are enclosed by box brackets such as [1][2] .\\nWe divide the corpus Dinto 100-word passages\\nfollowing previous works on open-domain question\\nanswering (Karpukhin et al., 2020; Petroni et al.,\\n2021; Piktus et al., 2021), in contrast to commer-\\ncial systems like Bing Chat, which cite entire Web\\npages. We take 100-word passages because it is\\neasier for humans to verify, and allows for more\\nretrieved passages to fit in LLMs’ limited context.\\nWe choose QA datasets so that (1) they contain\\nfactual questions, in which references are impor-\\ntant; (2) questions require long-text answers that\\ncover multiple aspects; (3) answering the questions\\nrequires synthesizing multiple sources. We select\\nthree datasets (Table 1) and introduce them below.\\nSee §B for additional statistics.\\nASQA (Stelmakh et al., 2022) is a long-form fac-\\ntoid dataset. As shown in Figure 1, each question is\\nan ambiguous question from AmbigQA (Min et al.,\\n2020) that requires multiple short answers to cover\\ndifferent aspects, and the dataset provides a long-\\nform answer that covers all short answers. Since\\nmost questions can be answered by Wikipedia, we\\nuse the 2018-12-20 Wikipedia snapshot as D.\\nQAMPARI (Rubin et al., 2022) is a factoid QA\\ndataset constructed from Wikipedia, where the an-\\nswer is a list of entities that are drawn from differ-\\nent passages. Same as ASQA, we use the 2018-12-\\n20 Wikipedia as the corpus.\\nELI5 (Fan et al., 2019) is a long-form QA\\ndataset built on the Reddit forum “Explain Like I’m\\nFive”.6Most ELI5 questions are how/why/what\\nquestions that require long answers and multiple\\npassages as evidence. Due to the diverse topics dis-\\ncussed in the questions, we use Sphere (Piktus et al.,\\n2021)—a filtered version of Common Crawl7—as\\nthe corpus. The ELI5 dataset is widely used in\\nrelated work due to its challenging nature (Nakano\\net al., 2021; Menick et al., 2022; Liu et al., 2023).\\nWe randomly select 1,000 examples from the\\ndevelopment set of each dataset for ALCE. Our\\nbenchmark primarily assesses the citation capabili-\\nties of existing LLMs and does not provide training\\ndata, as there are no available examples that pro-\\nvide supervision for citations in these datasets.\\n6https://www.reddit.com/r/explainlikeimfive/\\n7https://commoncrawl.org . We also filter out any Web\\npages from Reddit.3 Automatic Evaluation\\nOur benchmark measures the following three di-\\nmensions of system responses:\\n•Fluency : whether the model’s generated text is\\nfluent and coherent.\\n•Correctness : whether the answer is accurate\\nand covers all aspects of interest.\\n•Citation quality : whether the answer is well\\nsupported by the cited passages and no irrelevant\\npassages are cited.\\nIn the following, we present automatic metrics for\\neach dimension and discuss why the combination\\nof the three metrics provides a robust evaluation.\\n3.1 Fluency\\nWe use MAUVE (Pillutla et al., 2021) to evaluate\\nthe fluency of the output (§C). We deploy MAUVE\\nfor ASQA and ELI5 and omit it for QAMPARI, as\\nQAMPARI only requires a list of short answers as\\nthe response and LLMs consistently adhere to the\\nformat in our experiments. As MAUVE is sensitive\\nto output length and text style, and most LLMs are\\ncapable of producing fluent text, we mainly employ\\nit as a sanity check as long as the MAUVE scores\\nare high enough.\\n3.2 Correctness\\nOur objective is to measure the informativeness and\\nutility of the generation to the question. Liu et al.\\n(2023) propose to directly evaluate perceived utility\\nby humans, a process difficult to automate. There-\\nfore, we use correctness—whether the response is\\naccurate compared to a ground truth answer—as a\\nproxy. Evaluating the correctness of long-form gen-\\neration is a challenging task (Krishna et al., 2021),\\nand we describe our strategy for each dataset be-\\nlow. Figure 2 illustrates the metrics and we include\\nadditional implementation details in §C.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 2}), Document(page_content='eration is a challenging task (Krishna et al., 2021),\\nand we describe our strategy for each dataset be-\\nlow. Figure 2 illustrates the metrics and we include\\nadditional implementation details in §C.\\nForASQA , we follow Stelmakh et al. (2022)\\nand calculate the recall of correct short answers by\\nchecking whether the short answers (provided by\\nthe dataset) are exact substrings of the generation\\n(exact match recall ; EM recall).\\nForQAMPARI , we follow Rubin et al. (2022)\\nand calculate the precision andrecall of the model\\nprediction, by checking the exact match to the gold\\nanswer list. We add one additional adjustment:\\nconsidering that users often want to know only a\\nfew example answers of the question, our evalua-\\ntion considers recall to be 100% if the prediction\\nincludes at least 5 correct answers ( recall-5 ).', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 2}), Document(page_content='ASQA \\n  Exact Match Recall\\nELI5 \\n  Claim Recall\\nWhen did the US break away from England?QuestionShort answers (from the dataset)July 2, 1776September 3, 1783July 4, 1776\\n… it declared independence on July 3, 1776 … The Treaty of Paris was signed on September 3, 1783 …Model output\\nHow do student loans aﬀect getting a mortgage?Question\\nThey do not appear in credit history and do not aﬀect debt to income ratio. Thus they do not aﬀect getting a mortgage.Model outputClaims (generated by text-davinci-003 based on gold answers)Student loan does not appear in credit history.Student loan can aﬀect the debt to income ratio.Debt to income ratio aﬀects mortgage applications.String exact matchRecall=33.3%\\nNLIClaim recall=33.3%\\nQAMPARI \\n  Precision, Recall\\nWhich films have Gong Li as a member of their cast?QuestionShort answers (from the dataset)The Story of Qiu JuSaturday FictionMulan\\nSaturday Fiction, The Great Wall, Mulan, Shadow.Model outputString exact matchRecall=66.6%Precision=50%Figure 2: Evaluation of correctness (details in §3.2).\\nUnlike ASQA and QAMPARI, the ELI5 dataset\\ndoes not provide short entity answers. Fan et al.\\n(2019) use ROUGE for evaluation, which does\\nnot reflect the correctness well (Krishna et al.,\\n2021; §A). Inspired by works in summariza-\\ntion evaluation (Zhang and Bansal, 2021; Kamoi\\net al., 2023; Wang et al., 2020), we use Instruct-\\nGPT (text-davinci-003 ; Ouyang et al., 2022)\\nto generate three “sub-claims”. Then we use\\nTRUE8(Honovich et al., 2022), a T5-11B (Raf-\\nfel et al., 2020) model fine-tuned on a collection of\\nnatural language inference (NLI) datasets, to check\\nwhether the model output entails the sub-claims\\n(claim recall ). TRUE targets factual correctness\\nand has been used by previous works in similar\\ncontext (Bohnet et al., 2022; Gao et al., 2023). We\\ndemonstrate that claim recall provides a more ac-\\ncurate measure of correctness than existing metrics\\n(more details in §A).\\n3.3 Citation Quality\\nWe evaluate citation qualities using two metrics: (1)\\ncitation recall , which determines if the output is en-\\ntirely supported by cited passages, and (2) citation\\nprecision , which identifies any irrelevant citations.\\nAlthough we prioritize citation recall as it entails\\na well-supported and truthful answer, enhancing\\nprecision is crucial for better user satisfaction, re-\\nducing the need for human review of extraneous\\n8https://huggingface.co/google/t5_xxl_true_\\nnli_mixture . Details in §C.\\n[1]        [2]             \\nFor this question, citation precision = 4 / 6 = 66%\\n{statement 1} [1][2]. {statement 2} [3].{statement 3} [2][4][5].    Model output\\n“entailment”Premise: passage [1][2]Hypothesis: {statement 1}NLImodel\\nCitation Recall {statement 2} [3]{statement 1} [1][2]{statement 3} [2][4][5]    For this question, citation recall = 2 / 3 = 66%\\nCitation Precision Detect “irrelevant” citation: one citation alone does not support the claim, and removing it does not affect other citations combined to support the claim.\\nWhen did the US break away from England?Question\\n{statement 1} [1][2]      : {statement 3} [2][4][5]      :                          [2]        [4][5]        [4]        [2][5]        [5]        [2][4][2] is “irrelevant’’. Precision = 0Precision = 1{statement 2} [3]If recall = 0, then precision = 0Recall = 1 if the concatenation of all cited passages fully supports the segment.We use an NLI model to determine “fully support”.\\nPrecision = 1Precision = 1Precision = 1[2]        [1]             Figure 3: Evaluation of citation quality (details in §3.3).\\nWe use an NLI model to verify whether a statement is\\nsupported by its citations.\\npassages. Figure 3 provides an illustrated example.\\nWe use the NLI model TRUE (Honovich et al.,\\n2022) again to automatically examine whether the\\ncited passages entail the model generation. We con-\\nduct human evaluation (§6) to demonstrate strong\\nhuman correlation of our metric.\\nCitation recall. We calculate the citation recall of\\neach statement (0 or 1) and average over all state-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 3}), Document(page_content='duct human evaluation (§6) to demonstrate strong\\nhuman correlation of our metric.\\nCitation recall. We calculate the citation recall of\\neach statement (0 or 1) and average over all state-\\nments in the model response. For each statement si,\\nits citation recall is 1if and only if there is at least\\none citation ( Ci̸=∅) and ϕ(concat (Ci), si) = 1 ,\\nwhere ϕ(premise ,hypothesis )is the NLI model\\nthat outputs 1if the premise entails the hypoth-\\nesis, and 0otherwise; concat (Ci)concatenates all\\npassages in Citogether (details in §C). The NLI\\nevaluation is in accordance with the attributable to\\nidentified sources (AIS) framework (Rashkin et al.,\\n2023): ϕ(concat (Ci), si) = 1 implies that siis true\\nbased solely on concat (Ci).\\nCitation precision. Our citation precision evalua-\\ntion detects citations that are irrelevant, but it does\\nnot require citing a minimal set. We follow this de-\\nsign because human writing often cites redundant\\nsources to enhance credibility; human readers may\\nalso appreciate multiple citations, especially when\\nit pertains to critical claims such as medical advice.\\nWe calculate the citation precision for each ci-\\ntation (0 or 1) and average over all citations in the', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 3}), Document(page_content='Instruction: Write an accurate, engaging, and\\nconcise answer for ...\\n<Retrieve for the question>\\nDocument [1](Title: American Decolonization)\\n...\\nDocument [2](Title: Decolonization) ...\\nDocument [3](Title: American Revolution) ...\\n...\\nQuestion: When did US break away from England?\\nAnswer: The United States took the first step\\ntowards gaining independence ... [1][2]. The\\nTreaty of Paris was later signed ... [3].\\nTable 2: An example of our VANILLA method. Differ-\\nent colors represent prompt ,model generation , and\\n<actions> . We also provide two in-context demonstra-\\ntions before the test example.\\nresponse. We first define if a citation is “irrele-\\nvant”. Intuitively, a citation ci,jis “irrelevant” if (a)\\nci,jitself cannot support siand (b) removing ci,j\\ndoes not affect the rest of the citations to support\\nsi. Formally, ci,jis “irrelevant” if and only if\\n(a)ϕ(ci,j, si) = 0 , AND\\n(b)ϕ(concat (Ci\\\\ {ci,j}), si) = 1 .\\nci,jhas a precision of 1 if sihas recall =1and\\nci,jis not irrelevant. For example (Figure 3),\\nwhen s3cites three references [2][4][5] and\\nrecall =1,[2]is “irrelevant” if ϕ([2], s3) = 0 and\\nϕ([4][5] , s3) = 1 . For condition (b) to work, we\\nset recall =1as a prerequisite for precision = 1.\\nNote that this algorithm overlooks the scenario\\nwhen one citation partially supports the statement.\\nWe discuss the details in §E.\\n3.4 ALCE is Robust to Shortcut Cases\\nWe showcase how the ALCE evaluation is robust\\nto two possible shortcuts in §D: (1) using the top-1\\nretrieved passage as the response and citing itself,\\nand (2) using the first two sentences of the top-1\\npassage. Both cases have almost-perfect citation\\nscores, but (1) has low fluency due to its unnaturally\\nlong length compared to human answers, and (2)\\nhas low correctness due to low coverage.\\n4 Modeling\\nIn this section, we discuss three major modeling\\ncomponents for an ALCE system—retrieval, syn-\\nthesis, and post-editing.\\n4.1 Retrieval\\nWe explore simple, off-the-shelf retrievers. We use\\ndense retrievers for Wikipedia, including GTR (Niet al., 2022) and DPR (Karpukhin et al., 2020);\\nwe use BM25 for Sphere. For each question, we\\nretrieve the top-100 passages.\\n4.2 Synthesis\\nWe focus on how to prompt an LLM to interact\\nwith the retriever, and synthesize and cite the ev-\\nidence (without fine-tuning internal parameters).\\nOne noteworthy challenge is that existing LLMs\\nall have limited context window and thus can only\\nfit a handful of passages.\\nVANILLA .We simply provide the model with the\\ntop-k9passages and instruct the model to cite ac-\\ncordingly (Table 2). We also use in-context learn-\\ning (Brown et al., 2020) and prepend two demon-\\nstrations. The complete instruction is in Table 23.\\nSUMM /SNIPPET .With a 4K context window, we\\ncan at most safely fit k= 5passages. As shown in\\nFigure 4, top-5 retrieved passages can only cover\\n56.8% percent of the answers in ASQA.\\nTo tackle this limitation, we propose to provide\\nsummaries orsnippets of passages instead of the\\nfull text (summaries are abstractive but snippets are\\nspans from passages). We acquire summaries and\\nsnippets by prompting ChatGPT with instructions\\n(prompts in Table 25 and 26).10Then we replace\\nall passages with summaries/snippets. Summaries\\nor snippets significantly reduce the passage length,\\nallowing for more passages to fit in: for ASQA,\\nthey reduce passage length by 6 ×on average.\\nThough SUMM /SNIPPET allows for more re-\\ntrieved passages, they are lossy compressions. To\\nalleviate this problem, we propose INTERACT , an\\ninteractive prompting scheme to allow the model to\\ncheck the full text of certain passages. At each step,\\nthe model can execute one of three actions: (1)\\n“Check: Document [1][2] ” to check the full text\\nof the corresponding documents; (2) “ Output: ” to\\noutput a statement of the answer; (3) “ End. ” to end\\nthe generation. §C provides more details.\\nINLINE SEARCH .The above methods all dis-\\nplay retrieval results at the beginning. In INLI-\\nNESEARCH , we allow LLMs to call “search” dur-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 4}), Document(page_content='the generation. §C provides more details.\\nINLINE SEARCH .The above methods all dis-\\nplay retrieval results at the beginning. In INLI-\\nNESEARCH , we allow LLMs to call “search” dur-\\ning the generation process (Yao et al., 2023; Press\\net al., 2022; Jiang et al., 2023). At each step, the\\nmodel can execute one of three actions: “ Search:\\n9We can fit at most k= 3 for models with 2K window\\nand at most k= 5for models with 4K context window.\\n10We also query ChatGPT whether the passage is relevant\\nto the question, and filter out passages that are “irrelevant”.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 4}), Document(page_content='Instruction: ...\\n<Retrieve for question “...”>\\nQuestion: When did US break away from England?\\nSearch: Declaration of Independence\\n<Search the query among the top-100 passages>\\nDocument [1](Title: ...) ...\\nOutput: The United States ... [1].\\n<Remove Document [1] from context>\\nSearch: Treaty of Paris\\n<Search the query among the top-100 passages>\\nDocument [3](Title: ...) ...\\nOutput: The Treaty of Paris ... [3].\\n<Remove Document [3] from context>\\nEnd.\\nTable 3: An example of I NLINE SEARCH .\\n{query} ” to search among the top-100 passages11\\nby using GTR; the “ Output ” and “End” actions are\\nthe same as INTERACT . For each “ Search ” action,\\nwe display the best retrieved passage in the context.\\nThe passage is removed after one action to save\\ncontext space. Table 3 shows an example.\\nCLOSED BOOK .We also add a simple closed-\\nbook baseline, where the model is only prompted\\nwith the instruction and the question, without any\\nretrieved passages provided. Consequently, this\\nvariant does not cite any evidences.\\n4.3 Post-editing\\nIn this section we discuss two strategies for refining\\nthe output to further improve its quality.\\nRERANK .We randomly sample nsample = 4 re-\\nsponses for each question, and select the best re-\\nsponse using the automatic citation recall score.\\nwe expect RERANK to improve the citation quality.\\nPOSTCITE.For each statement, we find the best\\nmatching passage among the top-100 retrieved pas-\\nsages using GTR and cite it. We combine this with\\nCLOSED BOOK in our experiments.\\n5 Experiments\\nWe describe experiment details in §C. We use Chat-\\nGPT (gpt-3.5-turbo-0301 ) with a 4K context\\nwindow for most main experiments and ablations.\\nWe also report results with ChatGPT-16K ( gpt-\\n3.5-turbo-16k-0613 ) and GPT-4 ( gpt-4-0613 ;\\n8K context window). For open-source models,\\nwe test LLaMA (Touvron et al., 2023a) and its\\ninstruction-tuned versions, including Alpaca (Taori\\net al., 2023), Vicuna (Chiang et al., 2023), and\\n11We do not search over the entire corpus because {query}\\nmay leave out certain context in the question and searching\\namong the already-retrieved passages gives better results.Fluency Correct. Citation\\n(MAUVE) (EM Rec.) Rec. Prec.\\nChatGPT\\nVANILLA (5-psg) 66.6 40.4 73.6 72.5\\nw/ R ERANK 77.0 40.2 84.8 81.6\\nSUMM (10-psg) 70.0 43.3 68.9 61.8\\nw/ I NTERACT 69.0 39.1 73.4 66.5\\nSNIPPET (10-psg) 69.8 41.4 65.3 57.4\\nINLINE SEARCH 58.7 32.4 58.3 58.2\\nCLOSED BOOK 52.7 38.3 26.7 26.7\\nGPT-4 (VANILLA prompting)\\nGPT-4 (5-psg) 67.1 41.3 68.5 75.6\\nGPT-4 (20-psg) 64.9 44.4 73.0 76.5\\nLLaMA (VANILLA prompting)\\nLLaMA-13B (3-psg) 68.4 26.9 10.6 15.4\\nVicuna-13B (3-psg) 82.6 31.9 51.1 50.1\\nChat-13B (5-psg) 72.4 35.2 38.4 39.4\\nChat-70B (5-psg) 88.3 41.5 62.9 61.3\\nTable 4: Experiments on ASQA. For CLOSED BOOK,\\nwe use POSTCITEto get citations. k-psg: putting top-\\nkpassages from the retrieval results into the context.\\nChat-13B and Chat-70B refer to LLaMA-2-Chat.\\nOasst (Köpf et al., 2023). They all have a 2K con-\\ntext window. We use short instructions for LLaMA\\n(Table 24) to save context budget. Additionally,\\nwe test LLaMA-2-Chat, which were also trained to\\nfollow instructions (Touvron et al., 2023b). These\\nmodels have a context window of 4K tokens, which\\nallows for 5 passages per question.\\n5.1 Main Results\\nWe present the main results on three datasets in\\nTable 4, 5, and 6 respectively (full results in §G.6).\\nWe first note that all models achieve good fluency\\nscores (except some models on ELI5 mainly due to\\ntheir longer generations). We summarize the main\\ntakeaways from the experiments below.\\nVANILLA achieves strong performance. Despite\\nits simplicity, VANILLA (putting retrieved passages\\nin context) achieves close-to-the-best performance\\namong all prompting strategies.\\nUsing summaries or snippets improves correct-\\nness. We see a universal trend that SUMM orSNIP-\\nPETimproves correctness, though on ASQA and\\nELI5, such an improvement comes at a cost of\\ncitation quality due to the lossy compression. Com-\\nbining INTERACT with SUMM /SNIPPET does not', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 5}), Document(page_content='PETimproves correctness, though on ASQA and\\nELI5, such an improvement comes at a cost of\\ncitation quality due to the lossy compression. Com-\\nbining INTERACT with SUMM /SNIPPET does not\\nbring improvement, and we hypothesize that check-\\ning the full passages offers limited benefit and cur-\\nrent LLMs are not proficient in an interactive usage.\\nRetrieving text on the fly does not improve per-\\nformance. All datasets show that VANILLA out-\\nperforms I NLINE SEARCH on citation quality (and', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 5}), Document(page_content='Correctness Citation\\nRec.-5 Prec. Rec. Prec.\\nChatGPT\\nVANILLA (5-psg) 20.8 20.8 20.5 20.9\\nw/ R ERANK 22.8 21.4 21.2 21.4\\nSUMM (10-psg) 23.6 21.2 23.6 25.7\\nSNIPPET (10-psg) 24.5 21.5 22.9 24.9\\nw/ I NTERACT 21.9 23.0 21.9 23.4\\nINLINE SEARCH 17.2 20.4 14.9 14.9\\nCLOSED BOOK 32.9 19.8 10.0 10.0\\nGPT-4 (VANILLA prompting)\\nGPT-4 (5-psg) 22.2 25.0 25.9 27.0\\nGPT-4 (20-psg) 29.6 26.2 27.4 28.5\\nLLaMA (VANILLA prompting)\\nLLaMA-13B (3-psg) 9.7 9.1 6.7 7.1\\nVicuna-13B (5-psg) 14.0 15.9 12.5 13.4\\nChat-13B (5-psg) 21.1 18.2 9.6 9.7\\nChat-70B (5-psg) 21.8 18.4 15.1 15.6\\nTable 5: Experiments on QAMPARI. “Rec.-5”: we set\\nthe recall to be 100% if the prediction includes at least\\n5 correct answers.\\non correctness for ASQA and ELI5). By manually\\nexamining the examples, we find that it is challeng-\\ning to ask detailed questions without seeing any\\npassages. To improve INLINE SEARCH , one may\\nneed to provide more context about the questions\\nin advance or encourage the model to call retrievers\\nwith more detailed and diverse queries.\\nRERANK boosts citation quality. We observe that\\nRERANK leads to consistent improvement in cita-\\ntion quality (on ASQA and ELI5). As the automatic\\nscores may be biased in RERANK , we also conduct\\nhuman evaluation (§6) and verify its effectiveness.\\nCLOSED BOOK +POSTCITEdelivers strong cor-\\nrectness but poor citation quality. CLOSED -\\nBOOK outperforms VANILLA in correctness on\\nELI5 and QAMPARI, and has only a 2% gap on\\nASQA. However, CLOSED BOOK cannot provide\\nany citation; when combined with POSTCITE, the\\ncitation quality remains inadequate. For instance,\\ncitation recall of CLOSED BOOK+POSTCITEis\\nlower than V ANILLA by 47% on ASQA.\\nTo understand why CLOSED BOOK achieves bet-\\nter correctness and why POSTCITEcannot deliver\\nsatisfying citation quality, we manually examine\\nmodel outputs and find that: (1) open-book mod-\\nels are easily distracted by irrelevant passages and\\ngenerate responses with lower correctness, a phe-\\nnomenon also observed by Shi et al. (2023); (2)\\nCLOSED BOOK often generates texts that are cor-\\nrect but not similar to any retrieved passages, mak-\\ning it difficult to match a citation post-hoc.Fluency Correct. Citation\\n(MAUVE) (Claim) Rec. Prec.\\nChatGPT\\nVANILLA (5-psg) 57.2 12.0 51.1 50.0\\nw/ R ERANK 56.1 11.4 69.3 67.8\\nSUMM (10-psg) 40.3 12.5 51.5 48.2\\nSNIPPET (10-psg) 62.9 14.3 50.4 45.0\\nw/ I NTERACT 68.0 13.3 47.8 45.0\\nINLINE SEARCH 49.7 13.4 45.6 43.7\\nCLOSED BOOK 32.6 18.6 15.5 15.5\\nGPT-4 (VANILLA prompting)\\nGPT-4 (5-psg) 38.4 14.2 44.0 50.1\\nGPT-4 (20-psg) 41.5 18.3 48.5 53.4\\nLLaMA (VANILLA prompting)\\nLLaMA-13B (3-psg) 50.0 3.9 3.1 5.3\\nVicuna-13B (3-psg) 58.2 10.0 15.6 19.6\\nChat-13B (5-psg) 34.7 13.4 17.3 15.8\\nChat-70B (5-psg) 38.6 12.8 38.3 37.9\\nTable 6: Experiments on ELI5. We use claim recall\\nfor the correctness evaluation. Chat-13B and Chat-70B\\nrefer to LLaMA-2-Chat.\\nGPT-4 brings limited improvement but is better\\nat using long context. We evaluate GPT-4 with\\nVANILLA and different numbers of passages (more\\nresults in §G.6). GPT-4 brings consistent (but lim-\\nited) improvement on correctness, but often at a\\ncost of citation quality. GPT-4 can also incorporate\\nmore passages due to its longer context window,\\nwhich boosts both correctness and citation qual-\\nity. On the contrary, including more passages with\\nChatGPT-16K does not improve the results (Ta-\\nble 7), suggesting that processing more passages\\nis non-trivial and GPT-4 is better at synthesizing\\ninformation from its long context than ChatGPT.\\n5.2 Comparison of Different LLMs\\nTable 7 compares different LLMs on ASQA\\nusing VANILLA (more results in §G.6). No-\\ntably, instruction-tuned models (Vicuna-13B and\\nLLaMA-2-Chat) outperform the original LLaMA\\nmodels in correctness and considerably enhance\\nthe citation quality. We observe that while the orig-\\ninal LLaMA models are able to copy facts from\\nthe context, they struggle with accurately citing the\\nsources or simply do not cite. Notably, the best\\nopen-source model, LLaMA-2-70B-Chat, achieves', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 6}), Document(page_content='inal LLaMA models are able to copy facts from\\nthe context, they struggle with accurately citing the\\nsources or simply do not cite. Notably, the best\\nopen-source model, LLaMA-2-70B-Chat, achieves\\ncomparable correctness score as the OpenAI mod-\\nels, but still lags behind in citation quality.\\n5.3 Retrieval Analysis\\nThe retrieval results play a crucial role to the cor-\\nrectness and the citation quality. Figure 4 presents\\nthe retrieval recall@ kwith different datasets and', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 6}), Document(page_content='520 50 100\\nk304050607080Recall@k\\n Vanilla (5-psg)OracleASQA\\nDPR\\nGTR\\n520 50 100\\nk0204060\\nVanilla (5-psg)OracleQAMPARI\\nDPR\\nGTR\\n520 50 100\\nk102030\\nVanilla (5-psg)OracleELI5\\nBM25Fluency Correct. Citation\\n(MAUVE) (EM) Rec. Prec.\\nChatGPT (VANILLA )\\nDPR (5-psg) 61.8 36.1 65.0 65.6\\nGTR (1-psg) 69.5 38.4 56.0 64.0\\nGTR (3-psg) 66.6 39.6 72.8 73.9\\nGTR (5-psg) 66.6 40.4 73.6 72.5\\nFigure 4: Retrieval recall@ kon ASQA ( EM recall ), QAMPARI ( recall-5 ), and ELI5 ( claim recall ). Retrieval recall\\nserves as an upper bound for model performance, and we compare them with two models’ correctness results in the\\nfigure (dashed lines): “Vanilla (5-psg)” is ChatGPT VANILLA with top-5 passages in context; “Oracle” is the same\\nmodel except that it uses 5 gold passages (§G.1), whose recall matches Recall@100 on all three datasets.\\nFluency Correct. Citation\\n(MAUVE) (EM Rec.) Rec. Prec.\\nOpen-source (max #tokens=2K-4K)\\nLLaMA-13B (3-psg) 68.4 26.9 10.6 15.4\\nVicuna-13B (3-psg) 82.6 31.9 51.1 50.1\\nChat-13B (5-psg) 72.4 35.2 38.4 39.4\\nChat-70B (5-psg) 88.3 41.5 62.9 61.3\\nChatGPT (max #tokens=4K)\\nChatGPT (3-psg) 66.6 39.6 72.8 73.9\\nChatGPT (5-psg) 66.6 40.4 73.6 72.5\\nChatGPT-16K (max #tokens=16K)\\nChatGPT (5-psg) 60.3 36.1 76.2 76.5\\nChatGPT (10-psg) 56.3 36.7 75.3 75.0\\nChatGPT (20-psg) 56.7 36.1 73.7 73.5\\nGPT-4 (max #tokens=8K)\\nGPT-4 (5-psg) 67.1 41.3 68.5 75.6\\nGPT-4 (10-psg) 71.5 43.1 72.0 75.5\\nGPT-4 (20-psg) 64.9 44.4 73.0 76.5\\nTable 7: Comparison of different LLMs on ASQA\\n(GTR+ VANILLA ). LLaMA-13B and Vicuna-13B have\\na context limit of 2,048 tokens, and thus can only use a\\nshort version of instructions and at most top-3 passages.\\nChat-13B and Chat-70B refer to LLaMA-2-Chat.\\nretrievers. As the number of passages increases, re-\\ntrieval recall steadily improves. Additionally, Fig-\\nure 4 shows the correctness performance of two\\nmodels: (1) ChatGPT VANILLA with top-5 pas-\\nsages (our primary baseline); (2) an oracle ver-\\nsion of the same model employing 5 gold passages\\n(§G.1; the 5 gold passages match the retrieval re-\\ncall@100). Notably, both models’ correctness lags\\nbehind the corresponding retrieval recall (except\\nfor ELI5 top-5). The discrepancy suggests that de-\\nspite the presence of accurate answers in context,\\nLLMs struggle to utilize them in their outputs.\\nWe compare the impact of different retrievers\\nand different numbers of passages to LLMs. Fig-\\nure 4 (right) shows that GTR outperforms DPR\\nin both correctness and citation quality, emphasiz-\\ning the importance of deploying better retrievers.\\nContrary to the retrieval recall trend in Figure 4,\\nmore passages in context do not yield substantialimprovement for ChatGPT. Specifically, correct-\\nness plateaus at top-1 passage and citation qual-\\nity plateaus at top-3. GPT-4 (Table 7) exhibits\\nan increasing trend with more passages, but the\\nimprovement is not proportional to the retrieval\\nperformance. This indicates the limited ability of\\nLLMs in utilizing multiple passages within context.\\n5.4 Other Ablations\\nWe provide additional ablations in §G. In summary,\\nwe find that (1) using comprehensive instructions\\nenhances the citation quality of instruction-tuned\\nmodels (§G.2); (2) including at least one demon-\\nstration improves the performance (§G.3); (3) fine-\\ntuned models (FiD; Izacard and Grave, 2021) with\\nPOSTCITElag behind LLMs in both correctness\\nand citation quality and fail to generalize (§G.4).\\n6 Human Evaluation\\nTo verify that our automatic evaluation correlates\\nwith human judgement, we conduct human eval-\\nuation on selected models and request workers to\\njudge model generations on three dimensions simi-\\nlar to Liu et al. (2023)—(1) utility: a 1-to-5 score\\nindicating whether the generation helps answer the\\nquestion; (2) citation recall: the annotator is given\\na sentence and all passages that the sentence cited,\\nand is asked to judge whether the passages fully\\nsupport the sentence; (3) citation precision: given\\na sentence and one of its citations, the annotator', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 7}), Document(page_content='and is asked to judge whether the passages fully\\nsupport the sentence; (3) citation precision: given\\na sentence and one of its citations, the annotator\\nis asked to judge whether the citation “fully sup-\\nports”, “partially supports”, or “does not support”\\nthe sentence. Each citation gets a precision score\\n1 if the output sentence has a citation recall of 1\\nand this citation at least “partially supports” it. See\\nAppendix F for more details.\\nModel outputs score high utility. The utility\\nscores do not differ significantly between models,\\nranging 3.7-3.9for ASQA and 3.5-3.6for ELI5.\\nUpon inspection, all tested models are mostly able', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 7}), Document(page_content='Human scores ALCE scores\\nRec. Prec. Rec. Prec.\\nChatGPT V ANILLA 74.7 76.6 75.3 74.4\\nw/ R ERANK 79.3 81.9 83.9 80.8\\nVicuna-13B V ANILLA 51.6 51.5 50.3 50.1\\nTable 8: Human citation quality evaluation vs. ALCE\\ncitation quality evaluation on ASQA.\\nHuman scores ALCE scores\\nRec. Prec. Rec. Prec.\\nChatGPT V ANILLA 50.8 52.4 52.8 50.4\\nw/ R ERANK 59.7 60.6 63.0 60.6\\nVicuna-13B V ANILLA 13.4 19.2 13.6 18.1\\nTable 9: Human citation quality evaluation vs. ALCE\\ncitation quality evaluation on ELI5.\\nto output fluent answers that are related to the ques-\\ntion, despite differences in factual correctness.\\nOur automatic evaluation of citation quality\\nstrongly correlates with human judgements. As\\nshown in Table 8 (ASQA) and Table 9 (ELI5), the\\nrelative rankings induced by human and our auto-\\nmatic metrics are consistent. The absolute citation\\nscores from human and ALCE are very close except\\nforRERANK (which uses the automated citation\\nrecall for reranking). This suggests that an im-\\nprovement on ALCE citation metrics translates to\\nimprovement on human preferences. Furthermore,\\nthe Cohen’s kappa coefficient between human and\\nALCE suggests substantial agreement for citation\\nrecall ( 0.698) and moderate agreement for citation\\nprecision ( 0.525). We also show in §G.5 that our\\nautomatic evaluation achieves high accuracy when\\ntreating human annotations as gold labels ( 85.1%\\nfor citation recall and 77.6%for citation precision).\\n7 Related Work\\nEvaluating citations. Generating text with ci-\\ntations is closely related to attribution. Rashkin\\net al. (2023) define the “attributable to identified\\nsources” (AIS) score to measure how faithful a gen-\\nerated text is to its sources. Bohnet et al. (2022) ap-\\nply AIS scores on a single-document short-answer\\nQA dataset. Honovich et al. (2022); Yue et al.\\n(2023) study automatic evaluations for the AIS\\nscore. A concurrent work (Liu et al., 2023) conduct\\nhuman evaluation on commercial generative search\\nengines to examine their citation qualities.\\nScientific citation text generation (Funkquist\\net al., 2022) is a related task to ALCE where themodel is provided the papers-to-cite and context\\nand is required to recover the citing text. It is differ-\\nent from ALCE as all citations are provided and the\\nmodel only needs to perform the summarization.\\nRetrieval-augmented LMs. Many studies have ex-\\nplored augmenting LMs with externally retrieved\\ninformation. Guu et al. (2020); Borgeaud et al.\\n(2022); Izacard et al. (2022) pre-train language\\nmodels with retrieved passages, while Khandel-\\nwal et al. (2020); Zhong et al. (2022) augment\\nLLMs’ output by interpolating it with a kNN mod-\\nule; though none of them explicitly provide cita-\\ntions to the retrieved sources. Other works prompt\\nor fine-tune LLMs to “retrieve on-the-fly” (Parisi\\net al., 2022; Schick et al., 2023; Shuster et al., 2022;\\nJiang et al., 2023; Yao et al., 2023; Press et al.,\\n2022), which offers flexibility of when and what to\\nsearch. Gao et al. (2023); He et al. (2022) propose\\nto first generate text without accessing external doc-\\numents and then retrieve relevant documents and\\nrevise the generation to be consistent.\\nAmong previous explorations, Nakano et al.\\n(2021); Menick et al. (2022) are the closest to our\\nsetting, where LLMs are trained to answer ques-\\ntions while providing citations. However, they do\\nnot explore retrieval strategies and simply use com-\\nmercial search engines, which are not reproducible,\\nand their models and training data are closed-\\nsource. To the best of our knowledge, we are the\\nfirst to implement end-to-end systems that retrieve,\\nsynthesize, and cite documents with LLMs.\\n8 Conclusion\\nWe propose ALCE, the first automatic benchmark\\nfor evaluating LLM generations with citations. We\\ndeploy automatic metrics to measure fluency, cor-\\nrectness, and citation quality, and verify their effi-\\ncacy via human evaluation. We explore a variety of\\nstrategies for incorporating citations in LLMs and\\ndemonstrate that current systems have considerable', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 8}), Document(page_content='cacy via human evaluation. We explore a variety of\\nstrategies for incorporating citations in LLMs and\\ndemonstrate that current systems have considerable\\nroom for improvement on ALCE.\\nOur experiments highlight a number of promis-\\ning research directions, including (1) enhancing re-\\ntrieval and refining retrieval integrations in LLMs,\\n(2) developing long-context LLMs, and (3) advanc-\\ning LLMs’ ability to synthesize multiple sources.\\nWhat’s even more intriguing is that these research\\nproposals extend beyond the ALCE setup (for ex-\\nample, long-context LLMs have numerous exciting\\napplications), and ALCE can serve as a valuable\\ntestbed for their development.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 8}), Document(page_content='Limitations\\nOur evaluation still has room for improvement: (1)\\nMAUVE is found to be sensitive to output length\\nand may provide unstable results; (2) for the ELI5’s\\ncorrectness evaluation, the automatically generated\\nclaims may not cover all possible answers due to\\nthe open-ended nature of the questions; (3) our ci-\\ntation quality evaluation is limited by the accuracy\\nof the NLI model; for citation precision, the NLI\\nmodel cannot detect the case of “partially support”\\nand thus leads to a lower citation precision score\\nthan the human evaluation.\\nAlthough we believe our curated datasets closely\\nresemble the distribution of real-world user ques-\\ntions, we acknowledge that they do not cover more\\nchallenging scenarios, such as multi-hop reasoning,\\nmath reasoning, and code completion.\\nIn our experiments, we focus on prompting\\nLLMs without updating their model weights. Train-\\ning a model directly to incorporate citations re-\\nmains challenging due to the lack of supervised\\ndata. However, we observe that certain human-\\ninstruction datasets contain examples similar to our\\ntask setup. We leave the exploration of training\\nLLMs to generate citations for future work.\\nAcknowledgments\\nWe appreciate the helpful feedback from the mem-\\nbers of the Princeton NLP group. We thank Alexan-\\nder Wettig, Nelson Liu, Tianyi Zhang, Yu Meng,\\nSadhika Malladi, Yangsibo Huang, Zhiyuan Zeng,\\nand Dan Friedman for the valuable discussion. We\\nthank Surge AI (especially Anna Folinsky and Ed-\\nwin Chen) for their support with the human eval-\\nuation. Tianyu Gao is supported by an IBM PhD\\nFellowship. This research is supported by an NSF\\nCAREER award (IIS-2239290), a Sloan Research\\nFellowship, and Microsoft Azure credits through\\nthe “Accelerate Foundation Models Academic Re-\\nsearch” Initiative.\\nReferences\\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni,\\nDaniel Andor, Livio Baldini Soares, Jacob Eisenstein,\\nKuzman Ganchev, Jonathan Herzig, Kai Hui, et al.\\n2022. Attributed question answering: Evaluation and\\nmodeling for attributed large language models. arXiv\\npreprint arXiv:2212.08037 .\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\\ncan, George Bm Van Den Driessche, Jean-BaptisteLespiau, Bogdan Damoc, Aidan Clark, Diego\\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\\nElsen, and Laurent Sifre. 2022. Improving language\\nmodels by retrieving from trillions of tokens. In Inter-\\nnational Conference on Machine Learning (ICML) ,\\nvolume 162, pages 2206–2240.\\nSamuel Bowman, Gabor Angeli, Christopher Potts, and\\nChristopher D Manning. 2015. A large annotated\\ncorpus for learning natural language inference. In\\nEmpirical Methods in Natural Language Processing\\n(EMNLP) .\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. In Advances in Neural Information Process-\\ning Systems (NeurIPS) .\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\\nsource chatbot impressing gpt-4 with 90%* chatgpt\\nquality.\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\\nier, Jason Weston, and Michael Auli. 2019. ELI5:\\nLong form question answering. In Association for\\nComputational Linguistics (ACL) , pages 3558–3567.\\nMartin Funkquist, Ilia Kuznetsov, Yufang Hou, and\\nIryna Gurevych. 2022. CiteBench: A benchmark for\\nScientific Citation Text Generation. arXiv preprint\\narXiv:2212.09577 .\\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-\\ncent Y Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,\\net al. 2023. RARR: Researching and revising what', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 9}), Document(page_content='Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-\\ncent Y Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,\\net al. 2023. RARR: Researching and revising what\\nlanguage models say, using language models. In\\nAssociation for Computational Linguistics (ACL) .\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\\nand Ming-Wei Chang. 2020. REALM: Retrieval-\\naugmented language model pre-training. In Interna-\\ntional Conference on Machine Learning (ICML) .\\nHangfeng He, Hongming Zhang, and Dan Roth. 2022.\\nRethinking with retrieval: Faithful large language\\nmodel inference. arXiv preprint arXiv:2301.00303 .\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\\nYejin Choi. 2020. The curious case of neural text de-\\ngeneration. In International Conference on Learning\\nRepresentations (ICLR) .\\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\\nScialom, Idan Szpektor, Avinatan Hassidim, and\\nYossi Matias. 2022. TRUE: Re-evaluating factual', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 9}), Document(page_content='consistency evaluation. In North American Chapter\\nof the Association for Computational Linguistics: Hu-\\nman Language Technologies (NAACL-HLT) , pages\\n3905–3920.\\nGautier Izacard and Edouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open do-\\nmain question answering. In Proceedings of the 16th\\nConference of the European Chapter of the Associ-\\nation for Computational Linguistics: Main Volume ,\\npages 874–880, Online. Association for Computa-\\ntional Linguistics.\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\\nYu, Armand Joulin, Sebastian Riedel, and Edouard\\nGrave. 2022. Atlas: Few-shot learning with re-\\ntrieval augmented language models. arXiv preprint\\narXiv:2208.03299 .\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\\nMadotto, and Pascale Fung. 2023. Survey of halluci-\\nnation in natural language generation. ACM Comput-\\ning Surveys , 55(12):1–38.\\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing\\nSun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\\nJamie Callan, and Graham Neubig. 2023. Ac-\\ntive retrieval augmented generation. arXiv preprint\\narXiv:2305.06983 .\\nRyo Kamoi, Tanya Goyal, Juan Diego Rodriguez,\\nand Greg Durrett. 2023. WiCE: Real-World En-\\ntailment for Claims in Wikipedia. arXiv preprint\\narXiv:2303.01432 .\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020. Dense passage retrieval for open-\\ndomain question answering. In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) , pages 6769–6781,\\nOnline. Association for Computational Linguistics.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2020. Generalization\\nthrough memorization: Nearest neighbor language\\nmodels. In International Conference on Learning\\nRepresentations (ICLR) .\\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\\nScitail: A textual entailment dataset from science\\nquestion answering. In Conference on Artificial In-\\ntelligence (AAAI) , volume 32.\\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\\nHurdles to progress in long-form question answering.\\nInProceedings of the 2021 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies ,\\npages 4940–4957, Online. Association for Computa-\\ntional Linguistics.Andreas Köpf, Yannic Kilcher, Dimitri von Rütte,\\nSotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\\nAbdullah Barhoum, Nguyen Minh Duc, Oliver\\nStanley, Richárd Nagyfi, Shahul ES, Sameer Suri,\\nDavid Glushkov, Arnav Dantuluri, Andrew Maguire,\\nChristoph Schuhmann, Huu Nguyen, and Alexander\\nMattick. 2023. Openassistant conversations – de-\\nmocratizing large language model alignment. arXiv\\npreprint arXiv:2304.07327 .\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Association for Com-\\nputational Linguistics (ACL) , pages 6086–6096.\\nNelson F Liu, Tianyi Zhang, and Percy Liang. 2023.\\nEvaluating verifiability in generative search engines.\\narXiv preprint arXiv:2304.09848 .\\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\\nJohn Aslanides, Francis Song, Martin Chadwick,\\nMia Glaese, Susannah Young, Lucy Campbell-\\nGillingham, Geoffrey Irving, et al. 2022. Teaching\\nlanguage models to support answers with verified\\nquotes. arXiv preprint arXiv:2203.11147 .\\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2020. AmbigQA: Answering am-\\nbiguous open-domain questions. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 5783–\\n5797, Online. Association for Computational Lin-\\nguistics.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders,\\net al. 2021. WebGPT: Browser-assisted question-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 10}), Document(page_content='Long Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders,\\net al. 2021. WebGPT: Browser-assisted question-\\nanswering with human feedback. arXiv preprint\\narXiv:2112.09332 .\\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo\\nHernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,\\nKeith Hall, Ming-Wei Chang, and Yinfei Yang. 2022.\\nLarge dual encoders are generalizable retrievers. In\\nEmpirical Methods in Natural Language Processing\\n(EMNLP) , pages 9844–9855.\\nOpenAI. 2023. GPT-4 Technical Report.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow instruc-\\ntions with human feedback. Advances in Neural In-\\nformation Processing Systems (NeurIPS) , 35:27730–\\n27744.\\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. TALM:\\nTool augmented language models. arXiv preprint\\narXiv:2205.12255 .\\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\\nVassilis Plachouras, Tim Rocktäschel, and Sebastian', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 10}), Document(page_content='Riedel. 2021. KILT: a benchmark for knowledge\\nintensive language tasks. In Proceedings of the 2021\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies , pages 2523–2544, Online.\\nAssociation for Computational Linguistics.\\nAleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\\nDmytro Okhonko, Samuel Broscheit, Gautier Izac-\\nard, Patrick Lewis, Barlas O ˘guz, Edouard Grave,\\nWen-tau Yih, et al. 2021. The Web Is Your Oyster–\\nKnowledge-Intensive NLP against a Very Large Web\\nCorpus. arXiv preprint arXiv:2112.09924 .\\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\\nHarchaoui. 2021. MAUVE: Measuring the gap be-\\ntween neural text and human text using divergence\\nfrontiers. In Advances in Neural Information Pro-\\ncessing Systems .\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah A Smith, and Mike Lewis. 2022. Measuring\\nand narrowing the compositionality gap in language\\nmodels. arXiv preprint arXiv:2210.03350 .\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2020. Exploring the limits\\nof transfer learning with a unified text-to-text Trans-\\nformer. The Journal of Machine Learning Research\\n(JMLR) , 21(140).\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. SQuAD: 100,000+ questions\\nfor machine comprehension of text. In Empirical\\nMethods in Natural Language Processing (EMNLP) ,\\npages 2383–2392.\\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\\nLora Aroyo, Michael Collins, Dipanjan Das, Slav\\nPetrov, Gaurav Singh Tomar, Iulia Turc, and David\\nReitter. 2023. Measuring Attribution in Natural Lan-\\nguage Generation Models. Computational Linguis-\\ntics, pages 1–64.\\nSamuel Joseph Amouyal Ohad Rubin, Ori Yoran, Tomer\\nWolfson, Jonathan Herzig, and Jonathan Berant.\\n2022. QAMPARI: An Open-domain Question An-\\nswering Benchmark for Questions with Many An-\\nswers from Multiple Paragraphs. arXiv preprint\\narXiv:2205.12665 .\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\\nCancedda, and Thomas Scialom. 2023. Toolformer:\\nLanguage models can teach themselves to use tools.\\narXiv preprint arXiv:2302.04761 .\\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021.\\nGet your vitamin C! robust fact verification with con-\\ntrastive evidence. In North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies (NAACL-HLT) , pages 624–\\n643.Freda Shi, Xinyun Chen, Kanishka Misra, Nathan\\nScales, David Dohan, Ed Chi, Nathanael Schärli,\\nand Denny Zhou. 2023. Large language models can\\nbe easily distracted by irrelevant context. In Interna-\\ntional Conference on Machine Learning (ICML) .\\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs,\\nStephen Roller, Arthur Szlam, and Jason Weston.\\n2022. Language models that seek for knowledge:\\nModular search & generation for dialogue and\\nprompt completion. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2022 , pages\\n373–393.\\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\\nWei Chang. 2022. ASQA: Factoid questions meet\\nlong-form answers. In Proceedings of the 2022 Con-\\nference on Empirical Methods in Natural Language\\nProcessing , pages 8273–8288, Abu Dhabi, United\\nArab Emirates. Association for Computational Lin-\\nguistics.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford Alpaca:\\nAn Instruction-following LLaMA model.\\nJames Thorne, Andreas Vlachos, Christos\\nChristodoulopoulos, and Arpit Mittal. 2018.\\nFEVER: a large-scale dataset for fact extraction\\nand VERification. In North American Chapter\\nof the Association for Computational Linguistics:\\nHuman Language Technologies (NAACL-HLT) ,\\npages 809–819.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 11}), Document(page_content='Human Language Technologies (NAACL-HLT) ,\\npages 809–819.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, et al. 2023a. LLaMA: Open and Effi-\\ncient Foundation Language Models. arXiv preprint\\narXiv:2302.13971 .\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\\nScialom. 2023b. Llama 2: Open foundation and\\nfine-tuned chat models.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 11}), Document(page_content='Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\\nAsking and answering questions to evaluate the fac-\\ntual consistency of summaries. In Proceedings of the\\n58th Annual Meeting of the Association for Compu-\\ntational Linguistics , pages 5008–5020, Online. Asso-\\nciation for Computational Linguistics.\\nAdina Williams, Nikita Nangia, and Samuel Bowman.\\n2018. A broad-coverage challenge corpus for sen-\\ntence understanding through inference. In North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies\\n(NAACL-HLT) .\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik R Narasimhan, and Yuan Cao. 2023.\\nReact: Synergizing reasoning and acting in language\\nmodels. In International Conference on Learning\\nRepresentations (ICLR) .\\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su,\\nand Huan Sun. 2023. Automatic evaluation of at-\\ntribution by large language models. arXiv preprint\\narXiv:2305.06311 .\\nShiyue Zhang and Mohit Bansal. 2021. Finding a bal-\\nanced degree of automation for summary evaluation.\\nInEmpirical Methods in Natural Language Process-\\ning (EMNLP) , pages 6617–6632.\\nYuan Zhang, Jason Baldridge, and Luheng He. 2019.\\nPAWS: Paraphrase adversaries from word scrambling.\\nInNorth American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies (NAACL-HLT) , pages 1298–1308.\\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-\\ning language models with memory augmentation. In\\nEmpirical Methods in Natural Language Processing\\n(EMNLP) , pages 5657–5673.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 12}), Document(page_content='A Generating Claims for ELI5\\nWe elect not to use ROUGE-L as our main cor-\\nrectness metrics since it does not account for the\\ndifferent ways of expressing the same answer and it\\ncan be easily gamed (Krishna et al., 2021). We fur-\\nther illustrate this issue in Table 10. A system can\\neasily achieve high ROUGE-L score by retrieving\\nand returning the top passage from a BM25 index.\\nHowever, the claims evaluation metric does not\\nreward this approach since the output often lacks\\ndifferent aspects of the answers.\\nROUGE-L Claim recall\\nChatGPT V ANILLA 20.6 12.0\\nChatGPT O RACLE 21.2 21.3\\nLLaMa-13B V ANILLA 16.2 3.9\\nTop-1 passage 19.1 3.0\\nTable 10: Comparison between ROUGE-L and claim\\nrecall scores on ELI5.\\nInstead, we leverage the original answers to gen-\\nerate sub-claims and use them to serve as an esti-\\nmate of the different aspects of the answers that\\nwe expect the model to cover. This approach is\\ninspired by works in summarization evaluation and\\nclaim verification (Zhang and Bansal, 2021; Kamoi\\net al., 2023; Wang et al., 2020).\\nSpecifically, we use text-davinci-003 to gen-\\nerate the sub-claims. We first manually annotate\\nthree question and answer pairs from the original\\nELI5 training set with 3 sub-claims each. Then,\\nwe prompt text-davinci-003 with these pairs as\\ndemonstrations. The full prompt with an example\\nis shown in Table 22.\\nInstructGPT generates coherent and faithful\\nsub-claims. To ensure that the generated sub-\\nclaims are of good quality, we manually inspect\\na random sample of 40 answers and their generated\\nsub-claims (totaling to 120 sub-claims). For each\\nsub-claim, we assign a score of 1if it is relevant to\\nthe question and faithful to the facts presented in\\nthe ground truth, and 0otherwise. We found that\\n112out of the 120(93.33%) sub-claims received a\\nscore of 1, meaning that our generated sub-claims\\nare of high quality and faithful to the ground truth.\\nFurthermore, the average number of words in the\\ngenerated sub-claims is 14words, and they are\\ntypically just one sentence long. This is aligned\\nwith the intent behind the metric—to capture short\\nfactual claims made by the original answer.NLI model accurately predicts the entailment\\nof sub-claims. We further analyze our sub-claim\\nevaluation metrics by checking the error rate of\\nthe final prediction of the NLI model. To this end,\\nwe first manually annotate the entailment scores\\nbetween 40 outputs and their sub-claims (in total\\nof 120 pairs; these are the same questions from the\\nprevious analysis). We then use the NLI model\\nto obtain the entailment scores for the output and\\nsub-claims. Using the human annotations as the\\nground truth label, we found that the NLI model\\nachieved an accuracy of 80.0%.\\nB Dataset Statistics\\nFor ASQA, human answers have an average length\\nof 65 words. For QAMPARI, each question has\\non average 13 answers. For ELI5, human answers\\nhave an average length of 131 words.\\nC Implementation Details\\nNLI model. We use the version of TRUE\\nmodel from https://huggingface.co/google/\\nt5_xxl_true_nli_mixture , which is trained on\\nSNLI (Bowman et al., 2015), MNLI (Williams\\net al., 2018), Fever (Thorne et al., 2018), Sci-\\ntail (Khot et al., 2018), PAWS (Zhang et al., 2019),\\nand VitaminC (Schuster et al., 2021). This model\\nuses the following prompt: “ premise: {PREMISE}\\nhypothesis: {} ” and outputs “ 1” if the premise\\nentails the hypothesis. We format each passage\\n(when used as premise) by the format of “ Title:\\n{TITLE}\\\\n{TEXT} ” and concatenate all passages\\nwith “\\\\n” as a separator.\\nMAUVE. When running MAUVE, we concatenate\\nthe question and the model output (or human an-\\nswer) by space. We truncate both the references\\nand the model generations to 100 words, as we\\nfound MAUVE results are unstable beyond this\\nlength for ELI5 (this is due to that ELI5 has a lot\\nof extremely long human answers).\\nExact match for ASQA and QAMPARI. Both\\nASQA and QAMPARI provide aliases for their\\nshort answers. We normalize the response and the\\nshort answers similarly to Rajpurkar et al. (2016)', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 13}), Document(page_content='Exact match for ASQA and QAMPARI. Both\\nASQA and QAMPARI provide aliases for their\\nshort answers. We normalize the response and the\\nshort answers similarly to Rajpurkar et al. (2016)\\nand report the score with the best-matching aliases.\\nFor ASQA, Stelmakh et al. (2022) also propose a\\nQA-based evaluation which we found to be not as\\nstable, and thus we do not report it in our paper.\\nOutput truncation. Before evaluation, we trun-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 13}), Document(page_content='cate model output by new lines, as non-instruction-\\ntuned models may generate more content after new\\nlines that are irrelevant.\\nINTERACT .Empirically, we found that models\\ntend to execute too many consecutive “ check ” ac-\\ntions, so we force the model to always “ output ”\\nafter each “ check ”. We limit the maximum number\\nof passages to check as 3 to avoid exceeding the\\nlength limit. The full passages are removed from\\nthe context after one action to save context space.\\nTable 27 provides an example for I NTERACT .\\nMain experiments. For all experiments except\\nChatGPT RERANK , we run each model three times\\nwith different seeds and each time we sample two\\ndemonstrations from a pool of four. We report the\\naveraged scores for all experiments in the main\\npaper and we report the standard deviations in Ap-\\npendix G.6.\\nDecoding methods. Based on preliminary experi-\\nments we choose the following decoding methods:\\nFor ChatGPT and GPT-4, we use sampling with\\ntemperature 0.5; for all open-source models, we\\nuse Nucleus sampling (Holtzman et al., 2020) and\\nset top_p = 0.95.\\nD ALCE Catches Shortcut Cases\\nFluency Correct. Citation\\n(MAUVE) (EM Rec.) Rec. Prec.\\nChatGPT 66.6 40.4 73.6 63.0\\nTop-1 passage 20.8 35.1 99.4 99.4\\nFirst 2 sents 67.2 18.9 98.7 98.7\\nTable 11: ASQA cheating cases. “ChatGPT”: the Chat-\\nGPT VANILLA model with GTR-retrieved top-5 pas-\\nsages. “Top-1 passage”: use the top-1 retrieved passage\\nas the response. “First 2 sents”: use the first 2 sentences\\nof the top-1 retrieved passage.\\nTable 11 demonstrates the experiments to show\\nthat ALCE is robust to shortcut cases. Using the\\ntop-1 passages or first two sentences of the top-1\\npassages induces almost perfect citation quality,\\nbut fluency and correctness are dramatically lower.\\nE Citation Recall Discussion\\nOur citation precision evaluation cannot detect a\\ncitation that partially supports the statement and\\nhence will falsely penalize it. Consider a state-\\nments3and its citations [2][4][5] : if[2]entails\\npartial information of s3that[4][5] also entails,[2]will be counted as “irrelevant” while it should\\nnot be penalized. Liu et al. (2023) conduct hu-\\nman evaluation on citation precision in a different\\nway: For each citation, they ask annotators to judge\\nwhether the citation (1) fully support, (2) partially\\nsupport, or (3) does not support si. One citation ci,j\\nis precise if (a) ci,jfully supports sior (b)Cifully\\nsupports si,ci,jpartially supports si, and no c∈ Ci\\nalone fully supports si. This evaluation solved the\\ncorner case we mentioned in the main paper (one\\ncitation partially supports the claim but is identi-\\nfied as “irrelevant”). However, it is challenging to\\nconduct such evaluation automatically, as there is\\nno existing model that can judge whether a cita-\\ntion “partially” supports a claim. We also explore\\nprompting ChatGPT to conduct such a task, which\\nyields poor results. We defer it to future work to\\ncollect supervised data to train a better ϕthat can\\ndetect “partial support”.\\nF Human Evaluation\\nWe employ Surge AI ( https://www.surgehq.\\nai/) for our human evaluation. The average pay to\\nworkers is 20 USD per hour. We randomly sample\\n100 examples from ASQA and ELI5 and annotate\\noutputs of selected models: ChatGPT VANILLA ,\\nChatGPT R ERANK , and Vicuna-13B V ANILLA .\\nF.1 Utility\\nTo check if the model output is useful to down-\\nstream users, we measure the utility of the response\\nS. We first show the query qand model response\\nSto the worker and ask them to rate their agree-\\nment with the statement \"The response is a helpful\\nand informative answer to the query\" on a Likert\\nscale of 1-5, corresponding to Strongly Disagree ,\\nDisagree ,Neutral ,Agree , and Strongly Agree .\\nF.2 Citation Recall\\nThe annotators are shown the question q, the state-\\nment si, and all of its citations Ci, and they rate if\\nthe joint set of citations fully support the statement\\n(recall=1) or if they do not support all the claims\\n(recall=0). We calculate the overall recall score', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 14}), Document(page_content='the joint set of citations fully support the statement\\n(recall=1) or if they do not support all the claims\\n(recall=0). We calculate the overall recall score\\nfor the generation by taking an average of all the\\nstatements’ recall scores.\\nF.3 Citation Precision\\nWe show the question qand a pair of a statement\\nsiand one of its citation ci,j∈ Cito the annotator.\\nWe ask the annotator if the citation fully supports ,', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 14}), Document(page_content='R@1 R@3 R@5 R@20 R@100\\nDPR 29.6 44.5 51.5 64.6 74.1\\nGTR 35.1 50.7 56.8 70.3 78.4\\nOracle 63.8 72.8 78.4 - -\\nTable 12: Retrieval results for ASQA (EM recall).\\nR@1 R@3 R@5 R@20 R@100\\nDPR 6.7 13.5 17.6 30.4 47.6\\nGTR 14.6 24.7 31.6 49.7 65.6\\nOracle 44.3 58.7 65.6 - -\\nTable 13: Retrieval results for QAMPARI (recall-5).\\npartially supports , ordoes not support the factual\\nclaims in si. Citation ci,jhas a citation precision\\nof1ifsihas a recall of 1, andci,jfully or partially\\nsupports si. Finally, we take an average of preci-\\nsion scores of all citations in the statement Sto\\nobtain the citation precision score.\\nG More Experiments\\nG.1 Retrieval Analysis\\nOracle. Since the original datasets do not contain\\ngold passages at the same granularity level as our\\nsetting (100-word passages), we approximate gold\\npassages by running the following algorithm on the\\ntop-100 retrieved passages. We first calculate the\\nrecall score for each passage. Then, we sort the\\npassages using their recall score and take the top 5\\npassages as our initial oracle set. Finally, we iterate\\nthrough all passages that were not initially in the or-\\nacle set and try to replace the passages in the oracle\\nset in a greedy fashion: we calculate the change in\\nthe recall score of the oracle set for every possible\\nreplacement and proceed with the replacement that\\nresults in the largest recall improvement. The set\\nof 5 oracle passages were able to match the recall\\nscores of the top-100 retrieved passages.\\nDetailed retrieval results. We show detailed re-\\ntrieval results in Tables 12, 13, and 14.\\nG.2 Effect of Instructions\\nTable 15 shows results of using a full instruction\\n(Table 23) and a short version of the instruction\\n(Table 24). We see that the full version induces\\nstronger correctness and citation recall, while the\\ntwo instructions lead to similar citation precision.R@1 R@3 R@5 R@20 R@100\\nBM25 3.0 6.6 9.6 19.3 31.8\\nOracle 25.3 29.7 31.8 - -\\nTable 14: Retrieval results for ELI5 (claim recall).\\nFluency Correct. Citation\\n(MAUVE) (EM Rec.) Rec. Prec.\\nChatGPT (VANILLA , 5-doc)\\nShort instruction 64.1 39.5 69.6 73.2\\nFull instruction 66.6 40.4 73.6 72.5\\nTable 15: Effect of different instructions on ASQA.\\nG.3 Effect of Demonstrations\\nTable 16 shows results on effect of different num-\\nbers of demonstrations. We see that numbers of\\ndemonstrations do not affect ChatGPT’s correct-\\nness but using at least one demonstration ensures\\nhigh citation recall. For the original LLaMA model,\\nTable 16 shows the trend that more demonstrations\\nlead to better performance.\\nG.4 Fine-tuned Models\\nTo better understand the differences between fine-\\ntuned models and prompted large language mod-\\nels, we train state-of-the-art question answer-\\ning model, Fusion-in-Decoder (FiD; Izacard and\\nGrave (2021)), and evaluate it in conjunction with\\nPOSTCITE. Due to the lack of training data with\\ncitation annotation, we first train a T5-base FiD\\nmodel for 5 epochs on the ASQA training set with\\na batch size of 64 and a learning rate of 1e-4. Dur-\\ning evaluation, we use POSTCITEto add citations\\nto the output. We also use k= 5passages during\\nboth training and evaluation of the FiD model.\\nThen, we evaluate this model on both ASQA\\n(in-domain) and ELI5 (out-of-domain), and the re-\\nsults can be found in Tables 17 and 18. Note that\\nthis is not a direct comparison, as ALCE assumes\\nonly evaluation data available and uses only few-\\nshot data for prompting. As the results show, the\\nFiD baseline still significantly lags behind prompt-\\ning ChatGPT in both correctness and citation qual-\\nity (even though it is trained on 4000+ examples).\\nWhen tested on another dataset (ELI5), FiD per-\\nforms even worse, showing that it is challenging\\nto solve the problem by fine-tuning a small pre-\\ntrained model.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 15}), Document(page_content='Fluency Correct. Citation\\n(MAUVE) (EM) Rec. Prec.\\nChatGPT (VANILLA )\\n#demo = 0 74.5 41.9 69.3 73.4\\n#demo = 1 68.9 39.8 74.6 73.2\\n#demo = 2 66.6 40.4 73.6 72.5\\nTable 16: Different demonstrations on ASQA.\\nFluency Correct. Citation\\n(MAUVE) (EM Rec.) Rec. Prec.\\nChatGPT 66.6 40.4 73.6 72.5\\nFiD + P OSTCITE 75.8 28.4 58.1 58.0\\nTable 17: Comparison of Fusion-in-Decoder with Chat-\\nGPT on ASQA. Both models use top-5 GTR passages.\\nG.5 More Human Evaluation\\nWe evaluate the accuracy of our automatic metrics\\nby treating the human annotations as gold labels.\\nFor citation recall, ALCE achieves an accuracy of\\n85.1%; for citation precision, ALCE has an accu-\\nracy of 77.6%. Regarding detecting insufficient\\ncitations, ALCE has a recall of 82.3%and a pre-\\ncision of 84.2%; regarding detecting “irrelevant”\\ncitations, ALCE has a recall of 75.6%and a pre-\\ncision of 66.1%—ALCE is effective in detecting\\n“irrelevant” citations, but due to the limitation of\\nthe NLI model (cannot detect “partial support”), it\\nhas a relatively high false positive rate.\\nG.6 Main Results\\nWe show full results of our experiments along with\\nthe standard deviation in Tables 19, 20, and 21. We\\nrepeat all experiments with three different random\\nseeds. However, for ChatGPT RERANK , we use\\nonly one seeded run since each run repeats the\\ngeneration step four times, and more experiments\\nwould incur significant costs. Similarly, due to the\\ncost of running ChatGPT-16K and GPT-4, we only\\nuse one seeded run for each model.\\nG.7 Open-source Models\\nIn addition to the open-source models discussed in\\nthe main text, we also show the results of LLaMA-\\n7B, Alpaca-7B, Vicuna-7B, LLaMA-33B, Oasst-\\n33B, and Stable Beluga 2 in the Tables 19, 20, and\\n21. For selected models, we also tested them using\\napproaches beyond V ANILLA .\\nAlthough open-source models generally lag be-\\nhind the state-of-the-art models (i.e. GPT-4) inFluency Correct. Citation\\n(MAUVE) (Claim) Rec. Prec.\\nChatGPT 57.2 12.0 51.1 50.0\\nFiD + P OSTCITE 25.2 4.4 39.3 39.3\\nTable 18: Comparison of Fusion-in-Decoder with Chat-\\nGPT on ELI5. Both models use top-5 GTR passages.\\nboth correctness and citation quality, the largest\\ninstruction-following models (i.e. LLaMA-2-70B-\\nChat and Stable Beluga 2) can sometimes achieve\\ncompetitive correctness to the SoTA.\\nFurthermore, we found that the open-source\\nmodels follow a similar trend between different\\napproaches as ChatGPT. Specifically, using SUMM\\nandSNIPPET improves correctness and RERANK\\nboosts citation quality.\\nH Prompts\\nWe show detailed prompts used in our paper in\\nTables 23, 24, 25, 26, 27, 28, and 29.\\nI Examples\\nIn Tables 30 and 31 we show some examples of\\nquestions and model generated outputs.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 16}), Document(page_content='Fluency Correct. Citation\\n(MAUVE) (EM Rec.) Rec. Prec. ROUGE-L Length\\nChatGPT\\nVANILLA (5-psg) 66.8 (2.0) 40.4 (0.6) 73.6 (1.1) 72.5 (1.8) 37.0 (0.4) 40.0 (3.1)\\nw/ R ERANK 77.0 (−) 40.2 (−) 84.8 (−) 81.6 (−) 36.9 (−) 40.8 (−)\\nSUMM (10-psg) 70.0 (1.2) 43.3 (0.8) 68.8 (0.6) 61.8 (1.1) 36.9 (0.2) 49.8 (4.3)\\nw/ I NTERACT 69.0 (2.7) 39.1 (0.5) 73.4 (0.2) 66.5 (4.9) 35.7 (0.2) 34.0 (0.9)\\nSNIPPET (10-psg) 69.8 (2.5) 41.4 (0.6) 65.3 (0.6) 57.4 (0.9) 36.4 (0.4) 43.0 (3.5)\\nINLINE SEARCH 58.7 (1.3) 32.4 (0.6) 58.3 (1.3) 58.3 (1.3) 58.2 (1.1) 23.7 (1.1)\\nCLOSED BOOK 52.7 (4.9) 38.2 (0.1) 26.7 (1.1) 26.7 (1.1) 37.1 (0.3) 61.1 (4.5)\\nORACLE (5-psg) 64.4 (0.6) 48.9 (1.2) 74.5 (0.6) 72.7 (1.0) 38.2 (1.0) 37.4 (3.0)\\nChatGPT-16K\\nVANILLA (5-psg) 60.3 (−) 36.1 (−) 76.2 (−) 76.5 (−) 36.2 (−) 24.7 (−)\\nVANILLA (10-psg) 56.3 (−) 36.7 (−) 75.3 (−) 75.0 (−) 35.6 (−) 23.5 (−)\\nVANILLA (20-psg) 56.7 (−) 36.1 (−) 73.7 (−) 73.5 (−) 35.5 (−) 23.1 (−)\\nGPT-4\\nVANILLA (5-psg) 67.1 (−) 41.3 (−) 68.5 (−) 75.6 (−) 39.2 (−) 31.8 (−)\\nVANILLA (10-psg) 71.5 (−) 43.1 (−) 72.0 (−) 75.5 (−) 39.7 (−) 33.8 (−)\\nVANILLA (20-psg) 64.9 (−) 44.4 (−) 73.0 (−) 76.5 (−) 40.1 (−) 34.3 (−)\\nOpen-source\\nLLaMA-7B V ANILLA (3-psg) 69.8 (2.0) 22.6 (0.9) 6.2 (2.7) 9.2 (2.9) 29.1 (0.2) 61.3 (14.3)\\nAlpaca-7B V ANILLA (3-psg) 84.2 (2.7) 32.1 (1.7) 12.3 (7.2) 14.1 (7.0) 33.1 (0.8) 51.7 (12.8)\\nVicuna-7B V ANILLA (3-psg) 82.9 (5.0) 34.6 (0.7) 40.3 (0.5) 42.6 (1.0) 35.9 (0.7) 48.9 (6.6)\\nLLaMA-13B V ANILLA (3-psg) 68.4 (6.4) 26.9 (0.4) 10.6 (4.7) 15.4 (5.2) 29.8 (0.5) 67.1 (19.1)\\nw/ R ERANK 60.9 (14.5) 25.2 (2.5) 28.1 (9.3) 37.0 (7.2) 27.9 (2.4) 50.5 (14.3)\\nLLaMA-13B S UMM (10-psg) 76.8 (4.7) 33.3 (0.7) 19.6 (3.9) 23.7 (4.7) 32.1 (0.3) 54.4 (1.5)\\nLLaMA-13B S NIPPET (10-psg) 72.0 (0.8) 31.3 (1.1) 18.2 (3.1) 21.1 (3.6) 30.8 (0.4) 50.5 (4.5)\\nLLaMA-13B O RACLE (3-psg) 69.5 (11.4) 34.3 (0.9) 10.8 (4.9) 15.8 (5.9) 30.6 (0.1) 67.3 (17.9)\\nVicuna-13B V ANILLA (3-psg) 82.6 (9.4) 31.9 (3.9) 51.1 (1.4) 50.1 (2.5) 34.9 (1.3) 39.1 (6.6)\\nw/ R ERANK 73.5 (2.1) 32.9 (1.3) 71.9 (1.9) 65.4 (1.5) 34.6 (0.3) 35.7 (4.2)\\nVicuna-13B S UMM (10-psg) 67.7 (0.3) 43.2 (0.1) 52.7 (2.6) 50.0 (2.1) 36.7 (0.2) 66.0 (1.2)\\nVicuna-13B S NIPPET (10-psg) 81.4 (3.0) 42.1 (1.2) 53.4 (1.9) 48.7 (1.6) 36.9 (0.4) 61.2 (7.4)\\nVicuna-13B O RACLE (3-psg) 72.9 (3.5) 42.5 (1.6) 52.2 (0.8) 50.7 (1.6) 36.5 (0.9) 38.7 (3.5)\\nLLaMA-33B V ANILLA (3-psg) 83.7 (5.4) 31.0 (0.8) 19.5 (5.3) 23.0 (5.3) 32.3 (0.6) 44.1 (9.3)\\nw/ R ERANK 82.1 (3.0) 31.3 (1.1) 41.3 (6.4) 44.7 (5.5) 32.5 (0.9) 39.4 (8.0)\\nLLaMA-33B S UMM (10-psg) 72.0 (3.0) 33.1 (1.9) 34.7 (5.8) 35.2 (6.0) 31.1 (0.8) 43.7 (5.0)\\nLLaMA-33B S NIPPET (10-psg) 70.8 (3.1) 30.9 (1.4) 31.4 (4.2) 31.5 (5.3) 30.1 (0.7) 42.8 (3.6)\\nLLaMA-33B O RACLE (3-psg) 82.6 (7.1) 39.3 (2.9) 20.2 (6.2) 23.9 (6.3) 33.1 (0.9) 42.0 (9.3)\\nOasst-33B V ANILLA (3-psg) 82.9 (2.7) 34.8 (1.5) 36.2 (1.7) 38.3 (2.7) 35.5 (0.7) 45.2 (6.3)\\nw/ R ERANK 83.2 (2.4) 35.1 (1.4) 66.7 (0.2) 64.3 (1.0) 35.0 (0.6) 41.8 (6.0)\\nOasst-33B S UMM (10-psg) 74.3 (4.6) 40.9 (1.1) 45.5 (1.9) 44.0 (2.9) 35.8 (0.6) 54.3 (4.8)\\nOasst-33B S NIPPET (10-psg) 79.3 (1.0) 40.1 (0.9) 45.0 (1.3) 43.3 (2.2) 35.8 (0.2) 50.9 (4.1)\\nOasst-33B O RACLE (3-psg) 85.1 (2.8) 44.3 (2.4) 37.0 (1.0) 39.6 (1.5) 36.5 (1.1) 44.2 (5.8)\\nLLaMA-2-7B-Chat V ANILLA (5-psg) 80.1 (6.5) 33.9 (2.1) 50.9 (4.5) 47.5 (3.7) 35.1 (0.9) 42.3 (10.1)\\nLLaMA-2-13B-Chat V ANILLA (5-psg) 72.4 (6.3) 35.2 (1.2) 38.4 (5.9) 39.4 (4.8) 35.8 (0.9) 38.0 (6.4)\\nLLaMA-2-70B-Chat V ANILLA (5-psg) 88.3 (4.1) 41.5 (0.8) 62.9 (1.4) 61.3 (2.1) 37.1 (0.4) 52.9 (9.5)\\nStable Beluga 2 V ANILLA (5-psg) 59.7 (3.3) 37.2 (1.0) 63.6 (0.7) 63.5 (0.1) 35.3 (0.7) 23.8 (3.2)\\nTable 19: ASQA full results.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 17}), Document(page_content='Correctness Citation\\nRec.-5 Prec. Rec. Prec. Num Pred.\\nChatGPT\\nVANILLA (5-psg) 20.8 (2.2) 20.8 (0.2) 20.5 (0.7) 20.9 (0.7) 5.0 (0.5)\\nw/ R ERANK 22.8 (−) 21.4 (−) 21.2 (−) 21.4 (−) 5.4 (−)\\nSUMM (10-psg) 23.6 (0.9) 21.2 (0.5) 23.6 (0.7) 25.7 (0.8) 6.7 (0.4)\\nSNIPPET (10-psg) 24.5 (1.4) 21.5 (1.8) 22.9 (1.6) 24.9 (0.4) 7.2 (0.9)\\nw/ I NTERACT 21.9 (0.9) 23.0 (0.4) 21.9 (1.2) 23.4 (0.9) 6.7 (0.4)\\nINLINE SEARCH 17.2 (1.1) 20.4 (0.8) 14.9 (0.8) 14.9 (0.8) 6.7 (0.2)\\nCLOSED BOOK 32.9 (1.1) 19.8 (1.6) 10.0 (0.4) 10.0 (0.4) 17.0 (2.9)\\nORACLE 37.0 (3.1) 36.9 (0.6) 24.1 (1.2) 24.6 (1.3) 5.3 (0.6)\\nChatGPT-16K\\nVANILLA (5-psg) 21.1 (−) 22.0 (−) 20.7 (−) 21.2 (−) 4.9 (−)\\nVANILLA (10-psg) 23.4 (−) 21.9 (−) 21.6 (−) 22.0 (−) 5.7 (−)\\nVANILLA (20-psg) 26.4 (−) 21.1 (−) 19.4 (−) 19.7 (−) 7.6 (−)\\nGPT-4\\nVANILLA (5-psg) 22.2 (−) 25.0 (−) 25.9 (−) 27.0 (−) 4.4 (−)\\nVANILLA (10-psg) 26.8 (−) 25.1 (−) 26.2 (−) 27.2 (−) 5.7 (−)\\nVANILLA (20-psg) 29.6 (−) 26.2 (−) 27.4 (−) 28.5 (−) 6.8 (−)\\nOpen-source\\nLLaMA-7B V ANILLA (3-psg) 7.8 (3.4) 7.4 (2.7) 5.1 (0.5) 5.7 (0.8) 5.7 (0.6)\\nAlpaca-7B V ANILLA (3-psg) 9.4 (3.7) 9.5 (3.6) 6.4 (0.5) 6.8 (0.5) 5.1 (0.1)\\nVicuna-7B V ANILLA (3-psg) 11.3 (1.4) 13.3 (2.3) 10.1 (0.6) 10.9 (0.5) 3.9 (0.3)\\nLLaMA-13B V ANILLA (3-psg) 9.7 (3.6) 9.1 (3.1) 6.7 (0.9) 7.1 (0.9) 5.9 (0.6)\\nw/ R ERANK 10.0 (3.3) 10.7 (3.3) 9.9 (1.2) 10.2 (1.1) 5.4 (0.5)\\nLLaMA-13B S UMM (10-psg) 14.8 (2.5) 12.6 (1.5) 7.4 (0.5) 8.0 (0.6) 8.1 (0.9)\\nLLaMA-13B S NIPPET (10-psg) 17.7 (1.4) 15.7 (0.9) 8.8 (0.7) 9.9 (0.6) 8.2 (0.4)\\nLLaMA-13B O RACLE (3-psg) 16.8 (6.6) 15.4 (5.6) 7.7 (1.0) 8.3 (1.1) 5.7 (0.7)\\nVicuna-13B V ANILLA (3-psg) 14.0 (0.6) 15.9 (1.7) 12.5 (0.8) 13.4 (0.7) 4.7 (0.3)\\nw/ R ERANK 13.0 (0.7) 17.2 (2.2) 17.3 (0.8) 17.7 (0.6) 4.4 (0.3)\\nVicuna-13B S UMM (10-psg) 21.1 (1.4) 17.1 (0.3) 15.7 (0.2) 17.8 (0.1) 6.9 (0.7)\\nVicuna-13B S NIPPET (10-psg) 21.9 (0.8) 18.2 (0.3) 16.8 (0.3) 19.7 (0.6) 7.5 (0.4)\\nVicuna-13B O RACLE (3-psg) 25.9 (1.6) 28.4 (2.6) 15.8 (1.4) 16.8 (1.4) 4.9 (0.5)\\nLLaMA-33B V ANILLA (3-psg) 14.7 (3.3) 12.0 (2.2) 7.9 (0.7) 8.3 (0.6) 7.2 (0.7)\\nw/ R ERANK 14.0 (3.4) 13.9 (2.6) 10.7 (0.6) 11.1 (0.5) 6.4 (0.7)\\nLLaMA-33B S UMM (10-psg) 19.0 (1.9) 14.8 (0.8) 12.5 (0.2) 15.0 (0.3) 7.6 (0.6)\\nLLaMA-33B S NIPPET (10-psg) 19.6 (1.1) 15.7 (0.1) 12.8 (1.1) 15.2 (1.2) 7.8 (0.5)\\nLLaMA-33B O RACLE (3-psg) 23.9 (6.9) 20.3 (5.2) 9.8 (1.2) 10.4 (1.2) 6.8 (0.9)\\nOasst-33B V ANILLA (3-psg) 15.5 (1.5) 14.9 (1.4) 9.0 (1.6) 10.1 (1.8) 5.6 (0.3)\\nw/ R ERANK 14.1 (1.1) 15.8 (1.0) 15.0 (1.6) 15.9 (1.6) 4.7 (0.3)\\nOasst-33B S UMM (10-psg) 21.0 (0.6) 17.5 (1.0) 12.9 (1.2) 16.6 (1.2) 7.1 (0.4)\\nOasst-33B S NIPPET (10-psg) 22.0 (0.4) 17.4 (0.3) 13.6 (1.7) 17.7 (1.6) 7.5 (0.1)\\nOasst-33B O RACLE (3-psg) 26.9 (3.7) 26.0 (3.3) 11.7 (1.0) 12.9 (1.2) 5.6 (0.4)\\nLLaMA-2-7B-Chat V ANILLA (5-psg) 16.2 (1.3) 15.3 (1.6) 10.6 (0.9) 10.9 (1.0) 5.5 (0.0)\\nLLaMA-2-13B-Chat V ANILLA (5-psg) 21.1 (0.9) 18.2 (0.5) 9.6 (1.5) 9.7 (1.5) 6.5 (0.3)\\nLLaMA-2-70B-Chat V ANILLA (5-psg) 21.8 (0.7) 18.4 (0.1) 15.1 (1.2) 15.6 (1.3) 7.1 (0.2)\\nStable Beluga 2 19.7 (0.7) 21.2 (1.4) 18.6 (1.2) 20.7 (1.1) 5.1 (0.3)\\nTable 20: QAMPARI full results.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 18}), Document(page_content='Fluency Correct. Citation\\n(MAUVE) (Claim) Rec. Prec. ROUGE-L Length\\nChatGPT\\nVANILLA (5-psg) 57.2 (1.6) 12.0 (0.6) 51.1 (4.2) 50.0 (4.8) 20.6 (0.2) 91.5 (6.5)\\nw/ R ERANK 56.1 (−) 11.4 (−) 69.3 (−) 67.8 (−) 20.3 (−) 103.4 (−)\\nSUMM (10-psg) 40.2 (1.2) 12.5 (0.2) 51.5 (1.1) 48.2 (2.0) 20.3 (0.1) 90.0 (6.6)\\nSNIPPET (10-psg) 62.9 (2.2) 14.3 (0.1) 50.4 (1.1) 45.0 (2.w) 21.0 (0.1) 100.0 (6.8)\\nw/ I NTERACT 68.0 (5.8) 13.3 (0.5) 47.8 (3.3) 45.0 (3.1) 20.1 (0.2) 99.8 (6.1)\\nINLINE SEARCH 49.7 (4.6) 13.4 (1.1) 45.6 (2.5) 43.7 (3.9) 20.4 (0.3) 103.0 (18.1)\\nCLOSED BOOK 32.6 (1.1) 18.6 (0.5) 15.4 (0.3) 15.4 (0.3) 22.8 (0.1) 108.3 (8.9)\\nORACLE (5-psg) 59.4 (4.1) 21.3 (0.2) 57.8 (3.7) 56.0 (3.8) 21.2 (0.3) 93.0 (7.8)\\nChatGPT-16K\\nVANILLA (5-psg) 31.6 (−) 14.4 (−) 44.6 (−) 44.1 (−) 21.4 (−) 87.6 (−)\\nVANILLA (10-psg) 26.6 (−) 14.4 (−) 45.5 (−) 43.3 (−) 21.5 (−) 87.5 (−)\\nVANILLA (20-psg) 31.6 (−) 15.9 (−) 43.4 (−) 40.9 (−) 21.7 (−) 92.6 (−)\\nGPT-4\\nVANILLA (5-psg) 38.4 (−) 14.2 (−) 44.0 (−) 50.1 (−) 20.6 (−) 79.6 (−)\\nVANILLA (10-psg) 39.9 (−) 15.7 (−) 49.5 (−) 54.2 (−) 21.2 (−) 88.2 (−)\\nVANILLA (20-psg) 41.5 (−) 18.3 (−) 48.5 (−) 53.4 (−) 22.2 (−) 97.0 (−)\\nOpen-source\\nLLaMA-7B V ANILLA (3-psg) 28.6 (17.9) 1.6 (0.9) 1.2 (0.0) 2.7 (0.1) 12.2 (1.3) 46.9 (1.2)\\nAlpaca-7B V ANILLA (3-psg) 45.9 (5.3) 9.2 (0.1) 4.5 (1.6) 5.2 (1.9) 18.8 (0.3) 67.1 (1.2)\\nVicuna-7B V ANILLA (3-psg) 43.2 (3.9) 10.0 (0.5) 12.6 (2.3) 16.3 (2.6) 19.1 (0.4) 68.7 (2.0)\\nLLaMA-13B V ANILLA (3-psg) 50.0 (2.0) 3.9 (0.4) 3.1 (0.9) 5.3 (1.3) 16.1 (0.5) 63.3 (2.0)\\nw/ R ERANK 46.7 (2.9) 4.3 (0.4) 9.7 (2.1) 15.0 (2.2) 16.1 (0.7) 63.0 (2.3)\\nLLaMA-13B S UMM (10-psg) 28.6 (1.8) 2.9 (0.1) 2.5 (0.8) 3.8 (0.8) 8.5 (0.3) 33.1 (0.6)\\nLLaMA-13B S NIPPET (10-psg) 48.4 (3.1) 5.7 (0.9) 5.8 (0.6) 7.6 (0.9) 15.1 (1.1) 60.2 (3.2)\\nLLaMA-13B O RACLE (3-psg) 49.5 (2.4) 6.4 (0.6) 3.7 (0.7) 6.5 (1.0) 16.8 (0.5) 64.5 (1.7)\\nVicuna-13B V ANILLA (3-psg) 58.2 (25.1) 10.0 (0.3) 15.6 (2.2) 19.6 (2.0) 19.1 (0.3) 69.6 (0.6)\\nw/ R ERANK 45.9 (4.3) 9.2 (0.0) 31.7 (2.9) 38.2 (1.6) 18.6 (0.5) 69.7 (1.0)\\nVicuna-13B S UMM (10-psg) 22.4 (3.0) 4.9 (0.1) 9.7 (1.3) 12.2 (1.2) 9.3 (0.4) 33.0 (3.7)\\nVicuna-13B S NIPPET (10-psg) 48.1 (5.3) 11.2 (1.4) 27.2 (3.6) 27.9 (1.9) 18.4 (1.9) 76.8 (8.7)\\nVicuna-13B O RACLE (3-psg) 41.6 (3.1) 17.1 (0.4) 20.2 (3.0) 26.5 (3.0) 20.0 (0.3) 72.0 (0.3)\\nLLaMA-33B V ANILLA (3-psg) 58.8 (4.3) 6.2 (0.0) 9.3 (3.0) 12.1 (4.2) 16.9 (0.2) 60.0 (1.3)\\nw/ R ERANK 65.9 (2.5) 6.0 (0.7) 22.5 (5.2) 26.1 (6.9) 17.5 (0.4) 61.0 (1.2)\\nLLaMA-33B S UMM (10-psg) 23.3 (2.0) 3.0 (0.2) 6.2 (0.5) 8.2 (0.7) 7.5 (0.4) 26.2 (2.3)\\nLLaMA-33B S NIPPET (10-psg) 53.2 (4.0) 7.4 (1.3) 13.7 (0.5) 15.1 (0.4) 14.4 (1.7) 53.3 (8.5)\\nLLaMA-33B O RACLE (3-psg) 63.7 (2.8) 11.4 (0.5) 11.9 (2.6) 15.4 (3.6) 17.9 (0.2) 61.7 (2.6)\\nOasst-33B V ANILLA (3-psg) 46.8 (7.6) 9.5 (0.2) 16.0 (2.5) 21.6 (3.5) 18.6 (0.3) 67.8 (1.5)\\nw/ R ERANK 52.1 (6.1) 8.5 (0.5) 34.4 (2.9) 41.5 (2.5) 18.2 (0.3) 67.0 (1.5)\\nOasst-33B S UMM (10-psg) 24.8 (2.8) 3.9 (0.3) 12.3 (0.2) 16.3 (0.3) 9.1 (0.3) 31.6 (1.5)\\nOasst-33B S NIPPET (10-psg) 50.7 (4.6) 10.7 (1.2) 25.8 (3.3) 26.7 (2.3) 17.8 (1.8) 69.6 (8.6)\\nOasst-33B O RACLE (3-psg) 50.7 (12.1) 15.8 (0.1) 20.8 (2.8) 28.0 (3.2) 19.4 (0.1) 70.3 (1.1)\\nLLaMA-2-7B-Chat V ANILLA (5-psg) 27.8 (3.0) 10.9 (0.2) 19.8 (1.2) 15.0 (1.4) 20.5 (0.2) 87.8 (8.1)\\nLLaMA-2-13B-Chat V ANILLA (5-psg) 34.7 (1.5) 13.4 (0.4) 17.3 (1.3) 15.8 (1.4) 20.9 (0.2) 88.3 (6.3)\\nLLaMA-2-70B-Chat V ANILLA (5-psg) 38.6 (4.8) 12.8 (1.0) 38.3 (2.4) 37.9 (1.9) 21.3 (0.1) 110.8 (5.6)\\nStableBeluga2 33.0 (4.4) 14.0 (0.5) 27.9 (2.7) 29.0 (2.7) 20.6 (0.2) 75.6 (5.0)\\nTable 21: ELI5 full results.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 19}), Document(page_content='Read the original question and passage, and generate 3 additional claims that are supported by the\\npassage and answer the question.\\nOriginal question: What’s the difference between Shia vs. Sunni Islam?\\nPassage: The main difference between Shia and Sunni Muslim is related to ideological heritage and\\nissues of leadership. This difference is first formed after the death of the Prophet Muhammad in 632\\nA.D. The ideological practice of the Sunni branch strictly follows Prophet Muhammad and his\\nteachings, while the Shia branch follows Prophet Muhammad’s son-in-law Ali. Nowadays, Sunni and\\nShia are the major branches of Islam.\\nClaim 1: The major branches of Islam are Sunni and Shia.\\nClaim 2: Prophet Muhammad died in 632 A.D.\\nClaim 3: The ideological practice of the Sunni branch strictly follows Prophet Muhammad and his\\nteachings.\\nOriginal question: What causes Bi-polar disorder?\\nPassage: Bipolar disorder is an emotional disorder that causes extreme mood swings between\\nexcitement and depression. The spectrum of mood swing may span from days to months. We are still not\\ncertain of the exact factors that cause such disorder, but genetics is considered a major factor.\\nClaim 1: One symptom of Bi-polar disorder is extreme mood swings between excitement and depression.\\nClaim 2: Genetics could be one of the major factors that causes Bi-polar disorder.\\nClaim 3: The mood swing from Bi-polar disorder can last days to months.\\nOriginal question: How do we hear differences in sound besides volume and pitch?\\nPassage: Pitch refers to the frequency of soundwave, and volumn refers to the amplitude of the\\nsoundwave. Besides volumn and pitch, we can also tell the difference between sounds based on the\\ntone of sound. For example, we can differentiate the sound of different instruments based on the\\ntone of the sounds.\\nClaim 1: Volume of sound is the amplitude of the soundwave.\\nClaim 2: Pitch is the frequency of soundwave.\\nClaim 3: We can use the tone of the sounds to differentiate the sound of different instruments.\\nOriginal question: How are we able to discern whether a sound is coming from in front of us or\\nbehind us?\\nPassage: There are multiple explanations for why we can localize sounds. One explanation is that\\nsounds travelling to the corresponding side of one’s ear will be slightly louder. Another\\nexplanation is that there is a slight difference in the hitting time to one’s left and right ear\\nbased on the sound’s direction. However, these explanation means that when a sound is exactly in\\nfront of someone or exactly behind someone, he or she can not tell the difference.\\nClaim 1: We can localize sounds by recognizing that the sound travelling to the corresponding side\\nof one’s ear will be slightly louder.\\nClaim 2: We can also localize sounds by recognizing the difference in hitting time to one’s left and\\nright ear based on the sound’s direction.\\nClaim 3: We cannot tell the difference between a sound that is exactly in front of us or exactly\\nbehind us.\\nTable 22: Prompt used to generate the sub-claims for ELI5 questions. Blue text is model generation. Brown text\\nis the ELI5 example that we want to generate sub-claims for. We construct the prompt by manually writing the\\nsub-claims for three questions from the training set.\\nInstruction: Write an accurate, engaging, and concise answer for the given question using only\\nthe provided search results (some of which might be irrelevant) and cite them properly. Use an\\nunbiased and journalistic tone. Always cite for any factual claim. When citing several search\\nresults, use [1][2][3]. Cite at least one document and at most three documents in each sentence.\\nIf multiple documents support the sentence, only cite a minimum sufficient subset of the\\ndocuments.\\nTable 23: Instruction for V ANILLA .\\nInstruction: Write a high-quality answer for the given question using only the provided search\\nresults and cite them properly using [1][2][3].\\nTable 24: Short instruction for V ANILLA .', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 20}), Document(page_content='Summarize the following document within 50 words with the question of interest \"{QUESTION}\"\\nReturn \"irrelevant\" if the document is irrelevant to the question. Try to keep all the important\\ndates, numbers, and names.\\nTitle: {TITLE}\\nText: {TEXT}\\nSummary:\\nTable 25: Prompts for S UMM .\\nGiven the follow passage and the question \"{QUESTION}\", extract a useful span from the passage\\nthat can answer the question. Resolve all the coreference issues to make the extracted span\\nunderstandable standalone. If the passage is not helpful for answering the question, return\\n\"irrelevant\".\\nTitle: {TITLE}\\nText: {TEXT}\\nExtracted span:\\nTable 26: Prompts for S NIPPET .\\nInstruction: Write an accurate, engaging, and concise answer for the given question using only\\nthe provided search results and cite them properly. Use an unbiased and journalistic tone.\\nAlways cite for any factual claim.\\nYou are provided summaries/snippets of the search results. You can use \"Check: Document [1][2]\"\\nto check the corresponding full documents (you should only check relevant documents and you can\\nat most check 3 documents at a time) and use \"Output:\" to output a sentence in the answer. In the\\nanswer, cite properly by using [1][2][3]. Cite at least one document and at most three documents\\nin each sentence. If multiple documents support the sentence, only cite a minimum sufficient\\nsubset of the documents. Use \"End\" to end the generation.\\n<Retrieve for question “...”>\\n<Get summaries/snippets for the passages and delete those that are “irrelevant”>\\nDocument [1](Title: ...) {SUMMARY OR SNIPPET}\\n...\\nQuestion: When did US break away from England?\\nCheck: Document [1][2]\\nDocument [1] {FULL TEXT}\\nDocument [2] {FULL TEXT}\\nOutput: The United States ... [1]... [2].\\n<Remove the full text of [1][2] from context>\\nCheck: Document [3]\\nDocument [3] {FULL TEXT}\\nOutput: The Treaty of Paris ... [3].\\n<Remove the full text of [3] from context>\\nEnd.\\nTable 27: An example for I NTERACT .\\nInstruction: Write an accurate, engaging, and concise answer for the given question using only\\nthe provided search results and cite them properly. Use an unbiased and journalistic tone.\\nAlways cite for any factual claim.\\nYou can use \"Search: key words\" to check the most relevant document’s full text and use\\n\"Output:\" to output a sentence in the answer. In the answer, cite properly by using [1][2][3].\\nCite at least one document and at most three documents in each sentence. If multiple documents\\nsupport the sentence, only cite a minimum sufficient subset of the documents. Use \"End\" to end\\nthe generation.\\nTable 28: Instruction for I NLINE SEARCH .\\nInstruction: Write an accurate, engaging, and concise answer for the given question. Use an\\nunbiased and journalistic tone.\\nTable 29: Instruction for C LOSED BOOK.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 21}), Document(page_content='Instruction: Write an accurate, engaging, and concise answer for ...\\nDocument [1](Title: How to Treat and Prevent Food Poisoning - MsPrepper) just a typical gastro\\nupset. Salmonella is most commonly caused by eating undercooked or raw foods like eggs or meat.\\nYou know how your mom always warned you not to eat raw cookie dough? This is why. Most people do\\neat cookie dough and they are fine, but salmonella is a risk. If you do contract salmonella, you\\ncould start to feel bad within in a couple of hours after eating contaminated food, and sometimes\\nit could take a day or two. Common symptoms are nausea and vomiting, loose stools (sometimes\\nbloody), flu like symptoms, and stomach cramps. To treat\\nDocument [2](Title: FDA Issues Warning About Eating Raw Cookie Dough, But Not For Salmonella\\nRisks) FDA Issues Warning About Eating Raw Cookie Dough, But Not For Salmonella Risks Used to\\nlicking the spoon or placating yourself with full-on chunks of raw cookie dough? The Food and\\nDrug Administration issued a warning on Tuesday that strongly advises against continuing the\\nhabit. The agency asserted that consuming raw batter of any kind, whether for bread, cookies or\\npizza, could make a person sick. While you may have been warned in the past against eating raw\\ndough due to the risk of contracting salmonella from raw eggs, the FDA is citing raw flour as the\\nculprit for a\\nDocument [3](Title: It’s Probably OK to Eat Raw Cookie Dough — As Long As You’re Smart About It -\\nThe Crux - Very Top Secret Information) First, when most people think about health risks and\\ncookie dough, they think about raw egg. Eggs can be contaminated with salmonella bacteria, and\\nfood safety recommendations encourage people to cook eggs until the white and yolk are firm in\\norder to kill any bacteria. However, anyone making cookies can do things to reduce this risk by\\nusing pasteurized egg products. When my kids and I make cookie dough, we never use regular eggs.\\nInstead, we use shell eggs that have been pasteurized to kill any harmful bacteria without\\nactually cooking the egg itself. (A great public health innovation, if\\nDocument [4](Title: How Dangerous Is It to Eat Raw Cookie Dough? | Men’s Health) Can Eating Raw\\nCookie Dough Really Make You Sick? Scientists reveal the truth about this supposedly dangerous\\ndelicacy By Katherine Dempsey There are few things more tempting in life than eyeing a bowl of\\ncookie dough and deciding whether or not to stick your finger in for a scoop. It’s a bit like\\nplaying Russian roulette. You could get lucky and enjoy the delicious dough without conseqence,\\nbut there’s always the risk of getting serously sick with a food-borne illness. That’s because\\nmultiple ingredients within the dough could be contaminated by pathogens such as Salmonella and\\nE. coli, says Soohyoun\\nDocument [5](Title: How Dangerous Is It to Eat Raw Cookie Dough? | Men’s Health) is usually\\npasteurized, so it’s not likely that the egg would make you sick. However, other ingredients in\\nthe dough could potentially harbor pathogens. Experts say that a prime suspect in a 2009 E. coli\\noutbreak linked to prepackaged cookie dough was actually the flour. Nuts and chocolate have also\\nbeen linked to Salmonella outbreaks. Bottom line: You’re better off skipping raw cookie dough.\\nAt least you’ll have killer cookies as a consolation. The article How Bad Is It To Eat Raw Cookie\\nDough? originally ran on Prevention.com The 9 Foods Most Likely to Make You Sick Mmm, Just Don’t\\n...\\nQuestion: Why is it bad to eat cookie dough for risk of salmonella but things like Cookie Dough\\nBites are ok?\\nAnswer: Raw cookie dough is not recommended to be eaten due to the risk of salmonella [1][2].\\nEating raw flour is also a risk for food poisoning [2]. The egg in cookie dough is usually\\npasteurized, so it’s not likely that the egg would make you sick, but other ingredients in the\\ndough could potentially harbor pathogens [4][5]. However, prepackaged cookie dough like Cookie', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 22}), Document(page_content='pasteurized, so it’s not likely that the egg would make you sick, but other ingredients in the\\ndough could potentially harbor pathogens [4][5]. However, prepackaged cookie dough like Cookie\\nDough Bites is safe to eat because the dough is made with pasteurized egg products and\\nheat-treated flour [2][3]..\\nGround truth claims:\\n1. Cookie Dough Bites are safe to eat since they do not contain eggs.\\n2. Salmonella is a bacteria found in eggs.\\n3. Eating raw cookie dough carries the risk of salmonella infection.\\nTable 30: ELI5 example 1.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 22}), Document(page_content='Instruction: Write an accurate, engaging, and concise answer for ...\\nDocument [1](Title: Is Snapchat really worth $19 billion? - CSMonitor.com) reporting that the Los\\nAngeles-based company is aiming to raise $500 million at a valuation of $16 billion to $19 billion,\\nmaking it the third most highly valued tech start-up backed by venture capitalists. The Chinese\\nhandset maker Xiaomi is valued at $45 billion, while Uber is estimated to be valued at about $40\\nbillion, according to data from CB Insights. Read MoreVC investment hits $86B thanks to Uber, Xiaomi\\nSnapchat was valued at $10 billion in August, according to a Dow Jones report. Some of its investors\\nfrom previous rounds include Benchmark, Lightspeed Venture Partners and Kleiner Perkins Caufield\\nDocument [2](Title: What Are Venture Capital Investments? – DollarsAndSense.my) Ever wondered how\\nhighly valued technology giants like Google and Facebook were able to grow so fast and pay their\\nemployees so well in such a short amount of time, or how still growing start-ups like Uber are able\\nto lose 1.2 billion US dollars in just the first half of this year alone and still command a\\nvaluation upwards of 50 billion US dollars? The answer lies with a special category of investment\\nactivity known as venture capital. Venture capitalists are professional investors who invest in a\\nnumber of highly scalable high-risk technology ventures hoping to make a multi-fold\\nDocument [3](Title: Opinion | What Dara Khosrowshahi Must Do to Save Uber - The New York Times) at a\\ndiscount. These are troubling signs. Every start-up must one day fulfill the market’s demand that\\nit turn a profit, but Uber has never figured out how to do that. While ride sharing in some form\\nwill probably survive, it’s more likely that without some drastic changes, Uber won’t be around in\\nthree to five years. Mr. Khosrowshahi must avoid the mistakes of his predecessor by accepting that\\n“pivots” (Silicon Valley-speak for the desperate changes troubled companies make to reassure their\\nventure capitalist funders) are not the answer. None of the pivots Mr. Kalanick tried — like\\non-demand delivery\\nDocument [4](Title: Snapchat raising funding round at $19 billion valuation: Report) Snapchat\\nraising funding round at $19 billion valuation: Report The mobile messaging app Snapchat is said to\\nbe seeking funding at a $19 billion valuation, according to a report, citing a person with knowledge\\nof the matter. Bloomberg is reporting that the Los Angeles-based company is aiming to raise $500\\nmillion at a valuation of $16 billion to $19 billion, making it the third most highly valued tech\\nstart-up backed by venture capitalists. The Chinese handset maker Xiaomi is valued at $45 billion,\\nwhile Uber is estimated to be valued at about $40 billion, according to data from CB Insights.\\nDocument [5](Title: Unicorns And Wall Street | MoneyTips) not start showing profits soon. Even Uber,\\nwith its massive $62-billion-plus valuation, will test investor’s patience if it does not start\\nshowing signs of a profit in the next few years. Cracks are starting to show in terms of pre-IPO\\ndevaluations. Dropbox and Snapchat have seen a nearly one-quarter drop in their pre-IPO valuations,\\nwhile Square’s pre-IPO value has dropped by approximately one-third. Larger investors and venture\\ncapitalists are sending a message to the unicorns: Outline a solid path to profits and start\\nproducing them soon, or expect your valuation to be slashed. Diving ahead with an IPO is not\\n...\\nQuestion: How are firms like snapchat, uber etc valued so highly while still not making a profit? Do\\nventure capitalists not expect some form of repayment within a number of years?\\nAnswer: Venture capitalists invest in a number of highly scalable high-risk technology ventures\\nhoping to make a multi-fold return on their investment in a short amount of time [2]. Highly valued\\ncompanies like Snapchat, Uber, and Xiaomi are able to grow so fast and command high valuations', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 23}), Document(page_content='hoping to make a multi-fold return on their investment in a short amount of time [2]. Highly valued\\ncompanies like Snapchat, Uber, and Xiaomi are able to grow so fast and command high valuations\\nbecause they have the potential to generate significant revenue in the future [2][4]. Venture\\ncapitalists expect a return on their investment within a number of years, but they also invest in the\\nlong-term potential of the company [2]. It is important to note that every start-up must eventually\\nturn a profit, but some start-ups like Uber have not yet figured out how to do that [3][5].\\nGround truth claims:\\n1. Firms like Snapchat and Uber need to establish their brand and amass users before introducing\\nads.\\n2. Introducing ads too early can deter potential users.\\n3. Uber is reinvesting a lot of money to make their service better.\\nTable 31: ELI5 example 2.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/alce.pdf', 'page': 23}), Document(page_content='Attribute First, then Generate:\\nLocally-attributable Grounded Text Generation\\nAviv Slobodkin1*Eran Hirsch1∗Arie Cattan1Tal Schuster2Ido Dagan1\\n1Bar-Ilan University2Google Research\\n{lovodkin93, hirscheran, ariecattan}@gmail.com\\ntalschuster@google.com dagan@cs.biu.ac.il\\nAbstract\\nRecent efforts to address hallucinations in\\nLarge Language Models (LLMs) have focused\\non attributed text generation, which supple-\\nments generated texts with citations of support-\\ning sources for post-generation fact-checking\\nand corrections. Yet, these citations often point\\nto entire documents or paragraphs, burdening\\nusers with extensive verification work. In this\\npaper, we introduce a locally -attributable text\\ngeneration approach, prioritizing concise attri-\\nbutions. Our method, named “Attribute First,\\nthen Generate” , breaks down the conventional\\nend-to-end generation process into three intu-\\nitive steps: content selection, sentence plan-\\nning, and sequential sentence generation. By\\ninitially identifying relevant source segments\\n(“select first” ) and then conditioning the gen-\\neration process on them ( “then generate” ), we\\nensure these segments also act as the output’s\\nfine-grained attributions ( “select” becomes “at-\\ntribute” ). Tested on Multi-document Summa-\\nrization and Long-form Question-answering,\\nour method not only yields more concise\\ncitations than the baselines but also main-\\ntains—and in some cases enhances—both gen-\\neration quality and attribution accuracy. Fur-\\nthermore, it significantly reduces the time re-\\nquired for fact verification by human assessors.\\n1 Introduction\\nGrounded text generation, which includes tasks\\nlike summarization and question-answering, aims\\nto produce content that is derived from specific\\nsources, either user-provided or retrieved via re-\\ntrieval mechanisms (e.g., RAG; Lewis et al., 2020).\\nTo facilitate verification of models’ adherence to\\nthese sources, recent years have seen a growing\\ninterest in attributed text generation (Bohnet et al.,\\n2023), which aims to create text alongside sup-\\nporting evidence. This method enhances models’\\n*Equal contribution.credibility by enabling factuality verification. It\\nalso aids in detecting and addressing factual errors,\\na critical need given the observed frequency of such\\nerrors, termed “hallucinations”, in model outputs\\ncompared to source texts (Mishra et al., 2024a).\\nWhile these attributions are aimed to facilitate\\nfactuality evaluation by focusing people’s attention\\non relevant supporting texts, current approaches\\noften yield rather coarse attributions, pointing back\\nto whole documents or paragraphs. Such attribu-\\ntions, though better than having none, require hu-\\nman assessors to exhaustively sift through many\\nirrelevant details in the cited content, resulting in\\na time-consuming and somewhat ineffective fact-\\nchecking process. Can we do better?\\nIn this work, we extend attributed text generation\\nto also consider attribution conciseness, reformaliz-\\ning the task as Locally -attributable Text Generation\\n(see Section 2). We introduce a criterion for attribu-\\ntions to be precise , targeting only the most relevant\\ntext snippets, from specific sentences down to sub-\\nsentence spans, while avoiding non-essential de-\\ntails. This criterion complements the full coverage\\nrequirement, which stipulates that every generated\\nfact be backed by a cited snippet. As illustrated in\\nFig. 1, given a query and several documents, the\\nmodel output includes citations to relevant spans\\nwithin the source documents for each generated\\nsentence instead of broadly attributing the entire\\ndocuments (Gao et al., 2023b). These spans rep-\\nresent the minimal set of relevant source snippets\\nneeded to cover the content of the output sentences.\\nFollowing our localization criterion, we pro-\\npose a novel attribution-driven generation ap-\\nproach, aimed to ensure both conciseness and\\nfull coverage , termed “Attribute First, then Gen-\\nerate” . Rather than jointly producing text with', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 0}), Document(page_content='pose a novel attribution-driven generation ap-\\nproach, aimed to ensure both conciseness and\\nfull coverage , termed “Attribute First, then Gen-\\nerate” . Rather than jointly producing text with\\ncitations (Gao et al., 2023b) or getting the attri-\\nbution post-generation (Bohnet et al., 2023), we\\npropose to first choose relevant details from the\\nsource text(s), and then focus the entire genera-arXiv:2403.17104v2  [cs.CL]  1 Apr 2024', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 0}), Document(page_content='Eavestroughs and gutters are essentially the same thing, with eavestroughs being a speciﬁc type of gutter \\nthat is installed at the edge of a roof [1a, 2a, 3a] . An eavestrough is a trough that is connected to the eaves of \\na building, which are the sections of the roof that hang over the wall, and it is used to direct rainwater away \\nfrom the roof and towards a drainage system [2b, 3b] .What’s the difference between eavestroughs and rain gutters? \\nModel Output Prompt \\n… [a] They are more or less the same  \\nthing —gutters are simply a catch-all \\nterm for anything performing a \\nsimilar function.  That’s why you have \\ngutters near sidewalks and in bowling \\nalleys. They collect water (or bowling \\nballs) and move it elsewhere. In short,  \\n“eavestroughs” is just a word for a  \\nvery speciﬁc type of gutter: the ones  \\non your building’s eaves.  …[a] Gutter is a synonym of  \\neavestrough.  As nouns the \\ndifference between eavestrough \\nand gutter is that [b] eavestrough  \\nis a trough under the eaves of a  \\nbuilding  for draining water from \\nthe roof; gutter while gutter is a \\nprepared channel in a surface, \\nespecially at the side of a road \\nadjacent to a curb, intended for \\nthe drainage of water or … … [b] An eaves is a section of the  \\nroof that hangs over the wall  of \\nthe house, so that the wall is \\nprotected from rainwater . \\nTherefore, an eavestrough is a \\nchannel that is connected to the \\neaves, taking the water away from  \\nthe roof and  usually towards a  \\nsewer or drainage system . [a] A \\ngutter is a more general term.  …Attributed Documents \\n[1] https://rhinoexteriors.ca/2021/08/eavestroughs-… [2] https://wikidiff.com/eavestrough/gutter [3] https://raingutterprosinc.com \\nDocuments \\nPrompt: Figure 1: In our approach, demonstared here on LFQA, the input to the model assumes a question and a collection\\nof documents, which could be retrieved or provided by the user. The generated text is grounded in these docu-\\nments, accompanied with sentence-level citations to concise relevant spans from the supporting sources. Previous\\napproaches (e.g. Gao et al., 2023a,b) cite the whole source (truncated here for brevity), making it harder for readers\\nto reliably verify the generation.\\ntion process on them. Specifically, we decompose\\nthe conventional end-to-end grounded generation\\napproach into three intuitive steps, as depicted in\\nFigure 2: (1) content selection (choosing relevant\\ndetails from source texts), (2) sentence-level plan-\\nning (organizing and grouping content for sentence\\nfusion), and (3) sentence-by-sentence generation\\n(based on the selected and structured content). No-\\ntably, by dictating the generation process, the ini-\\ntially selected details de facto serve as attributions.\\nThis ensures a closer attribution adherence to the\\ngenerated output, as the entire generation process\\nis now guided by these attributions. It also offers\\nmore precise supporting evidence, in the form of\\nthese selected details, thereby fulfilling our objec-\\ntives of attribution accuracy and conciseness.\\nOur method, adaptable to various grounded gen-\\neration tasks, requires only slight adaptations in\\nthe content selection step to cater to specific lexi-\\ncal needs, e.g., salience in summarization or query\\nrelevance in question-answering. We showcase its\\neffectiveness in both Multi-document Summariza-\\ntion (MDS) and Long-form Question-answering\\n(LFQA), exploring both finetuning and few-shot\\nprompting strategies. Evaluations across automatic\\nand human assessments reveal that our strategy not\\nonly meets but in some cases surpasses existing\\nbaselines in generation quality while also achiev-ing high attribution accuracy. Importantly, it also\\nleads to significantly shorter citations, thereby re-\\nducing the effort needed for fact-checking by over\\n50%.\\nAltogether, our work is the first to propose local-\\nized attributions and to develop a method geared to\\nprovide such concise citations. This level of granu-\\nlarity in attribution enhances the efficiency of the', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 1}), Document(page_content='ized attributions and to develop a method geared to\\nprovide such concise citations. This level of granu-\\nlarity in attribution enhances the efficiency of the\\nfact-verification process, by focusing users’ atten-\\ntion on the most relevant details. We thus suggest\\nthat future research will build upon our paradigm\\nto further advance locally-attributed generation.\\n2 Locally-attributable Grounded Text\\nGeneration\\nThe task of Locally-attributable Grounded Text\\nGeneration involves processing a given set of doc-\\numents Dto produce an output text y. The set of\\ndocuments may either be provided by the output\\nof a retrieval system or by the user. The output,\\nconsisting of sentences s1, . . . , s n, should both ful-\\nfill the base underlying task (e.g., summarizing for\\nMDS or generating a response for LFQA) and also\\ninclude attributions C1, . . . , C npointing back to\\nsupporting text from D. For these attributions to ef-\\nfectively facilitate factuality verification by human\\nreaders, it is vital that they support as much of the\\noutput as possible, while also achieving the highest\\n2', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 1}), Document(page_content='Sentence-level Plan \\nA fire has badly damaged the \\nsupermarket  in Wellington. Attributed Fused \\nOutput \\nThe fire has destroyed a large  \\nsection  of the store. \\nFire crews are still on the  \\nscene. Pre-selected Content \\n(in context) \\nDocument Document \\nA fire has badly damaged the \\nsupermarket in Wellington. A large section of the supermarket \\nwas destroyed. [1] [2]\\nFire crews are still in Wellington. \\n[2] [3]\\n(c)\\nSentence \\nFusion (b)\\nGeneration \\nPlanning (a) \\nTask-speciﬁc \\nContent Selection Source \\nTexts \\nDocument Document Figure 2: Our attribute first process guides the model to output fluent texts that are consistent with input sources,\\nand include fine-grained sentence-level attributions to localized text spans (i.e., highlights) from the inputs.\\ngranularity in both the input and output.\\nConcretely, output granularity entails assigning\\nthe smallest information units in the generated text\\ny—in this work, individual sentences si—with a\\ncorresponding attribution set Ci, consisting of one\\nor more supporting citations cj\\ni∈Ci. Simultane-\\nously, input granularity requires each citation cj\\ni\\nto point to a specific span of text hk∈ D , pos-\\nsibly non-consecutive. Importantly, these spans\\nshould be as concise as possible while ensuring\\ncompleteness in attribution, i.e., that every piece\\nof information in the output, which in our setting\\nrefers to each output sentence si, is fully backed\\nby the union of its citations Ci. For example, in\\nFig. 1, Ciis depicted as squared brackets at the end\\nof each sentence with pointers to spans hk, shown\\nas colored highlights in D(we use “highlights” and\\n“spans” interchangeably).\\n3 Modeling\\nThis work aims to integrate source attribution\\nwith the content-grounded generation process, to\\nachieve more targeted attribution. Our approach is\\nbased on uncovering the inherent decision-making\\nmechanisms involved in text generation. We first\\ndescribe our suggested Attribute First, then Gen-\\nerate scheme in Section 3.1. Then, we propose\\ntwo strategies for this framework’s application: a\\nprompt-based in-context learning approach (Sec-\\ntion 3.2) and fine-tuning designated components\\n(Section 3.3). For each approach, we provide a\\nbrief overview of our implementation strategy for\\nthe framework’s phases, while a more in-depth elab-\\noration can be found in Appendix C.\\n3.1 Attribute First, then Generate\\nTo reliably achieve the desirable fine-grained attri-\\nbution, we introduce the Attribute First, then Gen-\\nerate paradigm, outlined in Fig. 2. This method isstructured around three key steps, mirroring the in-\\nherent decision-making processes involved in text\\ngeneration: content selection, sentence-planning,\\nand sentence-by-sentence generation. Though ap-\\nplicable to any grounded text generation task, we\\nspecifically test its application in Multi-document\\nSummarization (MDS) and Long-form Question-\\nanswering (LFQA), adapting only the content se-\\nlection step to each task’s unique requirements.\\nContent Selection The approach begins with\\nidentifying relevant spans from the source that\\nwould contribute information to the generated out-\\nput ((a) in Fig. 2). These spans then function as a\\nmore focused grounding content, guiding the lexi-\\ncal choices in the generation process. Notably, as\\nthese chosen segments constitute the primary infor-\\nmation for the resulting text, they effectively serve\\nas its (fine-grained) attribution.\\nSentence Planning This step is designed to\\nachieve sentence-level attribution in the out-\\nputthrough the introduction of an intermediate\\nsentence-level planning step ((b) in Fig. 2). Here,\\nthe selected spans from the previous step are or-\\nganized into coherently ordered clusters, where\\nthe spans grouped within each cluster naturally\\nfit within the same sentence. This step, which\\nbreaks down the constrained generation process\\ninto smaller, more manageable steps, serves two\\npurposes: it simplifies the model’s task by condi-\\ntioning each generation iteration on a single cluster', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 2}), Document(page_content='breaks down the constrained generation process\\ninto smaller, more manageable steps, serves two\\npurposes: it simplifies the model’s task by condi-\\ntioning each generation iteration on a single cluster\\nrather than the entire set of selected spans, while\\nalso leading to the desired, sentence-level attribu-\\ntion in the output , in the form of the corresponding\\ncluster’s highlights.\\nSentence-by-sentence Generation In the final\\nstep ((c) in Fig. 2), the model executes the text\\ngeneration according to the outlined plan, adopting\\na sentence-by-sentence approach to preserve the\\n3', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 2}), Document(page_content='built-in attribution from the previous steps. Specif-\\nically, it uses the next set of highlights Ci+1and\\nall preceding sentences s1:i, to generate the next\\nsentence si+1, by maximizing p(si+1|s1:i, Ci+1).\\nThis ensures each sentence expresses the content\\nof the designated highlights while also integrating\\nseamlessly with the preceding output.\\n3.2 In-context Learning\\nFor each of the substeps delineated in the previ-\\nous section, we propose a prompt-based in-context\\nlearning (ICL) strategy, where a model is guided\\nwith specific instructions and carefully selected\\nfew-shot examples. The content selection step\\n((a) in Fig. 2) is formulated as D → h1, . . . , h m,\\nwhere Dis the set of source documents and hi\\ncorresponds to a verbatim span from D. Namely,\\na model is tasked with outputting relevant spans\\nfromD, separated by a designated markup (see\\nprompt in Fig. B.1). In MDS, these spans are aimed\\nto capture salient information, while for LFQA,\\nthe model, presented with a query, is instructed to\\nidentify spans answering that query. These spans\\nare then located within the source documents via\\nstring-matching, with any undetectable span being\\nomitted (for more details, see Appendix C.1). No-\\ntably, in all subsequent steps, we “highlight” the\\nidentified spans within the document set Dvia des-\\nignated markups before and after each span, akin\\nto the methodology used in prior work (Gehrmann\\net al., 2018; Slobodkin et al., 2022, 2024).\\nFor the sentence planning step ((b) in Fig. 2), the\\ndocuments, now “highlighted”, are concatenated,\\ntruncating each at the final highlighted span to man-\\nage input length and focus the model’s attention.\\nThe model is prompted to organize h1, . . . , h minto\\nan ordered set of highlight clusters C1, . . . , C n,\\nwhere Ci={hk}|Ci|\\nk=1, to guide the subsequent gen-\\neration phase (see prompt in Fig. B.2). For more\\ndetails, see Appendix C.2.\\nFinally, in step (c), the model is guided to gen-\\nerate the output one sentence at a time, adhering\\nto the structured plan. Specifically, we separate\\nthe generation process into a series of model calls,\\neach focusing on a single cluster’s highlights, con-\\ncatenated in the same manner as in step (b). The\\nmodel is also presented with all previously gener-\\nated sentences s1:i. The task is then structured as\\nmaximizing p(si+1|s1:i, Ci+1), guiding the model\\nto produce the subsequent sentence si+1, consider-\\ning both the current cluster of highlights Ci+1andthe previously-generated sentences s1:i.1For more\\ndetails, see Appendix C.3 as well as the full prompt\\nstructure in Fig. B.3.\\nChain-of-Thought. We also explore a variation\\nthat combines the sentence-planning and sentence-\\nby-sentence generation stages ((b) and (c)), in-\\nspired by the Chain-of-Thought paradigm (CoT;\\nWei et al., 2022). Given the selected content\\nfrom step (a), the model is tasked with generat-\\ning sentence-level plans, deciding on highlights\\nto include in each synthesized sentence, alongside\\nproducing these sentences (see prompt in Fig. B.4).\\nThe model is guided to produce the final output,\\nafter forming the plan, focusing on unifying and re-\\nfining sentences for coherence. For sentence-level\\nattribution, we link each sentence in the output with\\nthe highlights from its planning stage in the CoT\\nprocess.2For more details, refer to Appendix C.3.\\n3.3 Fine-tuning\\nWe also explore fine-tuning models for the various\\nsteps of our framework, aligning the input-ouput\\nformulation of each subtask to its ICL counterpart\\nin Section 3.2. During content selection ((a) in\\nFig. 2), our model is taught to copy task-specific\\nsegments from Dverbatim, and to separate them\\nusing a special markup (see Appendix C.1 and\\nFig. B.5). To further ensure strict lexical fidelity to\\nthe source text, a constrained decoding approach\\nis adopted during inference (see Appendix C.6 for\\nmore details).\\nIn the sentence planning step ((b) in Fig. 2), we\\nuse special tokens to ’highlight’ the previously se-\\nlected spans, and teach our model to copy verbatim', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 3}), Document(page_content='more details).\\nIn the sentence planning step ((b) in Fig. 2), we\\nuse special tokens to ’highlight’ the previously se-\\nlected spans, and teach our model to copy verbatim\\nthe “highlighted” spans in a clustered and ordered\\nmanner, using special tokens to separate between\\ndifferent clusters and different highlights within a\\ncluster (see Appendix C.2 and Fig. B.6 for more\\ndetails). This phase similarly incorporates a con-\\nstrained decoding technique to maintain close lex-\\nical similarity to the highlighted content. Finally,\\nfor the sentence generation step ((c) in Fig. 2), we\\nuse special tokens to separate between the high-\\nlighted documents, the prefix, and for the LFQA\\nsetting the query (see Appendix C.3 and Fig. B.7).\\nJoint Selection and Planning. We also explore\\na joint strategy that combines content selection and\\nplan generation. Here, the entire document set D\\n1In Section 5.1 we show that conditioning on s1:iimproves\\ncoherence.\\n2We use spaCy sentencizer (Honnibal and Montani, 2017).\\n4', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 3}), Document(page_content='serves as input, and the output is an ordered series\\nof “highlight” clusters C1, . . . , C n. To ensure exact\\ncopying of source spans, we employ the aforemen-\\ntioned constrained decoding strategy. For more\\ndetails, see Appendix C.2.\\n4 Experimental Setup\\n4.1 Datasets\\nTo construct the training, development, and test\\nsplits for the different components within our\\nframework, we employ source-output alignment\\ndatasets that map corresponding spans between in-\\nputs and outputs. These alignments are adapted to\\nsuit the data requirements of each framework phase:\\nfor content selection, the alignments’ source -side\\nspans serve as the target labels; in sentence plan-\\nning, source spans corresponding to the same out-\\nput sentence are clustered, with the order of output\\nsentences determining the order of these clusters;\\nfor the sentence-by-sentence generation phase, the\\noutput sentences act as the target outputs for their\\nrespective source clusters.\\nFor MDS, we use an alignment dataset derived\\nfrom the DUC, TAC benchmarks3and the Multi-\\nNews dataset (Fabbri et al., 2019), comprising of\\npairs of summaries, corresponding source articles\\nand summary-source alignments.4For LFQA, we\\nrely on human-annotated alignments of large lan-\\nguage model responses with citations (Liu et al.,\\n2023). By filtering responses that were rated as\\nverifiable, we ensure high-quality alignment for\\nthis setting. Further details and dataset processing\\ninformation are provided in Appendix A.\\n4.2 Models and Baselines\\nOur study investigates both ICL prompt-based\\nLLMs and fine-tuned LMs.\\nIn-context Learning. For our ICL approach\\n(Section 3.2), we utilize the Gemini model (Gemini\\nTeam Google, 2023), applying designated few-shot\\nprompts tailored to each subtask.5For each setting,\\nwe also implement two baseline models for com-\\nparative analysis. For the first baseline, we prompt\\nGemini to perform the overarching task, namely\\nsummarization for MDS and answering a query for\\n3https://duc.nist.gov/\\n4https://anonymous.4open.science/r/\\nmulti-news-dataset-5B4F and https://github.com/\\noriern/SuperPAL .\\n5We used the Gemini API, which is free for academic\\npurposes: https://ai.google.dev/models/geminiLFQA, end-to-end (see Appendix C.4). This base-\\nline is primarily used to ensure the generated text’s\\nquality is not compromised in our framework. For\\nthe second baseline, we implement ALCE (Gao\\net al., 2023b), a prompt-based approach designed\\nto enable the simultaneous generation of text and\\nits attributions through in-line citations (e.g., [1] to\\ncite source 1). See Appendix C.5 for more details.\\nFine-tuned Models. For each of our proposed\\nsubtasks, we fine-tune models on our derived\\ndatasets (Section 3.3). We choose to use PRIMERA\\n(Xiao et al., 2022), a Longformer Encoder-Decoder\\nmodel (LED; Beltagy et al., 2020) optimized for\\nMDS, for both of our MDS and LFQA settings.\\nOur choice of model stems from PRIMERA ’s lead-\\ning performance in the MDS setting, attributed\\nto its capability to handle long contexts and its\\npre-training technique. Further, we found that\\nPRIMERA outperformed the Long-T5 large model\\n(Long-T5 Large; Guo et al., 2022) on our LFQA de-\\nvelopment set, where both models contain roughly\\nthe same number of parameters (Long-T5: 750M,\\nPRIMERA : 447M). Analogous to our ICL approach,\\nwe also fine-tune a variant to perform the corre-\\nsponding task comprehensively for both settings.\\n4.3 Evaluation\\nFollowing our task definition in Section 2, we\\naim to assess two aspects of the produced out-\\nputs. Firstly, we evaluate the generated texts’\\nquality, focusing on coherence and task fulfill-\\nment—specifically, summary quality in MDS tasks\\nand answer relevance in LFQA, consistent with\\nstandard evaluation practices in each respective\\ntask (Fabbri et al., 2021b; Stelmakh et al., 2022).\\nSecondly, we examine the attribution quality, look-\\ning at how well each sentence is supported by its ac-\\ncompanying citations and the conciseness of these\\ncitations (simply measured by the length of the', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 4}), Document(page_content='ing at how well each sentence is supported by its ac-\\ncompanying citations and the conciseness of these\\ncitations (simply measured by the length of the\\ncited texts). Our evaluation methodology combines\\nautomated metrics and human judgments.\\nAutomatic Evaluation. To assess compliance\\nwith each task, we employ the ROUGE L(Lin,\\n2004) and BERTSCORE (Zhang et al., 2020) met-\\nrics. These compare generated texts with reference\\noutputs, namely gold summaries for MDS and ref-\\nerence answers for LFQA.\\nFor assessing citation conciseness, we report the\\naverage length (number of tokens) of the cited con-\\ntent per output sentence. To evaluate citation qual-\\nity, we employ AUTOAIS, an NLI-based metric\\n5', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 4}), Document(page_content='Method R OUGE L↑BERTSCORE ↑AUTOAIS↑LENGTH ↓NOATT. (%) ↓ICLGEMINI 19.0 86.1 - - -\\nALCE 20.3 86.1 88.7 843.6 3.4\\nATTR. FIRST 16.7 85.2 79.5 92.7 0.0\\nATTR. FIRST CoT 19.5 86.1 72.8 75.3 1.5FTPRIMERA 19.9 85.8 - - -\\nATTR. FIRST 16.5 84.9 64.4 33.1 0.0\\nATTR. FIRST joint 18.0 85.3 48.9 57.1 0.0\\nTable 1: MDS results of both ICL and fine-tuned models. The left two columns are metrics that measure the quality\\nof the output, ignoring attributions. The right columns report the attribution quality, including the length (number of\\ntokens) of the cited text. N OATT. shows the percent of generated sentences lacking any attribution.\\nMethod R OUGE L↑BERTSCORE ↑AUTOAIS↑LENGTH ↓NOATT. (%) ↓ICLGEMINI 33.1 89.6 - - -\\nALCE 35.2 89.9 49.8 2153.3 26.9\\nATTR. FIRST 35.8 90.5 78.7 65.2 0.0\\nATTR. FIRST CoT 38.6 90.7 89.3 48.2 0.0FTPRIMERA 32.2 88.8 - - -\\nATTR. FIRST 24.7 88.0 52.7 20.9 0.0\\nATTR. FIRST joint 27.0 88.1 44.9 33.1 0.0\\nTable 2: LFQA results of both ICL and fine-tuned models. The left two columns are metrics that measure the quality\\nof the output, ignoring attributions. The right columns report the attribution quality, including the length (number of\\ntokens) of the cited text. N OATT. shows the percent of generated sentences lacking any attribution.\\nfor attribution evaluation, in line with prior studies\\n(Bohnet et al., 2023; Gao et al., 2023b) which have\\nshown its correlation with human-judgment of attri-\\nbution quality. Following those works, we employ\\nthe TRUE model from Honovich et al. (2022) for\\nour NLI framework.6The final AUTOAIS score\\nis the average number of sentences predicted as\\nattributed by the classifier.\\nLastly, we also report the average number of\\nnon-attributed sentences per instance.7\\nHuman Evaluation. The evaluation of generated\\nattributed texts consists of two steps. First, anno-\\ntators are asked to evaluate text coherence on a\\n5-point Likert scale, and in the LFQA setting also\\nits helpfulness in addressing the query. Next, anno-\\ntators proceed to evaluate the quality of citations.\\nWe follow the sentence-level attribution evaluation\\nprotocol from Gao et al. (2023a), which extends the\\nAttributable to Identified Sources (AIS; Rashkin\\net al., 2022) human evaluation framework of at-\\ntribution to the sentence-level. Annotators judge\\nwhether each output sentence is fully supported\\nby its attribution, scoring 1 for full support and 0\\notherwise. The final AIS score is the average of\\n6We also examined a more recent NLI model (TrueTeacher;\\nGekhman et al., 2023), and have found TRUE to correlate\\nbetter with human judgment. See Appendix B for more details.\\n7Given the absence of reliable automatic metrics for flu-\\nency, we restrict its evaluation to the human evaluation, fol-\\nlowing standard practices (Fabbri et al., 2021a).these scores across sentences. For more details, see\\nAppendix G.\\n5 Results and Analyses\\nWe provide results of our automated evaluation in\\nTables 1 and 2 and human evaluation in Tables 3\\nand 4. Considering the resource-intensive aspects\\nof human evaluation, we limit this assessment to\\nthe ALCE baseline and our paradigm’s most ef-\\nficient model, the ICL variant ATTR. FIRST CoT,\\nchosen for its optimal balance between task perfor-\\nmance and attribution accuracy, as indicated by the\\nautomatic evaluation.\\nWe first note that our approach, which extracts\\nfocused spans from the input documents, leads to\\nsignificantly shorter attributions than the ALCE\\nbaseline, where citations coarsely point to entire\\ndocuments. For example, the ATTR. FIRST CoT\\nvariant’s citations are on average 45 times shorter\\nthan ALCE’s.8Importantly, we find that assessing\\nour paradigm’s attribution takes annotators roughly\\nhalf the time compared to those of ALCE, em-\\nphasizing the benefit of attribution localization in\\nstreamlining the manual fact-checking process.\\nIn terms of attribution accuracy, our method\\nsurpasses the ALCE baseline in the LFQA domain\\n(both on the automatic AUTOAIS and the human\\nAIS metrics), while in MDS, although ALCE', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 5}), Document(page_content='In terms of attribution accuracy, our method\\nsurpasses the ALCE baseline in the LFQA domain\\n(both on the automatic AUTOAIS and the human\\nAIS metrics), while in MDS, although ALCE\\n8See examples of fine-grained attributions generated by\\nour approach in Appendix E.\\n6', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 5}), Document(page_content='Method Fluency ↑AIS↑Time (sec) ↓\\nALCE 4.7 89.3 47\\nATTR. FIRST ICL-CoT 4.9 79.9 22\\nTable 3: Human evaluation on the MDS outputs. Flu-\\nency evaluates the text coherence. AIS assesses the\\ntext’s attribution support for each sentence, with full\\nsupport scoring 1, and 0 otherwise. Time is a measure-\\nment of the amount of seconds it took an annotator to\\nverify if a text is fully supported by its attribution.\\nMethod Fluency ↑Helpful. ↑AIS↑Time (sec) ↓\\nALCE 4.9 4.7 87.6 59\\nATTR. FIRST ICL-CoT 4.9 4.5 94.4 35\\nTable 4: Human evaluation on the LFQA outputs. Met-\\nrics are similar to Table 3. The helpfulness metric mea-\\nsures the helpfulness of responses to queries on a 5-point\\nLikert scale.\\nshows higher accuracy, it leads to twice as many\\nunattributed sentences. We also note that while in\\nthe MDS setting our method may decrease attribu-\\ntion accuracy compared to ALCE’s full-document\\ncitations, readers can always fall back to examining\\nthe full document if the provided highlights are\\ninsufficiently supporting. Indeed, analyses of the\\nhuman annotations revealed that around 42% of the\\n109 sentences deemed unsupported were partially\\nsupported, with the remainder of the attributing\\ndetails frequently found in immediate proximity to\\nthe highlighted sections (usually within the same\\nor adjacent sentences).9This observation indicates\\nthat while our sentence fusion step occasionally in-\\ncludes slightly more information than the selected\\nhighlights (later used as fine attributions), it mostly\\nadds details from the surrounding context.10\\nHence, while suggesting room for improvement\\nin strict adherence to highlights, this does not\\nsignificantly hinder fact-checking effectiveness.\\nImportantly, the promising results on attribution\\ndo not compromise the quality of outputs. Our\\nautomatic analyses indicate minimal impact on\\nsummary quality in MDS and even an improve-\\nment in LFQA, as confirmed by the ROUGE Land\\nBERTSCORE metrics. These results are further evi-\\ndent in our human evaluation, both in terms of the\\noutput’s fluency and (for LFQA) in helpfulness for\\nanswering the query. Notably, our in-context vari-\\nants tend to produce higher-quality outputs than\\nfine-tuned models. This apparent advantage, ob-\\n9See partially-attributed examples in Appendix H.\\n10This trend is also observed in our automatic evaluation,\\nwhere extending sub-sentence attributing spans to their corre-\\nsponding full sentences improves AUTOAIS. See Appendix D.\\nFigure 3: Percentage of more cohesive sentences when\\nincluding the prefix in the input across varying upper\\nlimits of sentence index in the output.\\nserved also in attribution accuracy, is likely due\\nto Gemini’s stronger ability to generalize to long-\\ncontext settings, as well as its larger model size.\\nOverall, our experiments show that our approach\\nleads to orders of magnitude shorter attributions,\\nwith little impact on the attribution accuracy and\\nthe output’s quality. We leave further improve-\\nments in fine-tuning the individual designated com-\\nponents to future research.\\n5.1 Do Models Benefit from Seeing the Prefix?\\nWe also examine whether providing models with\\nthe context of previously generated sentences in-\\ndeed aids the generation process, particularly in\\nenhancing output cohesion as defined by Maimon\\nand Tsarfaty (2023). Cohesion involves sentences\\nbeing “referentially linked” , e.g., via co-referring\\nmentions, or linked through discourse connectors.\\nTo analyze the impact of the prefix on cohesion,\\nwe modified the sentence generation prompt to ex-\\nclude the prefix and conducted the sentence fusion\\nstep (step (c) in Fig. 2) using the ICL variant for\\nboth MDS and LFQA tasks, maintaining identical\\nsentence-level plans. Starting from the second sen-\\ntence to isolate the impact of the prefix, we exam-\\nined 20 instances per task. Our analysis of 113 sen-\\ntences reveals that approximately 30% were more\\ncohesive when including the prefix. Specifically,\\n23% showed improved referential linkage, mostly', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 6}), Document(page_content='ined 20 instances per task. Our analysis of 113 sen-\\ntences reveals that approximately 30% were more\\ncohesive when including the prefix. Specifically,\\n23% showed improved referential linkage, mostly\\nattributed to better use of co-reference, and 7%\\ndemonstrated better discourse connector usage.11\\nAdditionally, our findings suggest that prefixes play\\nan increasing role in cohesion as the output pro-\\ngresses (see Fig. 3), implying that cohesion gains\\nimportance as the text develops.12\\n11See Appendix I for examples.\\n12The observed plateau in cohesion levels in LFQA is at-\\ntributed to its outputs generally not exceeding three sentences.\\n7', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 6}), Document(page_content='6 Related Work\\nA few recent works have proposed several meth-\\nods to generate text accompanied by attribution,\\nwith varying levels of granularity in the source\\nand the generated text. In Long-Form Question\\nAnswering (LFQA), the LaMDA system (Thoppi-\\nlan et al., 2022) provides attribution for the entire\\nresponse in the form of URLs pointing to entire\\ndocuments. In GopherCite (Menick et al., 2022),\\ntheentire response is attributed to fine-grained\\nsnippets from the documents. ALCE (Gao et al.,\\n2023b) provides attribution for each generated sen-\\ntence to one or multiple entire input documents.\\nSEMQA (Schuster et al., 2023) is a recent multi-\\nsource semi-extractive dataset for LFQA, which en-\\ncourages models to integrate factual spans copied\\nverbatim from the input, along with free-text con-\\nnectors to ensure coherence. This method is by\\ndesign fine-grained both at the source and the re-\\nsponse level using string matching. Yet, this semi-\\nextractive paradigm is limited in scenarios requir-\\ning a higher level of abstraction, such as Multi-\\ndocument Summarization (MDS). In contrast to all\\nthose approaches, our localized attribution achieves\\ngranularity both in the input and the output, without\\nrestricting the model to extractive text generation.\\nIn information seeking, recent works have pro-\\nposed to find attribution in post-hoc, by retrieving\\ndocuments and matching each part of the answer\\nto specific pieces of text from the retrieved doc-\\numents (Bohnet et al., 2023; Gao et al., 2023a).\\nThese methods face a combinatorial challenge be-\\ncause the relevant snippets of text in the source\\ndocuments need to be determined.\\nThe field of attributed text generation is part of a\\nbroader endeavor to address hallucinations in mod-\\nels’ outputs and increase their overall trustworthi-\\nness. In this context, much work has been done on\\ncharacterizing the sources of hallucinations (Dziri\\net al., 2022; Rawte et al., 2023; Yao et al., 2023; Li\\net al., 2024) as well as on strategies to identify and\\nmitigate them (Azaria and Mitchell, 2023; Man-\\nakul et al., 2023; Min et al., 2023; Slobodkin et al.,\\n2023b; Mishra et al., 2024b), thereby advancing\\nthe reliability of model-generated content.\\nAnother related line of research focuses on text-\\nplanning for improving output quality and reduc-\\ning hallucinations (Gehrmann et al., 2018; Narayan\\net al., 2021, 2023; Zhao et al., 2020), by introduc-\\ning an additional step of planning before the final\\ntext generation. This area of study includes effortsto decompose the generation process into distinct\\nphases, with some works focusing on the content-\\nselection step (Gehrmann et al., 2018; Cho et al.,\\n2019; Ernst et al., 2021), while others focus on\\nthe content consolidation phase (Lebanoff et al.,\\n2019, 2020; Slobodkin et al., 2022, 2023a; Hirsch\\net al., 2023). Our work also leverages this planning\\nparadigm to improve the factuality of the output.\\nHowever, unlike previous methods, our pipeline is\\ncarefully designed to also provide high-quality fine-\\ngrained attributions. Perhaps most related to our\\nmethod is the work by Ernst et al. (2022) that di-\\nvides the MDS process into detecting salient propo-\\nsitions, clustering them, and transforming them\\ninto distinct sentences. However, their method\\nresults in outputs that resemble bullet-point sum-\\nmaries, consisting of single-proposition sentences,\\nrather than producing more fluid, multi-proposition\\nsentences typical of natural text. In contrast, our\\nmethod maintains the coherence and natural flow\\nof generated texts without being restricted to single-\\nproposition sentences.\\nText-planning is also related to the Chain-of-\\nThought (CoT) approach that demonstrated im-\\nproved performance on math problems (Wei et al.,\\n2022), coding and programming puzzles (Chen\\net al., 2021; Schuster et al., 2021), and other rea-\\nsoning tasks (Saparov and He, 2023). Both ap-\\nproaches build on the idea of decomposing the gen-\\neration task to allow an initial reasoning and plan-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 7}), Document(page_content='soning tasks (Saparov and He, 2023). Both ap-\\nproaches build on the idea of decomposing the gen-\\neration task to allow an initial reasoning and plan-\\nning stage, before generating the predicted output.\\nIn CoT, the planning and output prediction are typi-\\ncally performed in a single generation call, whereas\\nin text-planning the planning stage is separated and\\ncould be performed either by the same LM or using\\ndifferent models and operations. Therefore, CoT\\nis more suitable for in-context learning whereas\\ntext-planning works well with finetuned special-\\nized models. We explore both options in this work.\\n7 Conclusion\\nIn this work, we present “Attribute First, then Gen-\\nerate” , a novel scheme for attributable grounded\\ntext generation, comprised of separating the gener-\\nation process into multiple steps, such that attribu-\\ntion is achieved as a by-product of the generation.\\nTo the best of our knowledge, we are the first to\\nprovide such fine-grained attributions in both the\\ngenerated text and the references. Through auto-\\nmatic and manual evaluations, we showed that our\\ndecomposed generation pipeline either matches or\\n8', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 7}), Document(page_content='outperforms existing baselines in terms of task-\\nspecific metrics. Importantly, our approach yields\\norders of magnitude shorter attributions, shortening\\nthe fact-checking time by almost 50%, with little\\nimpact on the attribution accuracy. While this work\\nfocused on a particular implementation of content\\nselection and generation, we encourage future ex-\\nplorations to extend our “Attribute First, then Gen-\\nerate” paradigm to other configurations. We hope\\nthat our research will motivate future work in this\\ndirection, improving the quality of text generation\\nwhile also providing helpful concise attributions to\\nexternal sources, thereby increasing models’ trust-\\nworthiness and safe downstream use.\\nLimitations\\nOur Attribute First, Then Generate approach tends\\nto use more computing resources and is also slower\\ncompared to direct end-to-end generation. This is\\nbecause it requires several steps, each involving a\\nseparate call to the model.\\nFurthermore, our pipeline is comprised of sev-\\neral interlinked components, which can potentially\\nlead to error propagation. However, this potential\\nissue is partially alleviated by the inherent robust-\\nness of the individual components. For instance,\\nshould the content selection mechanism identify\\na broad array of text fragments, including some\\nthat are irrelevant, subsequent stages in the pro-\\ncess are designed to filter out such non-essential\\ninformation.\\nEthics Statement\\nCrowdsourcing. For our human evaluation we\\nused the Amazon Mechanical Turk13(MTurk)\\ncrowdsourcing platform. Recruitment of annota-\\ntors was conducted via email invitations dispatched\\nfrom the MTurk platform, aimed at individuals who\\nhad previously demonstrated high performance in\\nsimilar research endeavors. Compensation was pro-\\nvided not only for the actual annotation tasks but\\nalso for an initial onboarding phase, which included\\nfamiliarization with the annotation guidelines and\\ngoing over feedback from the authors. The pricing\\nfor each annotation instance was calibrated based\\non an estimated completion time, aiming to main-\\ntain a compensation rate of approximately 12$ per\\nhour. Additionally, we closely monitored the actual\\ntime required for task completion and offered post-\\nhoc reimbursements in compensation for instances\\n13https://worker.mturk.com/where the required time exceeded our initial esti-\\nmate by more than 5%.\\nDatasets licenses. The DUC and TAC datasets\\nwere acquired according to the required NIST\\nguidelines ( duc.nist.gov ), and we do not pub-\\nlish any data from these datasets. The annotation\\ndata from Liu et al. (2023) is published with the\\nMIT license. The Multi-News dataset (Fabbri et al.,\\n2019) is published with a non-commercial license\\nintended for research and educational purposes. Ac-\\ncordingly, our human evaluation annotations will\\nalso be limited to non-commercial use.\\nAI assistants AI assistants were exclusively em-\\nployed to enhance the grammatical structuring of\\nthe text.\\nReferences\\nAmos Azaria and Tom Mitchell. 2023. The internal\\nstate of an LLM knows when it’s lying. In Find-\\nings of the Association for Computational Linguistics:\\nEMNLP 2023 , pages 967–976, Singapore. Associa-\\ntion for Computational Linguistics.\\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\\nLongformer: The long-document transformer.\\nBernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aha-\\nroni, Daniel Andor, Livio Baldini Soares, Massimil-\\niano Ciaramita, Jacob Eisenstein, Kuzman Ganchev,\\nJonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma,\\nJianmo Ni, Lierni Sestorain Saralegui, Tal Schus-\\nter, William W. Cohen, Michael Collins, Dipanjan\\nDas, Donald Metzler, Slav Petrov, and Kellie Webster.\\n2023. Attributed question answering: Evaluation and\\nmodeling for attributed large language models.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 8}), Document(page_content='Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\\nKaiser, Mohammad Bavarian, Clemens Winter,\\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\\nWilliam Saunders, Christopher Hesse, Andrew N.\\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\\nMorikawa, Alec Radford, Matthew Knight, Miles\\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\\nSutskever, and Wojciech Zaremba. 2021. Evaluating\\nlarge language models trained on code.\\n9', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 8}), Document(page_content='Sangwoo Cho, Logan Lebanoff, Hassan Foroosh, and\\nFei Liu. 2019. Improving the similarity measure of\\ndeterminantal point processes for extractive multi-\\ndocument summarization. In Proceedings of the 57th\\nAnnual Meeting of the Association for Computational\\nLinguistics , pages 1027–1038, Florence, Italy. Asso-\\nciation for Computational Linguistics.\\nDaniel Deutsch, Rotem Dror, and Dan Roth. 2022. Re-\\nexamining system-level correlations of automatic\\nsummarization evaluation metrics. In Proceedings of\\nthe 2022 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies , pages 6038–6052.\\nNouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and\\nSiva Reddy. 2022. On the origin of hallucinations\\nin conversational models: Is it the datasets or the\\nmodels?\\nBradley Efron. 1987. Better bootstrap confidence inter-\\nvals. Journal of the American statistical Association ,\\n82(397):171–185.\\nOri Ernst, Avi Caciularu, Ori Shapira, Ramakanth Pa-\\nsunuru, Mohit Bansal, Jacob Goldberger, and Ido\\nDagan. 2022. Proposition-level clustering for multi-\\ndocument summarization. In Proceedings of the\\n2022 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies , pages 1765–1779, Seat-\\ntle, United States. Association for Computational\\nLinguistics.\\nOri Ernst, Ori Shapira, Ramakanth Pasunuru, Michael\\nLepioshkin, Jacob Goldberger, Mohit Bansal, and\\nIdo Dagan. 2021. Summary-source proposition-level\\nalignment: Task, datasets and supervised baseline.\\nInProceedings of the 25th Conference on Computa-\\ntional Natural Language Learning , pages 310–322,\\nOnline. Association for Computational Linguistics.\\nAlexander Fabbri, Irene Li, Tianwei She, Suyi Li, and\\nDragomir Radev. 2019. Multi-news: A large-scale\\nmulti-document summarization dataset and abstrac-\\ntive hierarchical model. In Proceedings of the 57th\\nAnnual Meeting of the Association for Computational\\nLinguistics , pages 1074–1084, Florence, Italy. Asso-\\nciation for Computational Linguistics.\\nAlexander R. Fabbri, Wojciech Kry ´sci´nski, Bryan Mc-\\nCann, Caiming Xiong, Richard Socher, and Dragomir\\nRadev. 2021a. SummEval: Re-evaluating summa-\\nrization evaluation. Transactions of the Association\\nfor Computational Linguistics , 9:391–409.\\nAlexander R. Fabbri, Wojciech Kry ´sci´nski, Bryan Mc-\\nCann, Caiming Xiong, Richard Socher, and Dragomir\\nRadev. 2021b. Summeval: Re-evaluating summariza-\\ntion evaluation.\\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\\nment among many raters. Psychological bulletin ,\\n76(5):378.Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent\\nZhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and\\nKelvin Guu. 2023a. RARR: Researching and revis-\\ning what language models say, using language mod-\\nels. In Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1:\\nLong Papers) , pages 16477–16508, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\\n2023b. Enabling large language models to generate\\ntext with citations. In Proceedings of the 2023 Con-\\nference on Empirical Methods in Natural Language\\nProcessing , pages 6465–6488, Singapore. Associa-\\ntion for Computational Linguistics.\\nSebastian Gehrmann, Yuntian Deng, and Alexander\\nRush. 2018. Bottom-up abstractive summarization.\\nInProceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing , pages\\n4098–4109, Brussels, Belgium. Association for Com-\\nputational Linguistics.\\nZorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen\\nElkind, and Idan Szpektor. 2023. TrueTeacher:\\nLearning factual consistency evaluation with large\\nlanguage models. In Proceedings of the 2023 Con-\\nference on Empirical Methods in Natural Language\\nProcessing , pages 2053–2070, Singapore. Associa-\\ntion for Computational Linguistics.\\nGemini Team Google. 2023. Gemini: A family of', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 9}), Document(page_content='ference on Empirical Methods in Natural Language\\nProcessing , pages 2053–2070, Singapore. Associa-\\ntion for Computational Linguistics.\\nGemini Team Google. 2023. Gemini: A family of\\nhighly capable multimodal models. arXiv preprint\\narXiv:2312.11805 .\\nMandy Guo, Joshua Ainslie, David Uthus, Santiago On-\\ntanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.\\n2022. LongT5: Efficient text-to-text transformer for\\nlong sequences. In Findings of the Association for\\nComputational Linguistics: NAACL 2022 , pages 724–\\n736, Seattle, United States. Association for Compu-\\ntational Linguistics.\\nEran Hirsch, Valentina Pyatkin, Ruben Wolhandler, Avi\\nCaciularu, Asi Shefer, and Ido Dagan. 2023. Re-\\nvisiting sentence union generation as a testbed for\\ntext consolidation. In Findings of the Association\\nfor Computational Linguistics: ACL 2023 , pages\\n7038–7058, Toronto, Canada. Association for Com-\\nputational Linguistics.\\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\\nNatural language understanding with Bloom embed-\\ndings, convolutional neural networks and incremental\\nparsing. To appear.\\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\\nScialom, Idan Szpektor, Avinatan Hassidim, and\\nYossi Matias. 2022. TRUE: Re-evaluating factual\\nconsistency evaluation. In Proceedings of the 2022\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies , pages 3905–3920, Seattle,\\n10', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 9}), Document(page_content='United States. Association for Computational Lin-\\nguistics.\\nJ Richard Landis and Gary G Koch. 1977. The mea-\\nsurement of observer agreement for categorical data.\\nbiometrics , pages 159–174.\\nLogan Lebanoff, Franck Dernoncourt, Doo Soon Kim,\\nLidan Wang, Walter Chang, and Fei Liu. 2020.\\nLearning to fuse sentences with transformers for sum-\\nmarization. In Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language Process-\\ning (EMNLP) , pages 4136–4142, Online. Association\\nfor Computational Linguistics.\\nLogan Lebanoff, John Muchovej, Franck Dernoncourt,\\nDoo Soon Kim, Seokhwan Kim, Walter Chang, and\\nFei Liu. 2019. Analyzing sentence fusion in abstrac-\\ntive summarization. In Proceedings of the 2nd Work-\\nshop on New Frontiers in Summarization , pages 104–\\n110, Hong Kong, China. Association for Computa-\\ntional Linguistics.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\\nRetrieval-augmented generation for knowledge-\\nintensive nlp tasks. In Advances in Neural Infor-\\nmation Processing Systems , volume 33, pages 9459–\\n9474. Curran Associates, Inc.\\nJunyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng,\\nWayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.\\n2024. The dawn after the dark: An empirical study\\non factuality hallucination in large language models.\\nChin-Yew Lin. 2004. ROUGE: A package for auto-\\nmatic evaluation of summaries. In Text Summariza-\\ntion Branches Out , pages 74–81, Barcelona, Spain.\\nAssociation for Computational Linguistics.\\nNelson Liu, Tianyi Zhang, and Percy Liang. 2023. Eval-\\nuating verifiability in generative search engines. In\\nFindings of the Association for Computational Lin-\\nguistics: EMNLP 2023 , pages 7001–7025, Singapore.\\nAssociation for Computational Linguistics.\\nAviya Maimon and Reut Tsarfaty. 2023. A novel com-\\nputational and modeling foundation for automatic\\ncoherence assessment.\\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales.\\n2023. Selfcheckgpt: Zero-resource black-box hal-\\nlucination detection for generative large language\\nmodels.\\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\\nJohn Aslanides, Francis Song, Martin Chadwick,\\nMia Glaese, Susannah Young, Lucy Campbell-\\nGillingham, Geoffrey Irving, and Nat McAleese.\\n2022. Teaching language models to support answers\\nwith verified quotes.Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,\\nWen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-\\nmoyer, and Hannaneh Hajishirzi. 2023. FActScore:\\nFine-grained atomic evaluation of factual precision\\nin long form text generation. In Proceedings of the\\n2023 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 12076–12100, Singa-\\npore. Association for Computational Linguistics.\\nAbhika Mishra, Akari Asai, Vidhisha Balachandran,\\nYizhong Wang, Graham Neubig, Yulia Tsvetkov, and\\nHannaneh Hajishirzi. 2024a. Fine-grained halluci-\\nnation detection and editing for language models.\\nArXiv , abs/2401.06855.\\nAbhika Mishra, Akari Asai, Vidhisha Balachandran,\\nYizhong Wang, Graham Neubig, Yulia Tsvetkov, and\\nHannaneh Hajishirzi. 2024b. Fine-grained hallucina-\\ntion detection and editing for language models.\\nShashi Narayan, Joshua Maynez, Reinald Kim Am-\\nplayo, Kuzman Ganchev, Annie Louis, Fantine Huot,\\nAnders Sandholm, Dipanjan Das, and Mirella Lap-\\nata. 2023. Conditional generation with a question-\\nanswering blueprint. Transactions of the Association\\nfor Computational Linguistics , 11:974–996.\\nShashi Narayan, Yao Zhao, Joshua Maynez, Gonçalo\\nSimões, Vitaly Nikolaev, and Ryan McDonald. 2021.\\nPlanning with learned entity prompts for abstractive\\nsummarization. Transactions of the Association for\\nComputational Linguistics , 9:1475–1492.\\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\\nLora Aroyo, Michael Collins, Dipanjan Das, Slav\\nPetrov, Gaurav Singh Tomar, Iulia Turc, and David\\nReitter. 2022. Measuring attribution in natural lan-\\nguage generation models.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 10}), Document(page_content='Lora Aroyo, Michael Collins, Dipanjan Das, Slav\\nPetrov, Gaurav Singh Tomar, Iulia Turc, and David\\nReitter. 2022. Measuring attribution in natural lan-\\nguage generation models.\\nVipula Rawte, Prachi Priya, S. M Towhidul Islam Ton-\\nmoy, S M Mehedi Zaman, Amit Sheth, and Amitava\\nDas. 2023. Exploring the relationship between llm\\nhallucinations and prompt linguistic nuances: Read-\\nability, formality, and concreteness.\\nPaul Roit, Ayal Klein, Daniela Stepanov, Jonathan\\nMamou, Julian Michael, Gabriel Stanovsky, Luke\\nZettlemoyer, and Ido Dagan. 2020. Controlled\\ncrowdsourcing for high-quality QA-SRL annotation.\\nInProceedings of the 58th Annual Meeting of the As-\\nsociation for Computational Linguistics , pages 7008–\\n7013, Online. Association for Computational Lin-\\nguistics.\\nAbulhair Saparov and He He. 2023. Language models\\nare greedy reasoners: A systematic formal analysis\\nof chain-of-thought. In The Eleventh International\\nConference on Learning Representations .\\nTal Schuster, Ashwin Kalyan, Alex Polozov, and Adam\\nKalai. 2021. Programming puzzles. In Proceedings\\nof the Neural Information Processing Systems Track\\non Datasets and Benchmarks , volume 1. Curran.\\nTal Schuster, Adam D. Lelkes, Haitian Sun, Jai Gupta,\\nJonathan Berant, William W. Cohen, and Donald\\n11', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 10}), Document(page_content='Metzler. 2023. Semqa: Semi-extractive multi-source\\nquestion answering.\\nAviv Slobodkin, Avi Caciularu, Eran Hirsch, and Ido Da-\\ngan. 2023a. Don’t add, don’t miss: Effective content\\npreserving generation from pre-selected text spans.\\nInFindings of the Association for Computational\\nLinguistics: EMNLP 2023 , pages 12784–12800, Sin-\\ngapore. Association for Computational Linguistics.\\nAviv Slobodkin, Omer Goldman, Avi Caciularu, Ido Da-\\ngan, and Shauli Ravfogel. 2023b. The curious case\\nof hallucinatory (un)answerability: Finding truths in\\nthe hidden states of over-confident large language\\nmodels. In Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing ,\\npages 3607–3625, Singapore. Association for Com-\\nputational Linguistics.\\nAviv Slobodkin, Paul Roit, Eran Hirsch, Ori Ernst, and\\nIdo Dagan. 2022. Controlled text reduction. In Pro-\\nceedings of the 2022 Conference on Empirical Meth-\\nods in Natural Language Processing , pages 5699–\\n5715, Abu Dhabi, United Arab Emirates. Association\\nfor Computational Linguistics.\\nAviv Slobodkin, Ori Shapira, Ran Levy, and Ido Dagan.\\n2024. Multi-review fusion-in-context.\\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\\nWei Chang. 2022. ASQA: Factoid questions meet\\nlong-form answers. In Proceedings of the 2022 Con-\\nference on Empirical Methods in Natural Language\\nProcessing , pages 8273–8288, Abu Dhabi, United\\nArab Emirates. Association for Computational Lin-\\nguistics.\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\\nMaarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-\\nChing Chang, Igor Krivokon, Will Rusch, Marc\\nPickett, Pranesh Srinivasan, Laichee Man, Kathleen\\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-\\nArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc\\nLe. 2022. Lamda: Language models for dialog appli-\\ncations.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\\nand Denny Zhou. 2022. Chain of thought prompt-\\ning elicits reasoning in large language models. In\\nAdvances in Neural Information Processing Systems .Wen Xiao, Iz Beltagy, Giuseppe Carenini, and Arman\\nCohan. 2022. PRIMERA: Pyramid-based masked\\nsentence pre-training for multi-document summariza-\\ntion. In Proceedings of the 60th Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers) , pages 5245–5263, Dublin,\\nIreland. Association for Computational Linguistics.\\nJia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan\\nNing, and Li Yuan. 2023. Llm lies: Hallucinations\\nare not bugs, but features as adversarial examples.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\\nating text generation with bert.\\nChao Zhao, Marilyn Walker, and Snigdha Chaturvedi.\\n2020. Bridging the structural gap between encoding\\nand decoding for data-to-text generation. In Proceed-\\nings of the 58th Annual Meeting of the Association\\nfor Computational Linguistics , pages 2481–2491, On-\\nline. Association for Computational Linguistics.\\nA Datasets Preprocessing\\nDatasets statistics are reported in Table A.1. For\\nLFQA, we utilize published human annotations\\nof LLM responses with in-line citations (Liu et al.,\\n2023). We use the existing train, dev, and test splits,\\nand we apply a few filters on this dataset as follows.\\nResponses from LLMs can contain statements that\\nare not supported by the retrieved documents and', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 11}), Document(page_content='and we apply a few filters on this dataset as follows.\\nResponses from LLMs can contain statements that\\nare not supported by the retrieved documents and\\nare annotated with “Partial Support” or “No Sup-\\nport”. We classify each statement as supported by\\nthe input documents if either one of its citations is\\nannotated with “Full Support” or if the union of\\nall citations is annotated with “Full Support”. We\\nremove examples from the dataset containing un-\\nsupported statements. In addition, the dataset also\\nincludes human annotations for perceived utility\\n(on a Likert scale between 1 to 5). We only keep\\nexamples with perceived utility 4 and 5 (“Strongly\\nAgree” and “Agree”). Lastly, the dataset contains\\nevidence excerpts for each generated statement,\\nwhich we use as citations. We remove examples\\nfrom the dataset if we cannot align the evidence\\nexcerpt with the source documents.\\nFor the training processes in the MDS setting, we\\nuse the alignment dataset from Ernst et al. (2022),\\nautomatically extracted from the DUC and TAC\\nsummarization datasets.14For the development and\\ntest splits, we use an alignment dataset extracted\\nfrom the Multi-News summarization dataset (Fab-\\nbri et al., 2019), which was assembled in the con-\\ntext of another work, currently under peer-review.15\\n14https://duc.nist.gov/\\n15Upon publication of that work, we will disclose its name.\\n12', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 11}), Document(page_content='The dataset consists of 100 summaries with a cor-\\nresponding set of source news articles, comprising\\nan average of 3 articles per summary. Addition-\\nally, for each such pair of summaries and set of\\ndocuments, the authors trained qualified annotators\\nto identify aligning spans, following the approach\\ndelineated in Slobodkin et al. (2022).\\nMethod Train Eval Test\\nEVALUATING (LIU ET AL ., 2023) 393 44 45\\nMULTI -NEWS (FABBRI ET AL ., 2019) - 28 65\\nDUC & TAC ( duc.nist.gov ) 475 119 -\\nTable A.1: Our different datasets sizes used for training,\\ndevelopment, testing and hyperparameter-tuning.\\nB Meta-Evaluation for A UTOAIS\\nPrior work that suggested using NLI models for au-\\ntomatic attribution evaluation (Bohnet et al., 2023;\\nGao et al., 2023b,a) has specifically advocated us-\\ning the TRUE model (Honovich et al., 2022). Sub-\\nsequent developments have introduced an enhanced\\nNLI model, named TrueTeacher (Gekhman et al.,\\n2023). In our study, to determine the most appropri-\\nate NLI model for the automatic assessment of at-\\ntribution within the AUTOAIS framework, we con-\\nduct a correlation analysis between the outcomes\\nof these models and our human-generated annota-\\ntions. We also evaluate an alternative method that\\naverages the models’ probabilities for the entailing\\nlabel (’1’) across all sentences, rather than binary\\noutcomes. To calculate correlation, we use both\\nKendall-Tau and Spearman’s rank correlations, as\\nsuggested in (Deutsch et al., 2022). We also apply\\nbootstrapping (Efron, 1987) by performing 1000\\nsamplings of 150 instances (with repetition) and\\ncalculating correlation scores for each such sub-\\nset. We report the average correlation and 95%\\nconfidence intervals for each metric. Our findings,\\npresented in Table A.2, indicate that despite the ad-\\nvancements offered by the TrueTeacher model, the\\nKendall-Tau Spearman\\nτ CI τ CI\\nTRUE 0.4659 0.46-0.47 0.5215 0.51-0.53\\nTRUE prob. 0.3480 0.34-0.36 0.4182 0.41-0.43\\nTrueTeacher 0.4550 0.45-0.46 0.5130 0.50-0.52\\nTrueTeacher prob. 0.3408 0.33-0.35 0.4152 0.41-0.42\\nTable A.2: Average Kendall-Tau and Spearman’s rank\\ncorrelations ( τ) and their 95% confidence intervals (CI)\\nfor tested evaluation metrics against human judgment.\\n“prob.” refers to the variants averaging the respective\\nmodel’s probabilities of the label ’1’.TRUE model still aligns more closely with human\\njudgment. Furthermore, the approach of averag-\\ning probabilities for the label ’1’ correlates signif-\\nicantly worse for both models. Consequently, we\\nopt to continue utilizing the TRUE model for our\\nAUTOAIS evaluations.\\nC Implementation Details\\nIn this section, we describe the implementation\\ndetails for all of our models described in Sec-\\ntion 3. Each component is described separately\\nfor its ICL and fine-tuning approach. ROUGE L\\nandBERTSCORE were calculated with the evaluate\\nlibrary16(Lin, 2004).\\nFor our ICL experiments, we deploy the Gemini-\\nPro model (Gemini Team Google, 2023), last up-\\ndated on December 2023, which is available via\\nAPI calls.17In all LFQA experiments, the query is\\nincluded as part of the input for every component.\\nFor all the subtasks, as well as the baselines, we\\nperform a grid search on the optimal number of\\nin-context examples (1-3 in the MDS setting and\\n1-4 in the LFQA setting) and temperature hyper-\\nparameter (0.1,0.3,0.5,0.7,0.9) by evaluating each\\ncombination on the respective development set. For\\nall fine-tuned experiments, we fine-tune PRIMERA\\nfor both the MDS and LFQA settings. We also\\ntested training LONG T5, but found PRIMERA per-\\nforms significantly better. The number of parame-\\nters in the PRIMERA model is 447M. All fine-tuned\\nmodels were trained for 10 epochs and the best\\nmodel checkpoint was chosen by rougeL , unless\\nmentioned otherwise. The training of each model\\ntook approximately 1 hour on one Nvidia A100\\n80GB GPU. Hyperparameter training included 3\\nepochs for each model and used the following\\nsweep configuration: learning rate (max: 5e-4; min:\\n5e-8), warmup steps (max: 300; min: 0), weight', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 12}), Document(page_content='80GB GPU. Hyperparameter training included 3\\nepochs for each model and used the following\\nsweep configuration: learning rate (max: 5e-4; min:\\n5e-8), warmup steps (max: 300; min: 0), weight\\ndecay (0.0, 0.2, 0.5).\\nC.1 Content Selection\\nICL In the ICL method, we present the model\\nwith the source texts and guide it to copy spe-\\ncific segments from those texts (see Fig. B.1 for\\na prompt example in the MDS setting). We use a\\ndesignated “<SPAN_DELIM>” markup as a de-\\nlimiter between such consecutive spans, for better\\nparsing. Within the MDS framework, the model is\\ntasked with identifying salient excerpts, whereas in\\n16https://huggingface.co/docs/evaluate/en/index\\n17https://ai.google.dev/models/gemini\\n13', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 12}), Document(page_content='the context of LFQA, the model is also provided\\nwith a query, and is asked to find the parts of the\\ntexts that are relevant to answering this question.\\nFurthermore, following our analyses of each set-\\nting’s development sets, we restrict the model to an\\naverage and maximum number of words to choose\\n(an average of 200 and 100 words and a maximum\\nof 900 and 200 words for MDS and LFQA, respec-\\ntively). Lastly, to manage source texts that exceed\\nthe model’s input size limit, we truncate the fewest\\nnecessary sentences from those texts to ensure they\\nfit within the specified limit.18\\nAfter performing hyperparameter tuning, in the\\nMDS we set T= 0.1and the number of in-context\\nexamples to 2, while in LFQA we set T= 0.3and\\nthe number of examples to 4.\\nFine-tuned An example is provided in Fig. B.5.\\nThe input to the model is the set of all documents,\\nconcatenated with a special markup “<doc-sep>”,\\nwhich was also used in pre-training in PRIMERA .\\nTo fit the input in a context length of 4096, we trim\\neach document to the length of 4096 divided by\\nthe number of documents in each example. The\\noutput is a set of highlights separated with a new\\nspecial markup “<highlights-separator>”. We run\\nhyperparameter-tuning on each setting based on\\nthe development set. The hyperparameters used for\\nMDS are warmup steps of 229, weight decay of\\n0.5, and a learning rate of 4e-4. Conversely, the\\nhyperparameters used for LFQA are warmup steps\\nof 234, weight decay of 0, and a learning rate of\\n1e-4. We chose the best model checkpoint by calcu-\\nlating Intersection-over-Union (IoU) between the\\ngold highlights spans and the predicted highlights\\nspans.\\nC.2 Generation Planning\\nICL In the ICL setting, we present the model\\nwith the source text where we insert the mark-ups\\n“<highlight_start>” and“<highlight_end>” before\\nand after each pre-selected span, (see Fig. B.2 for\\nthe MDS setting). We also enumerate the highlights\\nacross all documents, and for the LFQA setting, we\\nalso add the query. The model is directed to cluster\\nhighlighted segments that are likely to co-occur in\\na sentence in the same order it would then merge\\nthem into an output. We discovered that providing\\nthe model with task-specific motivation enhances\\nits performance; for instance, in MDS, the objec-\\n18We use spaCy (Honnibal and Montani, 2017) to separate\\ndocuments into sentences.tive is to fuse highlights into a summary, whereas\\nin LFQA, it aims to formulate a response to the\\nquery. To facilitate the parsing of the model’s re-\\nsponse, we encourage it to generate its response\\nas a list of single-value dictionaries, with “clus-\\nter” as the key and a list of indices corresponding\\nto the highlights within the current cluster as the\\nvalue. For hyperparameter tuning, we divide the\\ngold “highlights” into pairs. Pairs that correspond\\nto the same target output sentence are categorized\\nas positive, whereas others are considered nega-\\ntive. Subsequently, we evaluate the F1 score to\\nassess the alignment between the predicted and\\ngold clusters. This involves calculating the True\\nPositives, False Negatives, and False Positives for\\nthe highlights pairs, based on the predicted clusters.\\nFollowing our analyses, we opt for 2 examples and\\nT= 0.5in the MDS setting, and 1 example and\\nT= 0.1in the LFQA setting.\\nFine-tuned An example is provided in Fig. B.6.\\nThere are two variants for generation planning:\\ngenerative clustering and joint. In the generative\\nclustering approach, the input to the model is the\\nset of all highlights, concatenated with a special\\nmarkup “<highlight-separator>”, which is the same\\nas the output from the content selection model.\\nThe output is then a set of clusters, where each\\ncluster is separated by a special markup “<cluster-\\nseparator>”, and inside each cluster we have a set\\nof highlights in the same format as the input. We\\nrun hyperparameter-tuning on each setting based\\non the development set. The hyperparameters used\\nfor MDS are warmup steps 0, weight decay 0, and', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 13}), Document(page_content='of highlights in the same format as the input. We\\nrun hyperparameter-tuning on each setting based\\non the development set. The hyperparameters used\\nfor MDS are warmup steps 0, weight decay 0, and\\nlearning rate 5e-5. The hyperparameters used for\\nLFQA are warmup steps 298, weight decay 0, and\\nlearning rate 2e-4. At inference time, we used a\\nbeam size of 1. We use the same logits processor\\ndescribed in Appendix C.1 to enforce the model to\\ngenerate valid highlights.\\nIn the joint approach, the input to the model is\\nthe original source documents, similar to the input\\nin the content selection model. In contrast, the\\noutput is the clustered highlights. Similar to the\\ncontent selection model, we use the same logits\\nprocessor to enforce generating valid highlights.\\nThe hyperparameters used for LFQA are warmup\\nsteps 298, weight decay 0, and learning rate 2e-4.\\nC.3 Attributed Fused Output\\nICL For the ICL variant, we iteratively show the\\nmodel only the current cluster’s highlights, in the\\n14', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 13}), Document(page_content='same manner, we do so in the generation planning\\nICL experiments (see Appendix C.2). Additionally,\\nwe present the model with what it generated to that\\npoint and guide it to generate the next sentence in\\nthe paragraph, which should cover all and only the\\nhighlighted spans. We use 3 examples and T= 0.3\\nfor the MDS setting, and 2 examples with T= 0.1\\nfor the LFQA setting.\\nFor the Chain-of-Thought approach, which\\nmerges generation planning with the fusion steps,\\nwe present the model with the “highlighted” source\\ntexts and ask it to fuse those highlights into a coher-\\nent output. Additionally, we guide the model, both\\nin the instructions and through in-context demon-\\nstrations to iteratively cluster highlights and fuse\\nthem into a sentence, before generating the final\\noutput (see Fig. B.4). We find that 3 in-context\\nexamples and T= 0.1work best for MDS, while\\nfor LFQA it is 1 example and T= 0.3that yield\\nthe best results on the development set.\\nFine-tuned An example is provided in Fig. B.7.\\nThe input to the attributable fusion model is a single\\ncluster of highlights, where highlights are separated\\nby a special markup. The output is then a single\\nsentence of the response. This model is called itera-\\ntively, and at each iteration, we include the prefix of\\nthe already generated response. The output also in-\\ncludes citations for every sentence in the generated\\nresponse based on the highlights used to generate\\nthe particular sentence. The hyperparameters for\\nthe fusion model are for MDS are warmup steps\\n54, weight decay 0.2, and learning rate 1e-4. The\\nhyperparameters used for LFQA are warmup steps\\n175, weight decay 0.5, and learning rate 2e-5.\\nC.4 End-to-end Generation\\nWe include as a baseline end-to-end generation\\nwithout decomposing the process into multiple\\nsteps.\\nICL For the ICL setting, we instruct the model to\\nperform the corresponding setting’s task, namely to\\nsummarize the given documents for MDS, and to\\nanswer the query based on the supplied document\\nfor LFQA. In addition to the varying number of\\nin-context examples, we also evaluate the model\\nin a zero-shot setting. Our analyses show that for\\nMDS, 1 example and T= 0.5give the best perfor-\\nmance, while for LFQA 3 examples and T= 0.7\\nare optimal.Fine-tuned The input to the model is the set of\\nall documents, concatenated with a special markup\\n“<doc-sep>”, which was also used in pre-training in\\nPRIMERA . Similar to our content selection, we trim\\neach document to the length of 4096 divided by the\\nnumber of documents in each example. The output\\nis simply the full response, a summary for MDS,\\nor an answer for LFQA. The hyperparameters used\\nfor both MDS and LFQA are warmup steps 27,\\nweight decay 0.2, and learning rate 4e-5.\\nC.5 ALCE Variant\\nAs another baseline, we also implement the ALCE\\napproach (Gao et al., 2023b) for attributed genera-\\ntion, where the model is guided to jointly generate\\na text with in-line citations, pointing to the support-\\ning source documents. We use the same prompt as\\nin the original paper. Additionally, we find that for\\nboth settings, 2 examples and T= 0.5yield the\\nbest results on the respective development sets.\\nC.6 Constrained Decoding\\nBoth the content selection component and the plan-\\nning component are designed to exactly copy text\\nspans from source documents. To guide the gen-\\neration to precisely follow the lexical structure of\\nthe input’s text, we employ a constrained decoding\\nstrategy for the fine-tuned models. This strategy\\nrelies on a logits processor19, allowing the model\\nto avoid certain tokens by assigning their scores to\\nminus infinity. Specifically, the model is restricted\\nfrom generating n-grams absent from the source\\ndocuments.\\nAdditionally, to optimize the models for produc-\\ning concise and informative highlights, we impose\\nadditional constraints, including setting minimum\\nthresholds on the number of words in each high-\\nlight and the overall number of highlights gener-\\nated. Based on hyperparameter-tuning we set a', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 14}), Document(page_content='additional constraints, including setting minimum\\nthresholds on the number of words in each high-\\nlight and the overall number of highlights gener-\\nated. Based on hyperparameter-tuning we set a\\nminimum of three words for each highlight in both\\nMDS and LFQA, and a minimum number of 30\\nhighlights for MDS and 5 highlights for LFQA.\\nThese constraints contribute significantly to opti-\\nmizing the performance and reliability of the fine-\\ntuned generation models.\\nLastly, in the planning component, the model\\norganizes and forms clusters, each encapsulating\\nhighlights from the given input. To prevent ex-\\ncessively large clusters, we impose a maximum\\n19https://huggingface.co/docs/transformers/\\ninternal/generation_utils#logitsprocessor\\n15', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 14}), Document(page_content='Method A UTOAIS↑AUTOAISfull sentence ↑ICLALCE 88.7 88.7\\nATTR. FIRST 79.5 80.6\\nATTR. FIRST CoT 72.8 75.0FTATTR. FIRST 64.4 80.1\\nATTR. FIRST joint 48.9 83.9\\nTable C.1: MDS results comparing our reported AutoAIS to a variant of AutoAIS where the full-sentences containing\\nthe highlights are used as premise. The difference between the two AutoAIS variants is that the former might\\ncontain non-grammatical sentences.\\nMethod A UTOAIS↑AUTOAISfull sentence ↑ICLALCE 49.8 49.8\\nATTR. FIRST 78.7 80.9\\nATTR. FIRST CoT 89.3 91.5FTATTR. FIRST 52.7 56.0\\nATTR. FIRST joint 44.9 69.2\\nTable C.2: LFQA results comparing our reported AutoAIS to a variant of AutoAIS where the full-sentences\\ncontaining the highlights are used as premise. The difference between the two AutoAIS variants is that the former\\nmight contain non-grammatical sentences.\\nthreshold of two highlights per cluster. This facil-\\nitates the generation of coherent sentences from\\neach cluster.\\nD Full-sentence A UTOAIS\\nTables C.1 and C.2 present the results on the full-\\nsentence AutoAIS vs. regular AutoAIS\\nE Fine-grained Attribution Examples\\nTable D.1 shows examples of fine-grained attribu-\\ntions generated by the ATTR. FIRST CoTvariant in\\nthe MDS setting.\\nF Human Study\\nFor our human study, we followed the Controlled\\nCrowdsourcing Methodology (Roit et al., 2020) to\\ndetect qualified annotators via Amazon Mechani-\\ncal Turk.20We reached out to potential candidates,\\nwho had exhibited high proficiency in similar tasks\\nin the past, through email invitations sent from the\\nMTurk platform. During the onboarding process,\\ncandidates reviewed the annotation guidelines and\\ncompleted a closed qualification round consisting\\nof five annotation tasks. This process, overseen by\\nan author of the paper who provided individualized\\nfeedback, required approximately 20 minutes to\\ncomplete, with participants compensated 4.5$ for\\ntheir efforts. Ultimately, six out of eleven candi-\\ndates were selected for the actual evaluations, with\\ncompensation rates set at 2$ per ALCE instance\\n20www.mturk.comand 1$ otherwise, reflecting the varying complexity\\nof the instances. To ensure fair compensation, we\\nalso monitored the actual time required to complete\\neach instance and offered post-hoc reimbursements\\nwhen the required time exceeded our initial esti-\\nmates by more than 5%.\\nMoreover, to evaluate the consistency of attribu-\\ntion annotations, we computed the inter-annotator\\nagreement (IAA) using Fleiss’ Kappa coefficient\\n(Fleiss, 1971), based on a sample of 101 sentences\\nand their attributions from 23 instances across both\\nmodels, each annotated by 3 workers.21The result-\\ning Kappa coefficient was 0.37, indicating a fair\\nlevel of agreement (Landis and Koch, 1977).\\nG Annotation Interface\\nScreenshots from our annotation interface are pre-\\nsented in Figs. E.1 and E.2. Our interface mir-\\nrors the design from Liu et al. (2023), adapted\\nto support citations with highlights. In addition,\\nthe documents are presented within the interface\\nrather than via external links. Within this inter-\\nface, annotators assess each sentence to determine\\nthe level of support provided by its accompanying\\ncitations, choosing one of the following options:\\n“Full Support”, “Partial Support”, “No Support”,\\n“Citation Has Support but also Refutes Statement”,\\n“Statement is Unclear, Can’t Make Judgement”. Fi-\\nnally, when calculating the final AIS score, we\\ntreat instances labeled as “Partial Support” as non-\\n21We treat the annotators’ scores as binary, as described in\\nAppendix G.\\n16', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 15}), Document(page_content='Prediction\\n1Generated sentence: Hawaii’s tourism industry is worth $14 billion, and hotels and resorts are offering special\\npackages to attract same-sex couples. [1]\\n[1]. . . There’s a large number of firms that are specialized in the marriage business.\" InHawaii, tourism isa$14billion\\nindustry. Now, the island chain is positioning itself for a spike in visitors - among them, Honolulu-based hotel chain\\nAqua Hospitality. Italready offersLGBT travel deals, includingthe\"out andproud\" pack ageandthe\"civil unions\\nromance special.\" Bigger beachfront resorts are also getting in on the action. The Sheraton Waikiki is the only hotel\\non Oahu providing on-site marriage licenses. General Manager Kelly Sanders says now the hotel is running a new ad\\ncampaign in mainland LGBT publications . . .\\n2Generated sentence: Patten provided lobbying and consulting services to a Ukrainian oligarch and his political party,\\nbut failed to register as a foreign agent as required by law. [1,2]\\n[1]. . . From 2014, Pattenprovided a\"promi nent\" Ukrainian oligarch who isn’t named incourt papers andhisOpposi-\\ntionBloc politicalparty with lobbyingandconsultingservices, according to the criminal information. A company\\nPatten co-owned with a Russian national received more than $1 million for the work, the U.S. said. Details about the\\nUkrainian oligarch match those of Sergei Lyovochkin, an Opposition Bloc leader whom prosecutors have previously\\nidentified as funding Manafort’s work in Ukraine. He couldn’t immediately been reached for comment. Aspartofhis\\nlobbyingwork, Pattenviolated theForeign Agents RegistrationActbynotdisclosingthework totheU.S., prosecutors\\nsaid. No date has been set for his sentencing . . .\\n[2]. . . Prosecutors say Patten, who formed a consulting company with a person identified only as \"Foreigner A,\"\\nworked to set up meetings with members of Congress and also drafted talking points for Capitol Hill meetings.\\nThegoal, prosecutorssay,wastoinfluence U.S. policy,butthey sayPattennever filed undertheForeign Agents\\nRegistrationAct. The law is aimed at promoting transparency about lobbying efforts in the United States. . . .\\n3Generated sentence: On Tuesday, a shooting at a Home Depot store in North Dallas left three people injured: two\\npolice officers, Crystal Almeida and Rogelio Santander, and a Home Depot loss-prevention officer, Scott Painter.\\n[1,2,3]\\n[1]. . .Thewounded officerisCrys talAlmeida, 26.Thethird victimwasidentifiedasScott Painter, aHome Depot\\nloss-preventionofficer.Almeida andPainter were stillincriticalconditiononWednes day, but they were making\\n\"remarkable recoveries,\" Dallas Police Chief U. Renee Hall said at a news conference Wednesday morning. . . .\\n[2]. . . Along with Officer Santander, OfficerCrys talAlmeida andtheHome Depotsecurityguard, now identifiedas\\nScott Painter, allunderwent surgery aftertheshoot ing. Officer Almeida and Mr. Painter both remain hospitalized.\\nDuring the press conference Chief Hall said, \"We are happy to report that Officer Crystal Almeida and our loss-\\nprevention officer Scott Painter is making remarkable recovery. They are still in critical condition but we are optimistic\\nabout what we’re seeing with them right now.\" Theman who policesayshot theofficers andthesecurityguard -\\nArmando Luis Juarez -woke upbehind bars today. After the shooting, a manhunt, and late night police chase, Juarez\\nwas taken into custody and ultimately transferred to the Dallas County Jail very early Wednesday morning. Officers\\nSantander andAlmeida both joined theDallasPoliceDepartment there years ago, assigned totheNorth eastDivision.\\n. . .\\n[3]. . . Mayor Mike Rawlings was presiding over a city council meeting when he announced the death of Rogelio\\nSantander, a member of the police force for three years. Santander, officerCrys talAlmeida andaloss-prevention\\nofficerforHome Depotwere shot Tues daybyaman identifiedbypoliceas29-year-oldArmando Luis Juarez. The', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 16}), Document(page_content='officerforHome Depotwere shot Tues daybyaman identifiedbypoliceas29-year-oldArmando Luis Juarez. The\\ntwoofficersandthestore loss-preventionofficerunderwent surgery fortheir injuries aftertheshoot inginthenorth of\\nthecity, Dallas Police Chief U. Renee Hall said late Tuesday. Almeida andtheloss-preventionofficer,who hasn’t\\nbeen identified, were incriticalconditionWednes day. . . .\\nTable D.1: Attribution examples for several sentences generated by the ATTR. FIRST CoTvariant for the MDS\\nsettings.\\n17', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 16}), Document(page_content='attributed , while instances classified as “Citation\\nHas Support but also Refutes Statement” are la-\\nbeled as attributed , akin to Gao et al. (2023a).\\nH Partially-attributing Examples\\nTable H.1 demonstrates a few attribution exam-\\nples labeled by human annotators as partially-\\nsupporting.\\nI With Prefix Versus without Prefix\\nTable H.2 presents several instances from our test\\nset when the iterative sentence-level fusion was\\nonly conditioned on each iteration’s cluster, versus\\nwhen it was also conditioned on the prefix.\\n18', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 17}), Document(page_content='Prediction\\n1Output: Despite economic challenges and low approval ratings , Obama’s campaign finance report shows he can still\\nattract major cash.\\nAttribution: . . .Thepresident’s campaign finance report shows hecanstillpullinmajorcash despite astagnant\\necon omy, dipping approval ratings and grumblings among some liberal supporters that he has not done enough for\\ntheir cause. . . .\\n2Output: The Sheraton Waikiki is the only hotel on Oahu that provides on-site marriage licenses , and it is running\\na new ad campaign in mainland LGBT publications.\\nAttribution: . . . The Sheraton Waikiki is the only hotel on Oahu providing on-site marriage licenses. GeneralManager\\nKelly Sanders says now thehotelisrunning anew adcampaign inmain land LGBT publications. . . .\\n3Output: Tarek and his wife Christina tried for two years to have a son, going through two failed IVF attempts and a\\nmiscarriage before welcoming their son Brayden James in August.\\nAttribution: . . .Thecouple,who arealready parentstodaugh terTaylor,tried formore than twoyears togetpregnant\\nwith their son, goingthrough twofailed attempts atIVF - including one resulting in a miscarriage at eight weeks, just\\nbefore Tarek’s diagnosis. Now inremission, thenew dad, who welcomed sonBray denJames with Christina, 32,in\\nAugust, ispositiveabout hisfamily’s future.\" . . .\\nTable H.1: Example predictions of the ATTR. FIRST CoTwhich were annotated with partial support. The information\\ninferred from the words in bold is not supported by the highlights provided by the model, but is supported by\\nsentences in close proximity.\\nPrediction\\n1w/ prefix: The Ohio River starts at the confluence of the Allegheny and Monongahela Rivers in Pittsburgh, Pennsyl-\\nvania, and flows 981 miles to its ending point in Cairo, Illinois. The river flows in a southwesterly direction from\\nwestern Pennsylvania to its mouth on the Mississippi River at the southern tip of Illinois.\\nw/o prefix: The Ohio River starts at the confluence of the Allegheny and Monongahela Rivers in Pittsburgh,\\nPennsylvania, and flows 981 miles to its ending point in Cairo, Illinois. The Ohio River is a 981-mile (1,579 km)\\nlong river in the United States.\\n2w/ prefix: Idina Menzel and Taye Diggs, who starred together in the Broadway musical \"Rent,\" have decided to end\\ntheir marriage. Their main concern is their son. The couple , who met while starring in the Broadway musical \"Rent,\"\\nhave a 4-year-old son, Walker Nathaniel. Diggs and Menzel , both 42, met as costars in the original production of the\\nBroadway musical Rent in the mid-1990s. They worked at their marriage, going through tough times like everyone\\nelse, but they loved each other very much.\\nw/o prefix: Idina Menzel and Taye Diggs, who have been married for 10 years, have a 4-year-old son named Walker.\\nIdina Menzel and Taye Diggs have asked for privacy during this time, as their main concern is for their son. Taye\\nDiggs and Idina Menzel have decided to separate after ten years of marriage, but their main concern is for their son.\\nTaye Diggs and Idina Menzel , who met in the 1990s while starring in the Broadway musical Rent, have asked for\\nprivacy during this difficult time. Taye Diggs and Idina Menzel , who met in the mid-1990s while starring in the\\noriginal production of the Broadway musical Rent, married in 2003 and had a son, Walker, in September 2009.\\n3w/ prefix: By exposing students to different religious texts, practices, cultures, and social teachings, they can gain\\na deeper understanding of the world around them and develop critical thinking skills. However , some Americans\\nbelieve that teaching religious doctrine in public schools infringes on the First Amendment right to free exercise of\\nreligion.\\nw/o prefix: By exposing students to different religious texts, practices, cultures, and social teachings, they can gain a\\ndeeper understanding of the world around them and develop critical thinking skills. Some Americans believe that', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 18}), Document(page_content='deeper understanding of the world around them and develop critical thinking skills. Some Americans believe that\\nexcluding religious sentiment from public schools infringes on the First Amendment right to free exercise of religion.\\nTable H.2: Example predictions of the ICL A TTR. FIRST model when prompted with prefix and without prefix.\\n19', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 18}), Document(page_content='1In this task, you are presented with several documents, which need to be summarized. As an\\nintermediate step, you need to identify salient content within the documents. For each\\ndocument, copy verbatim the salient spans, and use <SPAN_DELIM> as a delimiter between\\neach consecutive span. IMPORTANT: The output must be of the format Document [<DOC_ID>]:\\n<SPAN_DELIM>-delimited consecutive salient spans. IMPORTANT: Each salient content must\\nbe a single consecutive verbatim span from the corresponding passages. IMPORTANT: make\\nsure the total number of copied words (from all documents) is around 200 words, and not\\nmore than 900.\\n2\\n3Document [1]: The United States on Wednesday warned US citizens of a possible spike in\\n\"terrorist activity\" in Colombia in the coming week as the left-wing Revolutionary\\nArmed Forces of Colombia (FARC) mark the one-year anniversary of the death of one of\\ntheir commanders ...\\n4Document [2]: Colombia \\'s hardline government said Friday it is willing to meet with\\nmembers of the country \\'s main leftist rebel group in an unprecedented offer aimed at\\nfreeing dozens of rebel hostages, including three Americans. Before any face-to-face\\nmeeting can take place, however, the rebels must agree to swap 15 hostages for 15\\nrebels jailed on minor charges, said Peace Commissioner Luis Carlos Restrepo ...\\n5...\\n6\\n7Answer:\\n8Document [1]: The United States on Wednesday warned US citizens <SPAN_DELIM> mark the\\none-year anniversary of the death of one of their commanders <SPAN_DELIM> ...\\n9Document [2]: Colombia \\'s hardline government said <SPAN_DELIM> it is willing to meet with\\nmembers of the country \\'s main leftist rebel group <SPAN_DELIM> ...\\n10 ...\\n11\\n12 In this task, you are presented with several documents, which need to be summarized. As an\\nintermediate step, you need to identify salient content within the documents. For each\\ndocument, copy verbatim the salient spans, and use <SPAN_DELIM> as a delimiter between\\neach consecutive span. IMPORTANT: The output must be of the format Document [<DOC_ID>]:\\n<SPAN_DELIM>-delimited consecutive salient spans. IMPORTANT: Each salient content must\\nbe a single consecutive verbatim span from the corresponding passages. IMPORTANT: make\\nsure the total number of copied words (from all documents) is around 200 words, and not\\nmore than 900.\\n13\\n14\\n15 Document [1]: Voters in 11 states will pick their governors tonight, and Republicans\\nappear on track to increase their numbers by at least one, with the potential to\\nextend their hold to more than two-thirds of the nation \\'s top state offices ...\\n16 Document [2]: Eight of the gubernatorial seats up for grabs today are now held by\\nDemocrats; three are in Republican hands. Republicans currently hold 29\\ngovernorships, Democrats have 20 ...\\n17 ...\\n18\\n19 Answer:\\nFigure B.1: Example prompt for the content selection subtask in the MDS setting.\\n20', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 19}), Document(page_content='1In this task, you are presented with several passages, where some parts are \"highlighted\"\\n(namely, there are <highlight_start> and <highlight_end> tokens before and after each\\nsuch span). The goal is to fuse all those highlights into a single summary. As an\\nintermediate step, you need to cluster highlights that can be merged into a sentence\\n(namely, each cluster will be later merged into one sentence). Make sure the clusters\\nare in the same order you would then write the corresponding summary sentences.\\nIMORTANT: make sure there are at least 7 clusters, and no more than 5-6 highlights per\\ncluster. IMPORTANT: The output must be of the format [\"cluster\":[comma-delimited\\nhighlights indices]]\\n2\\n3Document [1]: <highlight_start>The United States on Wednesday warned US\\ncitizens<highlight_end> of a possible spike in \"terrorist activity\" in Colombia in\\nthe coming week as the left-wing Revolutionary Armed Forces of Colombia (FARC)\\n<highlight_start>mark the one-year anniversary of the death of one of their\\ncommanders<highlight_end> ...\\n4Document [2]: <highlight_start>Colombia \\'s hardline government said<highlight_end> Friday\\n<highlight_start>it is willing to meet with members of the country \\'s main leftist\\nrebel group<highlight_end> in an unprecedented offer aimed at freeing dozens of rebel\\nhostages, including three Americans. Before any face-to-face meeting can take place,\\nhowever, the rebels must agree to swap 15 hostages for 15 rebels jailed on minor\\ncharges, said Peace Commissioner Luis Carlos Restrepo ...\\n5...\\n6\\n7The highlighted spans are:\\n8Document [1]:\\n9 1. The United States on Wednesday warned US citizens\\n10 2. mark the one-year anniversary of the death of one of their commanders\\n11 ...\\n12 Document [2]:\\n13 7. Colombia \\'s hardline government said\\n14 8. it is willing to meet with members of the country \\'s main leftist rebel group\\n15 ...\\n16\\n17 Answer: The highlighted spans are clustered as follows:\\n18 [{\"cluster\":[1,2,5]}, {\"cluster\":[7,8]} ...]\\n19\\n20 In this task, you are presented with several passages, where some parts are \"highlighted\"\\n(namely, there are <highlight_start> and <highlight_end> tokens before and after each\\nsuch span). The goal is to fuse all those highlights into a single summary. As an\\nintermediate step, you need to cluster highlights that can be merged into a sentence\\n(namely, each cluster will be later merged into one sentence). Make sure the clusters\\nare in the same order you would then write the corresponding summary sentences.\\nIMORTANT: make sure there are at least 7 clusters, and no more than 5-6 highlights per\\ncluster. IMPORTANT: The output must be of the format [\"cluster\":[comma-delimited\\nhighlights indices]]\\n21\\n22\\n23 Document [1]: <highlight_start>Voters in 11 states will pick their governors\\ntonight<highlight_end>, and Republicans appear on track to increase their numbers by\\nat least one, with the potential to extend their hold to more than two-thirds of the\\nnation \\'s top state offices ...\\n24 Document [2]: Eight of the gubernatorial seats up for grabs today are now held by\\nDemocrats; three are in Republican hands. <highlight_start>Republicans currently hold\\n29 governorships, Democrats have 20<highlight_end> ...\\n25 ...\\n26\\n27 The highlighted spans are:\\n28 Document [1]:\\n29 1. Voters in 11 states will pick their governors tonight\\n30 ...\\n31 Document [2]:\\n32 5. Republicans currently hold 29 governorships, Democrats have 20\\n33 ...\\n34 Answer: The highlighted spans are clustered as follows:\\nFigure B.2: Example prompt for the clustering subtask in the MDS setting.\\n21', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 20}), Document(page_content='1In this task, you are presented with several passages, where some parts are \"highlighted\"\\n(namely, there are <highlight_start> and <highlight_end> tokens before and after each\\nsuch span). You are also presented with a prefix of a paragraph. You job is to generate\\nthe next sentence in the paragraph, that covers all and only the \"highlighted\" spans.\\nMake sure it connects well with the prefix, and that it covers all and only the\\n\"highlighted\" spans.\\n2\\n3Document [1]: <highlight_start>The United States on Wednesday warned US\\ncitizens<highlight_end> of a possible spike in \"terrorist activity\" in Colombia in\\nthe coming week as the left-wing Revolutionary Armed Forces of Colombia (FARC)\\n<highlight_start>mark the one-year anniversary of the death of one of their\\ncommanders<highlight_end> ...\\n4Document [2]: Colombia \\'s hardline government said Friday it is willing to meet with\\nmembers of the country \\'s main leftist rebel group in an unprecedented offer aimed at\\nfreeing dozens of rebel hostages, including three Americans ...\\n5...\\n6\\n7Prefix: Colombia \\'s strict government agreed to meet with the Country \\'s main leftist rebel\\ngroup.\\n8\\n9The highlighted spans are:\\n10 Document [1]:\\n11 1. The United States on Wednesday warned US citizens\\n12 2. mark the one-year anniversary of the death of a terrorist commanders, American\\ncitizens recieved a warning from their government\\n13 ...\\n14 Answer:\\n15 The next sentence is:\\nFollowing the one-year anniversary of the death of one of the group \\'s commanders,\\nAmerican citizens received a warning from their government on Wednesday.\\n16\\n17 In this task, you are presented with several passages, where some parts are \"highlighted\"\\n(namely, there are <highlight_start> and <highlight_end> tokens before and after each\\nsuch span). You are also presented with a prefix of a paragraph. You job is to generate\\nthe next sentence in the paragraph, that covers all and only the \"highlighted\" spans.\\nMake sure it connects well with the prefix, and that it covers all and only the\\n\"highlighted\" spans.\\n18\\n19 Document [1]: <highlight_start>Voters in 11 states will pick their governors\\ntonight<highlight_end>, and Republicans appear on track to increase their numbers by\\nat least one, with the potential to extend their hold to more than two-thirds of the\\nnation \\'s top state offices ...\\n20 Document [2]: Eight of the gubernatorial seats up for grabs today are now held by\\nDemocrats; three are in Republican hands. <highlight_start>Republicans currently hold\\n29 governorships, Democrats have 20 ...\\n21 ...\\n22\\n23 Prefix: 29 governorships are currently held by Republicans, vs. Democrat \\'s 20.\\n24\\n25 The highlighted spans are:\\n26 Document [1]:\\n27 1. Voters in 11 states will pick their governors tonight\\n28 ...\\n29 Answer:\\n30 The next sentence is:\\nFigure B.3: Example prompt for iterative, sentence-by-sentence generation approach in the MDS setting.\\n22', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 21}), Document(page_content='1In this task, you are presented with several passages, where some parts are \"highlighted\"\\n(namely, there are <highlight_start> and <highlight_end> tokens before and after each\\nsuch span). Your job is to generate a coherent summary that covers all and only the\\n\"highlighted\" spans. Build the summary iteratively, where each time you cluster\\nhighlights to build the next sentence. Make sure ALL highlights are included.\\nIMPORTANT: make sure that the final summary consists of approximately 200 words and at\\nleast 7 sentences. IMPORTANT: The clustering steps must be of the form: \"Highlights\\n{comma-delimited highlights indices} are combined to form sentence\\n{sentence_id}:sentence_text\". Additionally, before generating the final summary, say\\n\"So the final summary is:\".\\n2\\n3Document [1]: <highlight_start>The United States on Wednesday warned US\\ncitizens<highlight_end> of a possible spike in \"terrorist activity\" in Colombia in\\nthe coming week as the left-wing Revolutionary Armed Forces of Colombia (FARC)\\n<highlight_start>mark the one-year anniversary of the death of one of their\\ncommanders<highlight_end> ...\\n4Document [2]: <highlight_start>Colombia \\'s hardline government said<highlight_end> Friday\\n<highlight_start>it is willing to meet with members of the country \\'s main leftist\\nrebel group<highlight_end> in an unprecedented offer aimed at freeing dozens of rebel\\nhostages, including three Americans ...\\n5...\\n6The highlighted spans are:\\n7Document [1]:\\n8 1. The United States on Wednesday warned US citizens\\n9 2. mark the one-year anniversary of the death of one of their commanders\\n10 ...\\n11 Document [2]:\\n12 7. Colombia \\'s hardline government said\\n13 8. it is willing to meet with members of the country \\'s main leftist rebel group\\n14 ...\\n15 Answer:\\n16 The highlighted spans are combined as follows:\\n17 Highlights 7,8 are combined to form sentence 1: Colombia \\'s strict government agreed to\\nmeet with the Country \\'s main leftist rebel group.\\n18 ...\\n19 So the final summary is: Colombia \\'s strict government agreed to meet with the Country \\'s\\nmain leftist rebel group. ...\\n20\\n21 In this task, you are presented with several passages, where some parts are \"highlighted\"\\n(namely, there are <highlight_start> and <highlight_end> tokens before and after each\\nsuch span). Your job is to generate a coherent summary that covers all and only the\\n\"highlighted\" spans. Build the summary iteratively, where each time you cluster\\nhighlights to build the next sentence. Make sure ALL highlights are included.\\nIMPORTANT: make sure that the final summary consists of approximately 200 words and at\\nleast 7 sentences. IMPORTANT: The clustering steps must be of the form: \"Highlights\\n{comma-delimited highlights indices} are combined to form sentence\\n{sentence_id}:sentence_text\". Additionally, before generating the final summary, say\\n\"So the final summary is:\".\\n22\\n23 Document [1]: <highlight_start>Voters in 11 states will pick their governors\\ntonight<highlight_end>, and Republicans appear on track to increase their numbers by\\nat least one, with the potential to extend their hold to more than two-thirds of the\\nnation \\'s top state offices ...\\n24 Document [2]: Eight of the gubernatorial seats up for grabs today are now held by\\nDemocrats; three are in Republican hands. <highlight_start>Republicans currently hold\\n29 governorships, Democrats have 20<highlight_end> ...\\n25 ...\\n26 The highlighted spans are:\\n27 Document [1]:\\n28 1. Voters in 11 states will pick their governors tonight\\n29 ...\\n30 Document [2]:\\n31 5. Republicans currently hold 29 governorships, Democrats have 20\\n32 ...\\n33 Answer:\\n34 The highlighted spans are clustered as follows:\\nFigure B.4: Example prompt for the CoT approach in the MDS setting.\\n23', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 22}), Document(page_content='1\\n2Input:\\n3Those documents were cleared earlier Thursday morning, according to spokesmen for both\\nsides. Nevertheless, the drama over the emails once marked \"\"committee confidential\"\"\\nengulfed much of the third day of Kavanaugh \\'s confirmation hearings, with Booker\\nvowing to disclose them in defiance of Senate rules.\\n4\\n5\"\"I openly invite and accept the consequences of releasing that email right now,\"\" Booker\\nsaid. \"\"\\n6\\n7Such theatrics have characterized Kavanaugh \\'s hearings, in which Democrats have\\nrepeatedly complained that Republicans have withheld documents from the committee and\\nthe public that shed important light on Kavanaugh \\'s past.\\n8...\\n9<doc-sep>\\n10 New Jersey Democrat Cory Booker said he was ready to risk expulsion from the Senate for\\nmaking public documents pertinent Kavanaugh \\'s time as a top White House aide to\\nPresident George W. Bush, but the GOP mocked him for grandstanding ahead of a\\npossible 2020 run -- saying it had already cleared the emails for release.\\n11\\n12 In a striking political gambit, Booker, backed up by Democratic Sen. Mazie Hirono of\\nHawaii, released emails -- which had been designated as \"\"committee confidential\"\" --\\nthat reference Kavanaugh \\'s position on racial profiling and thoughts on Roe v. Wade\\ndating from his time as a White House official under Bush.\\n13 ...\\n14\\n15\\n16 Output:\\n17 Those documents were cleared earlier Thursday morning, according to spokesmen for both\\nsides. Nevertheless, <highlight_start>the drama over the emails once marked\\n\"\"committee confidential\"\" engulfed much of the third day of Kavanaugh \\'s confirmation\\nhearings, with Booker vowing to disclose them in defiance of Senate\\nrules<highlight_end>.\\n18\\n19 <highlight_start>\"\"I openly invite and accept the consequences of releasing that email\\nright now,\"\" Booker said<highlight_end>. \"\"\\n20\\n21 Such theatrics have characterized Kavanaugh \\'s hearings, in which\\n<highlight_start>Democrats have repeatedly complained that Republicans have withheld\\ndocuments from the committee and the public that shed important light on Kavanaugh \\'s\\npast<highlight_end>.\\n22 ...\\n23 <doc-sep>\\n24 New Jersey Democrat Cory Booker said he was ready to risk expulsion from the Senate for\\nmaking public documents pertinent Kavanaugh \\'s time as a top White House aide to\\nPresident George W. Bush, but the GOP mocked him for grandstanding ahead of a\\npossible 2020 run -- saying it had already cleared the emails for release.\\n25\\n26 In a striking political gambit, <highlight_start>Booker<highlight_end>, backed up by\\nDemocratic Sen. Mazie Hirono of Hawaii, <highlight_start>released emails -- which had\\nbeen designated as \"\"committee confidential\"\" -- that reference Kavanaugh \\'s\\nposition<highlight_end> on racial profiling and thoughts on Roe v. Wade dating from\\nhis time as a White House official under Bush.\\n27 ...\\nFigure B.5: Example input and output for the fine-tuned content selection model in the MDS setting. Highlight start\\nand end markups are colored blue.\\n24', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 23}), Document(page_content='1Input:\\n2Those documents were cleared earlier Thursday morning, according to spokesmen for both\\nsides. Nevertheless, <highlight_start>the drama over the emails once marked\\n\"\"committee confidential\"\" engulfed much of the third day of Kavanaugh \\'s confirmation\\nhearings, with Booker vowing to disclose them in defiance of Senate\\nrules<highlight_end>.\\n3\\n4<highlight_start>\"\"I openly invite and accept the consequences of releasing that email\\nright now,\"\" Booker said<highlight_end>. \"\"\\n5\\n6Such theatrics have characterized Kavanaugh \\'s hearings, in which\\n<highlight_start>Democrats have repeatedly complained that Republicans have withheld\\ndocuments from the committee and the public that shed important light on Kavanaugh \\'s\\npast<highlight_end>.\\n7...\\n8<doc-sep>\\n9New Jersey Democrat Cory Booker said he was ready to risk expulsion from the Senate for\\nmaking public documents pertinent Kavanaugh \\'s time as a top White House aide to\\nPresident George W. Bush, but the GOP mocked him for grandstanding ahead of a\\npossible 2020 run -- saying it had already cleared the emails for release.\\n10\\n11 In a striking political gambit, <highlight_start>Booker<highlight_end>, backed up by\\nDemocratic Sen. Mazie Hirono of Hawaii, <highlight_start>released emails -- which had\\nbeen designated as \"\"committee confidential\"\" -- that reference Kavanaugh \\'s\\nposition<highlight_end> on racial profiling and thoughts on Roe v. Wade dating from\\nhis time as a White House official under Bush.\\n12 ...\\n13\\n14 Output:\\n15 the drama over the emails once marked \"\"committee confidential\"\" engulfed much of the\\nthird day of Kavanaugh \\'s confirmation hearings, with Booker vowing to disclose them\\nin defiance of Senate rules<doc-sep>Booker<highlight_separator>released emails --\\nwhich had been designated as \"\"committee confidential\"\" -- that reference Kavanaugh \\'s\\nposition<cluster_separator>\"\"I openly invite and accept the consequences of releasing\\nthat email right now,\"\" Booker said...\\nFigure B.6: Example input and output for the fine-tuned sentence planning model in the MDS setting. Highlights\\nseparators are colored blue and cluster separators are colored green.\\n1Input:\\n2the drama over the emails once marked \"\"committee confidential\"\" engulfed much of the\\nthird day of Kavanaugh \\'s confirmation hearings, with Booker vowing to disclose them\\nin defiance of Senate rules<doc-sep>Booker<highlight_separator>released emails --\\nwhich had been designated as \"\"committee confidential\"\" -- that reference Kavanaugh \\'s\\nposition\\n3\\n4Output:\\n5Day three of the Brett Kavanaugh confirmation hearing has opened with a bang: Democratic\\nSen. Cory Booker says he has ordered his staff to release a Kavanaugh email even\\nthough it \\'s supposed to remain under wraps.\\nFigure B.7: Example input and output for the fine-tuned sentence-by-sentence generation in the MDS setting.\\n25', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 24}), Document(page_content='Figure E.1: A screenshot from the first step in our human evaluation annotation interface for the LFQA setting,\\nwhich is a replication of the interface presented by Liu et al. (2023). The annotator is presented with a query and a\\nresponse and has to rate two aspects of the response on a Likert scale between 1 to 5. The first aspect is the fluency\\nand cohesiveness of the response. The second is the helpfulness of the response.\\n26', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 25}), Document(page_content='Figure E.2: A screenshot from the second step in our human evaluation annotation interface for the LFQA setting,\\nwhich is an adaption of the interface suggested by Liu et al. (2023). The annotator is presented with citations\\nprovided by the model. For each sentence, they have to rate the quality of the citations. Highlights are presented if\\navailable, with arrows to navigate between different highlights. Completed sentences are presented with a green\\narrow to indicate that the annotator can continue to the next sentence.\\n27', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/attribute firtst then generate.pdf', 'page': 26}), Document(page_content='Improving Attributed Text Generation of Large Language Models via\\nPreference Learning\\nDongfang Li, Zetian Sun, Baotian Hu, Zhenyu Liu, Xinshuo Hu, Xuebo Liu, Min Zhang\\nHarbin Institute of Technology (Shenzhen), Shenzhen, China\\n{lidongfang,hubaotian,zhangmin2021}@hit.edu.cn\\nAbstract\\nLarge language models have been widely\\nadopted in natural language processing, yet\\nthey face the challenge of generating unreliable\\ncontent. Recent works aim to reduce misin-\\nformation and hallucinations by resorting to\\nattribution as a means to provide evidence (i.e.,\\ncitations). However, current attribution meth-\\nods usually focus on the retrieval stage and\\nautomatic evaluation that neglect mirroring the\\ncitation mechanisms in human scholarly writ-\\ning to bolster credibility. In this paper, we\\naddress these challenges by modeling the at-\\ntribution task as preference learning and intro-\\nducing an Automatic Preference Optimization\\n(APO) framework. First, we create a curated\\ncollection for post-training with 6,330 exam-\\nples by collecting and filtering from existing\\ndatasets. Second, considering the high cost of\\nlabeling preference data, we further propose\\nan automatic method to synthesize attribution\\npreference data resulting in 95,263 pairs. More-\\nover, inspired by the human citation process,\\nwe further propose a progressive preference op-\\ntimization method by leveraging fine-grained\\ninformation. Extensive experiments on three\\ndatasets (i.e., ASQA, StrategyQA, and ELI5)\\ndemonstrate that APO achieves state-of-the-art\\ncitation F1 with higher answer quality.1\\n1 Introduction\\nLarge Language Models (LLMs) have demon-\\nstrated emergent abilities and have gained\\nwidespread application in Natural Language Pro-\\ncessing (NLP) (Brown et al., 2020; Wei et al., 2022;\\nOpenAI, 2022; Anil et al., 2023). For example,\\nLLMs have shown remarkable in-context learn-\\ning capabilities across a variety of domains and\\ntasks (Dong et al., 2023). Although LLMs have\\nbeen widely adopted, a prominent issue is that they\\nproduce hallucinations in certain situations (Ye\\net al., 2023a; Zhang et al., 2023). In other words,\\n1We will release the code upon publication.they generate information that sounds plausible but\\nis nonfactual, thereby limiting their applicability\\nin the real world. To mitigate hallucinations, re-\\nsearchers have resorted to grounding statements\\nin responses generated by LLMs to supported evi-\\ndence, either by providing rationales or by adding\\ncitations to the statements (Li et al., 2023a; Liu\\net al., 2023).\\nRecent works have utilized external knowledge\\nsources such as retrieved documents and knowl-\\nedge graphs for attribution (Shuster et al., 2021; Li\\net al., 2023c). Generally, these works are divided\\ninto two types: 1) the model generates an answer\\nwith citations based on the retrieved documents (Li\\net al., 2023b); 2) an answer is first generated, then\\nmodified again to add attribution references by re-\\ntrieving with query and initial answer (Gao et al.,\\n2023a). However, these works focus mainly on the\\nretrieval stage (Ye et al., 2023b) and the evaluation\\nprocess (Yue et al., 2023). Considering the selec-\\ntion of the model’s desired responses and behavior\\nfrom its very broad knowledge and capabilities, it is\\nmore necessary to optimize the generation process,\\nnot only reducing the hallucination of the original\\nanswer but also avoiding the hallucination of the\\nattribution process. On the other hand, fine-tuning\\nLLMs after pre-training can also significantly im-\\nprove performance for users’ downstream tasks.\\nFirst, given positive examples of correct behav-\\nior, supervised fine-tuning can be performed using\\nstandard likelihood-based training. Secondly, given\\npositive and negative examples (binary feedback\\nor pairwise feedback), methods such as unlikeli-\\nhood training on negative examples (Welleck et al.,\\n2020) or RLHF-PPO (Ziegler et al., 2019) can be\\nused for learning. However, these methods usu-\\nally suffer from expensive data collection process,', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 0}), Document(page_content='hood training on negative examples (Welleck et al.,\\n2020) or RLHF-PPO (Ziegler et al., 2019) can be\\nused for learning. However, these methods usu-\\nally suffer from expensive data collection process,\\nreward model training, sparse reward and text de-\\ngeneration problems, making them difficult to use\\nin practical applications (Azar et al., 2023).\\nIn this paper, inspired by the citation mecha-arXiv:2403.18381v1  [cs.CL]  27 Mar 2024', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 0}), Document(page_content='nisms in human scholarly writing (Brooks, 1986;\\nTeplitskiy et al., 2022), we address these chal-\\nlenges by conceptualizing the attribution task for\\nLLMs as preference learning and proposing an\\nAutomatic Preference Optimization (APO) frame-\\nwork, as shown in Figure 1. Initially, we assem-\\nble a curated dataset comprising 6,330 examples\\nsourced and refined from existing datasets for post-\\ntraining. This step makes the LLMs know the ba-\\nsic format and requirements of attribution. Con-\\nsidering the substantial cost and extremely time-\\nconsuming of preference pair annotations, we thus\\nintroduce an automated approach to generate at-\\ntribution preference data, yielding 95,263 pairs.\\nFurthermore, drawing inspiration from the human\\nprocess of citation and direct preference optimiza-\\ntion (Rafailov et al., 2023), we propose a progres-\\nsive preference optimization method with experi-\\nence replay bypassing the need for explicit reward\\nmodeling or reinforcement learning. We conduct\\nthe extensive experiment on three datasets (i.e.,\\nASQA, StrategyQA, and ELI5). The experiment\\nresults demonstrate that APO surpasses compared\\nbaselines across all datasets with improved cita-\\ntion F1 along with higher response quality. Our\\ncontributions are summarized as follows:\\n•To the best of our knowledge, we are the first\\nto apply preference learning for attribution\\ntasks. We also show that our method can be\\napplied under synthesized preference scenar-\\nios.\\n•We establish a full data collection pipeline\\nfor attribution tasks and will open-source our\\nall authorized data after publication for future\\nresearch.\\n•We propose a progressive preference optimiza-\\ntion method to alleviate the sparse reward\\nproblem by leveraging fine-grained informa-\\ntion. We further benchmark existing direct\\npreference optimization methods and provide\\ninsights for attribution tasks.\\n2 Related Work\\n2.1 Text Generation for Verification\\nPrior works have studied methods and evaluations\\nfor verification that identify supporting sources for\\nmodel outputs. For instance, Rashkin et al. (2021)\\nintroduce the concept of Attributable to Identified\\nSources (AIS) which transforms model outputs\\nrejectedchosenQuestion                                                   \\nAnswer    Statement1                    Statement2                    Statement3   \\nCritic\\n2_rejected2_chosenCritic\\nrejectedchosenCritic\\n   Statement1        2_chosen     \\n   Statement1        2_rejected     Preference\\nOptimizationFigure 1: A brief overview of our APO framework. We\\nshow APO in more detail in Figure 2.\\ninto standalone, interpretable propositions. The re-\\nsponse scan be attributed to a source Pif they meet\\nthe intuitive criterion “ According to P , s ”. Bohnet\\net al. (2022) adapt the AIS framework for QA sce-\\nnarios. Further, Gao et al. (2023b) extrapolate AIS\\nto evaluate generated text of LLMs with citations.\\nAdditionally, several works focus on building and\\nusing automated AIS evaluations (Honovich et al.,\\n2022; Gao et al., 2023a; Liu et al., 2023). For a\\ncomprehensive overview, please refer to Li et al.\\n(2023a). In contrast to existing approaches, our\\nwork broadens the scope of attribution beyond just\\nverifiable text generation and devises a methodol-\\nogy to enhance these attributions which frames it\\nas a preference learning problem.\\n2.2 Preference Optimization Methods\\nPreference Optimization (PO) methods signifi-\\ncantly improve generate quality to align with hu-\\nman values (Christiano et al., 2017; Ziegler et al.,\\n2019; Stiennon et al., 2020; Bai et al., 2022). It\\nusually first collects pairs of generations under the\\nsame context and a pairwise human preference\\nto indicate which generation is better. Then the\\nPO is used to optimize generating policy to gen-\\nerate better candidates from the pair. For exam-\\nple, Reinforcement Learning from Human Feed-\\nback (RLHF) is a model-based algorithm to opti-\\nmize preference learning (Ouyang et al., 2022).\\nHowever, the RLHF process is complex, time-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 1}), Document(page_content='ple, Reinforcement Learning from Human Feed-\\nback (RLHF) is a model-based algorithm to opti-\\nmize preference learning (Ouyang et al., 2022).\\nHowever, the RLHF process is complex, time-\\nconsuming, and unstable. The direct PO uses an\\noff-policy algorithm to directly optimize the gen-\\nerating policy, eliminating the need for a reward\\nmodel (Rafailov et al., 2023; An et al., 2023; Kang\\net al., 2023; Zhao et al., 2023). These approach are', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 1}), Document(page_content='more data-efficient and stable. For example, DPO\\nuses the Bradley-Terry model (Bradley and Terry,\\n1952) and log-loss, which can lead to over-fitting\\nto the preference data, especially when preference\\nis deterministic and ignores the KL-regularization\\nterm. The IPO algorithm (Azar et al., 2023) ad-\\ndresses this issue by using a root-finding MSE loss\\nto solve the problem of ignoring KL-regularization\\nwhen preference is deterministic. However, these\\nmethods fail to fully account for more fine-grained\\npreferences and that is exactly what we want to do.\\n3 Preliminary\\nThe main pipeline of preference learning usually\\nconsists of: 1) pretraining and Supervised Fine-\\nTuning (SFT), where SFT is not a must; 2) prefer-\\nence data collection; 3) preference optimization.\\nPretraining and SFT Phase Preference learning\\ntypically starts with a pretrained LLMs or LLMs\\nfine-tuned on high-quality data using maximum\\nlikelihood estimation. The final policy πrefafter\\nthis phase is represented as\\nπref≈arg max\\nπEx,y∼Dreflogπ(x) log( y|x), (1)\\nwhere Drefdenotes the training data distribution.\\nPreference Data Collection Phase After pre-\\ntraining and SFT phase, πrefis prompted by context\\nx, and generate two responses yw, yl∼πref(·|x).\\nThen x, yw, ylis labeled by humans to judge which\\nresponse is preferred and denote yw≻yl|xifyw\\nis preferred, and yl≻yw|xifylis preferred. We\\ndefine a new symbol I=I[yw≻yl|x], and all\\n<x, yw, yl, I> consist the preference dataset Dp:\\n⟨x, yw, yl, I⟩ ∼ Dp. (2)\\nPreference Optimization Phase In the final\\nphase, the prevailing method uses reinforcement\\nlearning algorithm to learn an explicit or implicit\\nreward from the preference data, and then using\\non-policy or off-policy policy gradient algorithm\\nto maximize the reward. Recently, some methods\\nhave derived the optimal policy using reward maxi-\\nmization under KL-regularization and also derive a\\nloss with optimal policy as its solution, then learn\\nthe optimal policy by minimizing the derived loss\\non empirical dataset.\\nReinforcement Learning from Human Feedback\\n(RLHF) The RLHF uses standard two-phase re-\\nward model-based reinforcement learning to maxi-\\nmize the reward. It contains two steps: 1) rewardestimation from preference data 2) reward maxi-\\nmization using PPO algorithm. It aims to maximize\\nreward with a KL constraint on the reference model\\nπref(inputs xomitted):\\nπ∗= arg max\\nπEy∼π\\x14\\nr(y)−βlogπ(y)\\nπref(y)\\x15\\n, (3)\\nwhere βis the regularization weight and r(y)is the\\nreward function learned using the Bradley-Terry\\nmodel on the preference dataset of generating y.\\nDirect Preference Optimization (DPO) DPO\\neliminates the training of reward model. It derives\\na loss on the current policy πθ(yw,ylomitted):\\nLdpo=−logσ\\x12\\nβlogπθ(yw)\\nπref(yw)−βlogπθ(yl)\\nπref(yl)\\x13\\n,(4)\\ni.e., the binary cross entropy with\\nˆpθ(yw≻yl) =σ\\x12\\nβlogπθ(yw)\\nπref(yw)−βlogπθ(yl)\\nπref(yl)\\x13\\n,\\n(5)\\nand target p(yw≻yl) = 1 . We describe more PO\\nmethods in details in Appendix A.\\n4 Methodology\\n4.1 Problem Formulation\\nFormally, consider a query qand a corpus of text\\ndocuments D. The goal is to produce an output S,\\nwhere Sis a collection of ndistinct statements:\\ns1, s2, . . . , s n. Each statement siis associated\\nwith a set of citations Ci. This set Ciis defined\\nasCi={ci,1, ci,2, . . .}, where each ci,jis a doc-\\nument from the corpus D. For application pur-\\nposes, the output from LLMs can be divided into\\nindividual statements using sentence boundaries.\\nThis approach is utilized because a single sentence\\ntypically encapsulates a coherent statement while\\nmaintaining brevity, facilitating easy verification.\\nRegarding the citation format, citations are typi-\\ncally presented in square brackets, e.g., The sun is\\nformed approximately 4.6 billion years ago [1][2] .\\nHowever, it should be noted that these citations can\\nbe attributed to specific phrases as well, not just at\\nthe end of sentences.\\nMoreover, in this paper, we define generation\\nhallucination refers to a situation where the model\\ngenerates content that is not based on factual infor-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 2}), Document(page_content='the end of sentences.\\nMoreover, in this paper, we define generation\\nhallucination refers to a situation where the model\\ngenerates content that is not based on factual infor-\\nmation and attribution hallucination means that the\\nstatement corresponding to one citation is unfaith-\\nful or not supported by the referred source content.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 2}), Document(page_content='Dataset Source # Examples\\nPost-training\\nEVIGSE Internet 3508\\nExpertQA Internet 906\\nHAGRID Wiki 1301+615(dev)\\nPreference Optimization\\nstanford_alpaca Wiki 7741\\noasst1 Wiki 2478\\nasqa Wiki 2333\\nsharegpt Wiki 2490\\nwow Wiki 3689\\ngpt4_alpaca Wiki 6679\\nflan_v2 Wiki 1693\\nTest\\nASQA Wiki 948\\nStrategyQA Wiki 490\\nELI5 Sphere 1000\\nTable 1: Statistics of data collections used at different\\nstages in the APO framework.\\n4.2 Overall Framework\\nAs shown in Figure 2, we introduce the APO frame-\\nwork to apply preference learning for attribution\\ntask. The APO framework consists of the post-\\ntraining procedure to ground the base model for\\nattribution (§4.3), and the preference optimization\\nprocedure to address both generation hallucination\\nand attribution hallucination (§4.4).\\n4.3 Post-training\\nThe goal of post-training procedure is to ensure that\\ngiven a specific question qand a corpus of text doc-\\numents D, the model can be successfully instructed\\nto generate answer Sand add citation Cifor each\\nstatement siin its response when necessary.\\nData Collection We construct the post-training\\ndata from training sets using existing attribution\\ndatasets including EVIGSE (Liu et al., 2023), Ex-\\npertQA (Malaviya et al., 2023) and HARGID (Ka-\\nmalloo et al., 2023). We select these datasets be-\\ncause they are high-quality attribution datasets with\\ndiverse domains and sources annotated by human\\nexperts or powerful LLMs. After preprocessing\\nand formatting, the final post-training data collec-\\ntion includes 6,330 samples. The pre-processing\\ndetails are shown in Appendices B and C, and the\\nstatistics of training data are shown in Table 1.\\nTraining After that, instruction Ipost, documents\\nDand question qare formatted to be the input\\nwhile answer Scomposed of multiple statements\\nis formatted as output. We tune the model using\\nautoregressive language modeling objectives, re-\\nsulting in initial generator Mg.Algorithm 1 Preference data sampling and labeling\\n1:Input Queries Q, Critic Mc, Generator Mg, Retriever R\\n2:Output Output initialized preference dataset Pinit\\n3:Pinit= {}\\n4:forq∈Qdo\\n5: Retrieve top- kpassages Dusing Rgiven q\\n6: Predicts relevant label Lrel∈ {0,1}using critic Mc\\nfor each dinDgiven q,d\\n7: Generate Sconstructed by statements {s1, s2, .., s n},\\nusing generator Mggiven (Ipost, q, D ).\\n8: forsi∈ S do\\n9: Predicts supported label Lsup∈ {0,1}for each\\nci,jusing critic Mcgiven q,d↔ci,j,si\\n10: end for\\n11: Add augmented (q, D,S,C,Lrel,Lsup)toPinit\\n12:end for\\n4.4 Preference Optimization\\nIn this section, we describe our preference opti-\\nmization procedure to enable a model-agnostic ap-\\nproach for improving the quality of generated re-\\nsponses. First, considering the cost of labeling pref-\\nerence data, we devise an automatic data collection\\nalgorithm motivated by errors where previous mod-\\nels may have misattributed. Second, we propose\\na progressive preference optimization approach to\\namplify the preference signal by using synthesized\\npreference pairs. We further apply the experience\\nreplay to alleviate the over-fitting and text degra-\\ndation phenomenon due to the distribution shift\\nintroduced by automatic data generation.\\nAutomatic Data Collection In general, at-\\ntributed text generation should be both relevant\\nand supported (Asai et al., 2023b). Being rele-\\nvant needs the reference document in the answer\\nto be helpful in handling the question. It is used to\\nmeasure whether Cprovides useful information to\\nsolve q. Being supported asks the generated text\\nbe grounded on the reference documents. It is used\\nto measure whether all of the verification-worthy\\nstatements in Sare supported by C.\\nFollowing the requirements above, we first get\\ninitial responses and related labels for each query\\nwith the Algorithm 1. The query comes from mul-\\ntiple open domain tasks or high-quality instruc-\\ntion data sets shown in Table 1. The source of\\nretrieved documents is English Wikipedia. The\\nretriever Rwe use here is gtr-t5-large2. The', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 3}), Document(page_content='tiple open domain tasks or high-quality instruc-\\ntion data sets shown in Table 1. The source of\\nretrieved documents is English Wikipedia. The\\nretriever Rwe use here is gtr-t5-large2. The\\nobjective is to generate the attributed text with rel-\\nevant andsupported labels for related documents\\nusing the critic model Mc. Here, we use pre-trained\\n2huggingface.co/sentence-transformers/\\ngtr-t5-large', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 3}), Document(page_content='selfrag_llama2_7b3asMcin Asai et al. (2023b)\\nbecause it can give fine-grained feedback using\\nreflection tokens.\\nAfter that, we generate preference pairs using\\nan automatic collection algorithm. Specifically, we\\ndetermine whether the citations Ciof each state-\\nment siof query qare all related to it based on the\\nrelevant tags. If it is all relevant, we add the cur-\\nrent statement and its preceding statements Sito\\nthe set Ptmpfor subsequent processing. For exam-\\nple, if s2meets the requirement, we add {s1, s2}\\ntoPtmp. The motivation here is that we want to\\nselect the statements that can answer the question\\nbased on the document as the initial set. Then, for\\neach entry in Ptmp, we first retrieve another top-\\nm(m≫k) documents and filter them into 10\\nirrelevant documents Dirscored by relevant logits\\npredicted by Mc. If all documents in Dirare rele-\\nvant, we use the last 10 documents as Dir. After\\nthat, we generate the positive and negative pair for\\neach statement si∈Ptmp. There are two situa-\\ntions: the statement siis fully supported by Ciand\\notherwise. For the first situation, we first expand\\nCiwith supported document by second judgment\\ninDusing Mc. Then, we generate one positive\\nstatement using q,Si−1and new Ciand two nega-\\ntive statements using q,Si−1,Dirandq,Si−1, new\\nCi, error instruction erespectively. Thus, there are\\ntwo preference pairs in this context. For the second\\nsituation, we generate one positive statement using\\nq,Si−1and new Ciand one negative statement us-\\ningq,Si−1,D− Ci, error instruction e. The full\\nprocedure is shown in Algorithm 2.\\nIn the generation of negative samples, we use\\nthe error instruction e∈ E, which defines two\\ntypes: irrelevant but supported means the gener-\\nated text siis grounded on unhelpful reference\\ndocuments Ci, while relevant but unsupported fur-\\nther has three fine-grained subtypes: 1) fabricated\\nstatement refers to the generated text contains facts\\nor information that cannot be derived from refer-\\nence documents; 2) mistaken synthesis means that\\nseveral reference documents are used, but facts or\\nlogics are mistakenly intermingled. The generated\\ntext thus contains factual error or logic error; 3)\\nunintentional omission means that reference docu-\\nments are used, but the key points are incomplete.\\nThere are no factual errors in generated text, but\\nsome information is omitted. The irrelevant but\\nsupported error derives from attribution hallucina-\\n3huggingface.co/selfrag/selfrag_llama2_7btion, whereas the relevant but unsupported error\\nis the result of generation hallucination. Note that\\nirrelevant and unsupported errors are not included,\\nsince it is more like easy negatives. The details of\\nerror instructions are in Appendix D.\\nProgressive Preference Optimization To rein-\\nforce the preference feature and alleviate sparse re-\\nward problem (Zheng et al., 2023; Lightman et al.,\\n2023), we propose a progressive preference opti-\\nmization method. Considering generations can be\\nseparated into several consecutive statements, each\\nstatement may contain hallucinations at all. The\\nentire response-level reward preference modeling\\nperforms in the global context and potentially over-\\nsights the fine-grained deterministic preferences we\\nconstructed. Hence, we use fine-grained statement-\\nlevel reward to perform preference optimization\\nto update the model in a more effective and effi-\\ncient way. Formally, assuming that deterministic\\npreference is performed at statement-level, we can\\nrewrite the preference optimization loss in Eqn. (4)\\nas follows ( −logσomitted):\\nL≜βlogπθ(yw)\\nπref(yw)−βlogπθ(yl)\\nπθ(yw)\\n=βlogP\\niπθ(sw\\ni|sw\\n:i−1)P\\niπref(sw\\ni|sw\\n:i−1)−βlogP\\njπθ(sl\\nj|sl\\n:j−1)P\\njπref(sl\\nj|sl\\n:j−1)\\n=βX\\nilogπθ(sw\\ni|sw\\n:i−1)\\nπref(sw\\ni|sw\\n:i−1)−βX\\njlogπθ(sl\\nj|sl\\n:j−1)\\nπref(sl\\nj|sl\\n:j−1)\\n=βX\\ni\\x12\\nlogπθ(sw\\ni|sw\\n:i−1)\\nπref(sw\\ni|sw\\n:i−1)−logπθ(sl\\ni|sl\\n:i−1)\\nπref(sl\\ni|sl\\n:i−1)\\x13\\n.\\n(6)\\nThe progressive preference optimization loss can\\nbe further written as follows ( −logσomitted):\\nL≜ E\\n(sw\\ni,sl\\ni∼D)\\x12\\nβlogπθ(sw\\ni)\\nπref(sw\\ni)−βlogπθ(sl', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 4}), Document(page_content='i|sl\\n:i−1)\\nπref(sl\\ni|sl\\n:i−1)\\x13\\n.\\n(6)\\nThe progressive preference optimization loss can\\nbe further written as follows ( −logσomitted):\\nL≜ E\\n(sw\\ni,sl\\ni∼D)\\x12\\nβlogπθ(sw\\ni)\\nπref(sw\\ni)−βlogπθ(sl\\ni)\\nπref(sl\\ni)\\x13\\n= E\\n(yw,yl∼D)1\\nnX\\ni\\x12\\nβlogπθ(sw\\ni)\\nπref(sw\\ni)−βlogπθ(sl\\ni)\\nπref(sl\\ni)\\x13\\n.\\n(7)\\nThe main difference between vanilla preference\\noptimization in Eqn. (4) and progressive preference\\noptimization is that the latter contains an implicit\\nmean pooling procedure when implementing the\\npreference optimization loss.\\nFurthermore, the directed preference optimiza-\\ntion may face the challenges of overfitting to some\\ndeterministic preference due to weak KL con-\\nstraint (Azar et al., 2023). Hence, we propose to\\nleverage experience replay (Rolnick et al., 2019) as\\nlearning with rehearsal to alleviate the over-fitting', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 4}), Document(page_content='phenomenon. The idea of replaying experience\\ntypically stores a few old training samples within\\na small memory buffer. Therefore, we iteratively\\nadd post-training autoregressive language model-\\ning loss to the preference optimization procedure\\nin a fixed interval, resulting in final generator Mp.\\n4.5 Inference and Refinement\\nDuring inference, for query q,Dis first retrieved\\nand then sent to Mpoutput to the final answer\\nSinitconsists of nstatements. As there may not be\\nall statements correctly attributing documents, we\\nadditionally perform the post-hoc refinement after\\nthe original generation. We maintain a collection of\\ncitations Ctmp. Starting from the last statement of\\nSinit, if the current sihas the citations, update the\\nCtmpto the citations of the current si; if the current\\nsidoes not have a citation, add the current citation\\nsetCtmpto this statement until all nstatements\\nhave been traversed. Then we concatenate these n\\nstatements together as the final answer S.\\n5 Setup\\n5.1 Datasets and Evaluation Metrics\\nDataset We mainly focus on attributable long-\\nform question-answering (QA) task using ASQA\\ndataset and ELI5 subsets from Gao et al. (2023b).\\nIn addition to these factoid long-form QA tasks,\\nwe test the generation quality on StrategyQA\\ndataset (Geva et al., 2021) which focuses on open-\\ndomain QA where the required reasoning steps are\\nimplicit in the question. We use the official test set\\nas our evaluation set.\\nMetrics Following Gao et al. (2023b), we re-\\nport citation recall ,precision , and F1which uses\\nTRUE (Honovich et al., 2022) as the attribu-\\ntion evaluation model ϕto automatically examine\\nwhether the cited documents entail the model gen-\\neration. For ASQA dataset, we report the recall\\nof correct short answers (EM-R) by checking\\nwhether the short answers (provided by the dataset)\\nare exact substrings of the generation. For ELI5\\ndataset, we report the claim recall (Claim) to check\\nwhether the model output entails the sub-claims,\\nthat are generated by text-davinci-003 (Ouyang\\net al., 2022). For StrategyQA dataset, we report the\\naccuracy for task performance.\\n5.2 Competitive Methods\\nWe compare APO with several baselines. For each\\nbaseline, we use gtr-t5-large as our retriever.In-Context Learning (ICLC ITE): We prompt\\nLLMs with few-shot examples, each consisting of\\na query, a set of retrieved documents and an answer\\nwith inline citations. The LLMs can in-context\\nlearn from the examples and generate grounded re-\\nsponses for the test query and retrieved documents.\\nPost-Hoc Cite (POSTCITE): Given query q, we\\nfirst instruct LLMs to answer qwithout retrieved\\ndocuments. Then, we use the attribution evaluation\\nmodel ϕto link each statement to the most relevant\\ndocument retrieved by the query.\\nPost-Hoc Attribute (POSTATTR): Instead of cit-\\ning the most relevant document, for each statement,\\nwe further retrieve a set of kdocuments and then\\nuse the ϕto link to the document that maximally\\nsupports the statement by threshold.\\nSelf-RAG (Asai et al., 2023b): Self-RAG is the\\nstate-of-the-art (SoTA) method that adaptively re-\\ntrieves documents on-demand. It generates with\\nreflection on retrieved documents and its genera-\\ntions by special token control.\\nAGREE (Ye et al., 2023b): AGREE leverages\\ntest-time adaptation to reinforce unverified state-\\nments which iteratively improves the responses of\\nLLMs. It tunes a pre-trained LLM to self-ground\\nits response in retrieved documents using automati-\\ncally collected data.\\n5.3 Implementation Details\\nIf not specified, we retrieve the top 5 documents\\nas the related documents to qand we set the de-\\ncoding temperature to 0.01 during inference. For\\nthe post-training, we tune the model for 2 epochs\\nwith a learning rate of 5e-5. For the preference\\noptimization, we tune the model with LoRA (Hu\\net al., 2022) for 1 epoch, and we set alpha to\\n2 and lora ranks to 16. We set mto 100. We\\nusellama-2-13b-base (Touvron et al., 2023) for\\nfair comparison. We run all the experiments on\\nNVIDIA A100 80G GPUs.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 5}), Document(page_content='2 and lora ranks to 16. We set mto 100. We\\nusellama-2-13b-base (Touvron et al., 2023) for\\nfair comparison. We run all the experiments on\\nNVIDIA A100 80G GPUs.\\n6 Results\\n6.1 Main Result\\nTable 2 shows the comparison results of APO with\\nother baselines on three datasets. In terms of cor-\\nrectness and citation quality, our method outper-\\nforms the baselines on all three datasets. It shows\\nthat APO has better overall generation performance\\nin various scenarios. Specifically, our method out-\\nperforms Self-RAG by 8.8 points on the EM-R\\nmetric. We speculate that this inconsistency stems', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 5}), Document(page_content='Dataset & MetricsASQA StrategyQA ELI5\\nCorrect Citation Correct Citation Correct Citation\\nEM-R Rec Prec F1 ACC Rec Prec F1 Claim Rec Prec F1\\nICLC ITE(Gao et al., 2023b) 35.2 38.4 39.4 38.9 65.5 20.6 33.1 25.4 13.4 17.3 15.8 16.5\\nPOSTCITE(Gao et al., 2023b) 25.0 23.6 23.6 23.6 64.3 8.7 8.7 8.7 7.1 5.7 5.8 5.8\\nPOSTATTR (Ye et al., 2023b) 25.0 33.6 33.6 33.6 64.3 12.5 12.5 12.5 7.1 12.2 12.2 12.2\\nSelf-RAG (Asai et al., 2023b) 31.7 70.3 71.3 70.8 62.1 31.4 36.5 33.8 10.7 20.8 22.5 21.6\\nAGREE (Ye et al., 2023b) 39.4 64.0 66.8 65.4 64.6 30.2 37.2 33.3 9.4 21.6 16.0 18.4\\nAPO (only post-training) 36.6 65.0 62.1 63.5 62.5 30.7 30.1 30.4 13.0 18.5 17.9 18.2\\nAPO (our method) 40.5 72.8 69.6 71.2 61.8 40.0 39.1 39.6 13.5 26.0 24.5 25.2\\nTable 2: The performance comparison between our method and extensive baselines. Experiments are evaluated\\non ASQA (Stelmakh et al., 2022), StrategyQA (Geva et al., 2021) and ELI5 dataset (Fan et al., 2019). For most\\nbaselines, we use the results of previous works (Gao et al., 2023b; Ye et al., 2023b).\\nMethod EM-R Rec Prec F1\\nOur Method 36.6 65.0 62.1 63.5\\nw/o asqa 38.8 71.7 67.2 69.4\\nw/o hallucinated statement 40.4 69.3 65.3 67.3\\nw/o mistaken synthesis 40.2 73.4 69.2 71.2\\nw/o unintentional omission 39.1 72.7 68.2 70.4\\nw/ response-level PO 38.9 69.1 65.1 67.1\\nw/ statement-level PO 40.5 72.8 69.6 71.2\\nTable 3: Ablation study on the ASQA dataset. We ablate\\nnot only the source and predefined error type used to\\nconstruct PO data, but also the training strategy.\\nfrom the difference between coherent generation\\nand step-wise generation in Self-RAG. Our method\\nalso shows consistent improvements over AGREE\\nacross multiple benchmarks which suggests that\\nAPO can more effectively exploit the power of\\nLLM to enhance retrieval. APO can be used to\\ncomplement these active or adaptive retrieval-based\\nmethods and we leave it for future work. Compared\\nto the post-training baseline, the preference opti-\\nmization shows further improvement with an 8.0\\naverage increased citation F1. Furthermore, we ob-\\nserve a trade-off between correctness and citation\\nquality in several baselines including Self-RAG\\nand AGREE, possibly due to the generation hal-\\nlucination and attribution hallucination defined in\\n§4.1. In contrast, APO helps to deal with these\\nhallucinations and performs well in terms of both\\ncorrectness and citation quality.\\n6.2 Ablation Study\\nWe evaluate the effectiveness of each predefined\\nerror type and the results are shown in Table 3.\\nSpecifically, we perform progressive PO on the\\nmodel after post-training and remove data corre-\\nsponding to a predefined type. We observe that\\nwithout data corresponding to hallucinated state-\\nment error, citation F1 drops significantly whichsuggests that our approach improves the grounded-\\nness of the model. Mistaken synthesis error seems\\nto contribute little to performance improvement,\\nbut we observe that it can help improve ground-\\nedness under human evaluation (§6.5). Without\\nunintentional omission error, the model shows poor\\ngeneration quality. This means that the model may\\ngenerate incomplete answers.\\nMoreover, we perform an ablation study on\\nthe training strategy of preference optimization.\\nWe find that the model can also be improved\\nunder the response-level preference optimization\\nmethod such as vanilla DPO, but the improvement\\nis slightly less. In addition, we ablation the PO\\nby removing the ASQA questions from our prefer-\\nence data. Note that we construct the preference\\ndata based on the training set of ASQA, and use\\nits test set for evaluation. We have verified and\\nguaranteed that there is no data overlap between\\nthe two. We find that the generation quality and\\ncitation quality have decreased. We attribute it to\\nhigh-quality in-domain questions in ASQA as a\\nlong-form question answering dataset.\\n6.3 Different Prompting Strategy\\nWe explore applying APO to four prompting strate-\\ngies (Gao et al., 2023b): 1) VANILLA that provides\\nthe top-5 retrieved documents for each question.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 6}), Document(page_content='6.3 Different Prompting Strategy\\nWe explore applying APO to four prompting strate-\\ngies (Gao et al., 2023b): 1) VANILLA that provides\\nthe top-5 retrieved documents for each question.\\nIt is our default setting. 2) SUMM that provides\\nsummaries instead of the full text of the top-10\\nretrieved documents for each question. 3) SNIP-\\nPETthat provides snippets instead of the full text\\nof the top 10 retrieved documents for each ques-\\ntion. 4) ORACLE that provides 5 gold documents\\nfor each question. We use llama-2-13b-chat as\\nthe comparison method because it has impressive\\ninstruction following ability and moderate size. As\\nshown in Table 4, we find that in most cases, APO', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 6}), Document(page_content='Method & MetricsASQA\\nCorrect Rec Prec F1\\nllama-2-13b-chat\\nVANILLA (5-psg) 32.6 60.0 52.1 55.8\\nSUMM (10-psg) 42.9 58.7 50.4 54.2\\nSNIPPET (10-psg) 41.3 57.4 52.1 54.6\\nORACLE (5-psg) 41.4 54.5 52.9 53.7\\nOur method\\nVANILLA (5-psg) 40.5 72.8 69.6 71.2\\nSUMM (10-psg) 42.7 60.9 53.4 56.9\\nSNIPPET (10-psg) 42.3 57.8 51.6 54.5\\nORACLE (5-psg) 52.4 70.5 66.2 68.3\\nMethod & MetricsELI5\\nCorrect Rec Prec F1\\nllama-2-13b-chat\\nVANILLA (5-psg) 12.1 16.4 19.7 17.9\\nSUMM (10-psg) 6.1 9.9 14.3 11.7\\nSNIPPET (10-psg) 11.9 29.4 28.6 29.0\\nORACLE (5-psg) 16.9 21.4 27.3 24.0\\nOur method\\nVANILLA (5-psg) 13.5 26.0 24.5 25.2\\nSUMM (10-psg) 12.7 37.8 35.7 36.7\\nSNIPPET (10-psg) 14.2 37.6 34.8 36.1\\nORACLE (5-psg) 21.7 32.6 30.8 31.7\\nTable 4: Comparisons with different retrieval context.\\nachieves better performance than baseline. For ex-\\nample, APO under VANILLA andORACLE settings\\nperforms best in Citation F1 on ASQA, while it\\nunder SUMM andSNIPPET settings in ELI5 has\\nimproved Citation F1. It shows that the format of\\nthe context has an impact on attribution task.\\n6.4 Different PO Methods\\nTable 5 illustrates the results of different direct pref-\\nerence optimization methods adopted by Mp. We\\ninclude a SFT baseline to tune the Mgusing the\\npositive part in the chosen preference pairs that\\nwe created. We observe that our method can be\\ntransferred to several different preference optimiza-\\ntion methods, but the performance swings in sev-\\neral metrics. All preference optimization methods\\nhave performance boosts compared with the post-\\ntraining baseline and the SFT baseline. It shows\\nthat preference optimization can help improve the\\ngeneration quality to some extent.\\n6.5 Error Analysis\\nWe conduct human evaluation of model response\\non ASQA dataset. Specifically, we collect 50 sam-\\nples that contain errors judged by the attribution\\nevaluation model ϕ. We then perform a detailed\\nmanual review of these samples to identify errorMethod & MetricsASQA\\nCorrect Rec Prec F1\\nAPO (only post-training) 36.6 65.0 62.1 63.5\\nw/ Positive statement SFT 29.0 66.7 56.8 61.4\\nw/ IPO (Azar et al., 2023) 39.9 72.7 69.2 70.9\\nw/ SLiC (Zhao et al., 2023) 40.1 72.5 69.1 70.8\\nw/ KTO (Kawin et al., 2023) 39.8 72.5 68.7 70.5\\nw/ Progressive PO 40.5 72.8 69.6 71.2\\nMethod & MetricsELI5\\nCorrect Rec Prec F1\\nAPO (only post-training) 13.0 18.5 17.9 18.2\\nw/ Positive statement SFT 10.6 34.5 30.8 32.5\\nw/ IPO (Azar et al., 2023) 13.5 26.5 24.8 25.6\\nw/ SLiC (Zhao et al., 2023) 13.7 30.7 22.0 25.6\\nw/ KTO (Kawin et al., 2023) 14.3 24.7 26.5 25.6\\nw/ Progressive PO 13.5 26.0 24.5 25.2\\nTable 5: Comparisons with different preference method.\\nError Type # Proportion (%)\\nAttribution hallucination 26.4\\nGeneration hallucination\\n- Fabrication 48.4\\n- Omission 18.7\\n- Synthesis 6.5\\nTable 6: Error types of the proposed methods.\\ntypes. Our evaluation results are shown in Table 6.\\nWe find that nearly half of the errors are of fabrica-\\ntion error. We reveal that the model either gener-\\nated text not supported by the reference documents\\nor incorrectly attributed information to irrelevant\\ndocuments. In certain instances, hallucinations\\nare due to the documents with low quality. For\\nexample, some documents are truncated, and the\\nmodel attempts to complete or extrapolate the in-\\ncomplete text. Additionally, we notice omission\\nerrors on both generated text and citation where the\\nmodel fails to generate necessary citations to sub-\\nstantiate its statements. Although synthesis errors\\nare less common, we observe some cases which\\nmodel conflated information from multiple doc-\\numents and generated counterfactual statements.\\nThe case study is shown in Appendix E.\\n7 Conclusion\\nThis paper introduces the APO framework for at-\\ntributed text generation. We treat attribution as\\na preference learning task, utilizing curated post-\\ntraining collections and an automated synthesis\\nalgorithm to reduce manual labeling costs. Experi-\\nments on three datasets demonstrate the effective-\\nness of APO which achieves leading citation F1', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 7}), Document(page_content='training collections and an automated synthesis\\nalgorithm to reduce manual labeling costs. Experi-\\nments on three datasets demonstrate the effective-\\nness of APO which achieves leading citation F1\\nand improved response quality. Future work can\\nexplore extending APO to real-world applications.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 7}), Document(page_content='Limitation\\nWe aim to improve the credibility and reliability of\\ncontent generated by LLMs using the APO frame-\\nwork. However, it faces limitations such as the\\nnarrow scope of datasets used, which may not\\nfully represent the diversity of real-world appli-\\ncations (Liu et al., 2023). The generalization ca-\\npabilities of the model are also a concern, as the\\nautomatic generated data may not cover all scenar-\\nios of hallucination. While addressing the high cost\\nof data labeling, the scalability and economic fea-\\nsibility in larger datasets remain unexplored. The\\napproximation of human citation processes may not\\ncapture all the complexities of scholarly writing,\\nand its reliance on external sources raises concerns\\nabout the quality and availability of these sources.\\nPotential biases in training data and synthesized\\ndata could lead to biased outputs (Wang et al., 2023;\\nHu et al., 2015). The robustness of the framework\\nagainst deliberate hallucination and its adaptability\\nto rapidly evolving NLP fields are not fully as-\\nsessed, highlighting areas for future improvement\\nand research in enhancing LLM reliability.\\nEthical Statement\\nThe ethical considerations surrounding the use of\\nLLMs that generate citations encompass a range\\nof concerns, including the risk of increased trust\\nwithout verification, challenges in time-critical\\ndecision-making, the assumption of inherent trust-\\nworthiness, and copyright issues. Ethically, it is\\ncrucial to encourage users to critically engage with\\nand verify machine-generated content to mitigate\\nmisinformation and hallucinations. Additionally,\\nrecognizing the limitations and potential legal chal-\\nlenges related to copyright when using such attri-\\nbutions is essential. Addressing these ethical is-\\nsues and educating users on the potential pitfalls\\nbecomes increasingly important to ensure the re-\\nsponsible and informed use of text generated by\\nLLMs.\\nReferences\\nGaon An, Junhyeok Lee, Xingdong Zuo, Norio Kosaka,\\nKyung-Min Kim, and Hyun Oh Song. 2023. Direct\\npreference-based policy optimization without reward\\nmodeling. In Neural Information Processing Sys-\\ntems.\\nRohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-\\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, JohanSchalkwyk, Andrew M. Dai, Anja Hauth, Katie Mil-\\nlican, David Silver, Slav Petrov, Melvin Johnson,\\nIoannis Antonoglou, Julian Schrittwieser, Amelia\\nGlaese, Jilin Chen, Emily Pitler, Timothy P. Lilli-\\ncrap, Angeliki Lazaridou, Orhan Firat, James Molloy,\\nMichael Isard, Paul Ronald Barham, Tom Henni-\\ngan, Benjamin Lee, Fabio Viola, Malcolm Reynolds,\\nYuanzhong Xu, Ryan Doherty, Eli Collins, Clemens\\nMeyer, Eliza Rutherford, Erica Moreira, Kareem\\nAyoub, Megha Goel, George Tucker, Enrique Pi-\\nqueras, Maxim Krikun, Iain Barr, Nikolay Savinov,\\nIvo Danihelka, Becca Roelofs, Anaïs White, Anders\\nAndreassen, Tamara von Glehn, Lakshman Yagati,\\nMehran Kazemi, Lucas Gonzalez, Misha Khalman,\\nJakub Sygnowski, and et al. 2023. Gemini: A family\\nof highly capable multimodal models. ArXiv preprint ,\\nabs/2312.11805.\\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi\\nChen. 2023a. Retrieval-based language models and\\napplications. In Proceedings of the 61st Annual Meet-\\ning of the Association for Computational Linguistics:\\nTutorial Abstracts, ACL 2023, Toronto, Canada, July\\n9-14, 2023 , pages 41–46.\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\\nHannaneh Hajishirzi. 2023b. Self-rag: Learning to\\nretrieve, generate, and critique through self-reflection.\\nArXiv preprint , abs/2310.11511.\\nMohammad Gheshlaghi Azar, Mark Rowland, Bilal\\nPiot, Daniel Guo, Daniele Calandriello, Michal\\nValko, and Rémi Munos. 2023. A general theoret-\\nical paradigm to understand learning from human\\npreferences. ArXiv preprint , abs/2310.12036.\\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\\nStanislav Fort, Deep Ganguli, Tom Henighan,\\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\\nTom Conerly, Sheer El Showk, Nelson Elhage, Zac', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 8}), Document(page_content='Askell, Anna Chen, Nova DasSarma, Dawn Drain,\\nStanislav Fort, Deep Ganguli, Tom Henighan,\\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\\nTom Conerly, Sheer El Showk, Nelson Elhage, Zac\\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\\nNanda, Catherine Olsson, Dario Amodei, Tom B.\\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\\nBenjamin Mann, and Jared Kaplan. 2022. Train-\\ning a helpful and harmless assistant with reinforce-\\nment learning from human feedback. ArXiv preprint ,\\nabs/2204.05862.\\nBernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aharoni,\\nDaniel Andor, Livio Baldini Soares, Jacob Eisen-\\nstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui,\\nTom Kwiatkowski, Ji Ma, Jianmo Ni, Tal Schuster,\\nWilliam W. Cohen, Michael Collins, Dipanjan Das,\\nDonald Metzler, Slav Petrov, and Kellie Webster.\\n2022. Attributed question answering: Evaluation\\nand modeling for attributed large language models.\\nArXiv preprint , abs/2212.08037.\\nRalph Allan Bradley and Milton E Terry. 1952. Rank\\nanalysis of incomplete block designs: I. the method\\nof paired comparisons. Biometrika , 39(3/4):324–\\n345.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 8}), Document(page_content='Terrence A Brooks. 1986. Evidence of complex citer\\nmotivations. Journal of the American Society for\\nInformation Science , 37(1):34–36.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In Ad-\\nvances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Process-\\ning Systems 2020, NeurIPS 2020, December 6-12,\\n2020, virtual .\\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan\\nMartic, Shane Legg, and Dario Amodei. 2017. Deep\\nreinforcement learning from human preferences. In\\nAdvances in Neural Information Processing Systems\\n30: Annual Conference on Neural Information Pro-\\ncessing Systems 2017, December 4-9, 2017, Long\\nBeach, CA, USA , pages 4299–4307.\\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\\nZhifang Sui. 2023. A survey for in-context learning.\\nArXiv preprint , abs/2301.00234.\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\\nier, Jason Weston, and Michael Auli. 2019. ELI5:\\nLong form question answering. In Proceedings of\\nthe 57th Annual Meeting of the Association for Com-\\nputational Linguistics , pages 3558–3567.\\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-\\ncent Y . Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,\\nand Kelvin Guu. 2023a. RARR: researching and\\nrevising what language models say, using language\\nmodels. In Proceedings of the 61st Annual Meeting\\nof the Association for Computational Linguistics (Vol-\\nume 1: Long Papers), ACL 2023, Toronto, Canada,\\nJuly 9-14, 2023 , pages 16477–16508.\\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\\n2023b. Enabling large language models to generate\\ntext with citations. In Proceedings of the 2023 Con-\\nference on Empirical Methods in Natural Language\\nProcessing, EMNLP 2023, Singapore, December 6-\\n10, 2023 , pages 6465–6488.\\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\\nWang. 2023c. Retrieval-augmented generation for\\nlarge language models: A survey. ArXiv preprint ,\\nabs/2312.10997.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\\nDan Roth, and Jonathan Berant. 2021. Did aristotle\\nuse a laptop? a question answering benchmark withimplicit reasoning strategies. Transactions of the\\nAssociation for Computational Linguistics , 9:346–\\n361.\\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\\nScialom, Idan Szpektor, Avinatan Hassidim, and\\nYossi Matias. 2022. TRUE: Re-evaluating factual\\nconsistency evaluation. In Proceedings of the Second\\nDialDoc Workshop on Document-grounded Dialogue\\nand Conversational Question Answering , pages 161–\\n175.\\nBaotian Hu, Qingcai Chen, and Fangze Zhu. 2015. LC-\\nSTS: A large scale chinese short text summarization\\ndataset. In Proceedings of the 2015 Conference on\\nEmpirical Methods in Natural Language Processing,\\nEMNLP 2015, Lisbon, Portugal, September 17-21,\\n2015 , pages 1967–1972. The Association for Com-\\nputational Linguistics.\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. 2022. Lora: Low-rank adaptation of\\nlarge language models. In The Tenth International\\nConference on Learning Representations, ICLR 2022,\\nVirtual Event, April 25-29, 2022 .\\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nan-\\ndan Thakur, and Jimmy Lin. 2023. HAGRID:\\nA human-llm collaborative dataset for generative\\ninformation-seeking with attribution. ArXiv preprint ,\\nabs/2307.16883.\\nYachen Kang, Diyuan Shi, Jinxin Liu, Li He, and', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 9}), Document(page_content='A human-llm collaborative dataset for generative\\ninformation-seeking with attribution. ArXiv preprint ,\\nabs/2307.16883.\\nYachen Kang, Diyuan Shi, Jinxin Liu, Li He, and\\nDonglin Wang. 2023. Beyond reward: Offline\\npreference-guided policy optimization. In Interna-\\ntional Conference on Machine Learning, ICML 2023,\\n23-29 July 2023, Honolulu, Hawaii, USA , volume\\n202 of Proceedings of Machine Learning Research ,\\npages 15753–15768.\\nEthayarajh Kawin, Xu Winnie, Jurafsky Dan, and Kiela\\nDouwe. 2023. Human-centered loss functions (ha-\\nlos). Technical report, Contextual AI.\\nDongfang Li, Zetian Sun, Xinshuo Hu, Zhenyu Liu,\\nZiyang Chen, Baotian Hu, Aiguo Wu, and Min\\nZhang. 2023a. A survey of large language models\\nattribution. ArXiv preprint , abs/2311.03731.\\nXiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin,\\nTianxiang Sun, and Xipeng Qiu. 2023b. Llatrieval:\\nLlm-verified retrieval for verifiable generation. ArXiv\\npreprint , abs/2311.07838.\\nXinze Li, Yixin Cao, Liangming Pan, Yubo Ma, and\\nAixin Sun. 2023c. Towards verifiable generation:\\nA benchmark for knowledge-aware language model\\nattribution.\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri-\\nson Edwards, Bowen Baker, Teddy Lee, Jan Leike,\\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\\n2023. Let’s verify step by step. ArXiv preprint ,\\nabs/2305.20050.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 9}), Document(page_content='Nelson F. Liu, Tianyi Zhang, and Percy Liang. 2023.\\nEvaluating verifiability in generative search engines.\\nInFindings of the Association for Computational\\nLinguistics: EMNLP 2023, Singapore, December\\n6-10, 2023 , pages 7001–7025.\\nChaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth\\nSieber, Mark Yatskar, and Dan Roth. 2023. Ex-\\npertqa: Expert-curated questions and attributed an-\\nswers. ArXiv preprint , abs/2309.07852.\\nOpenAI. 2022. Chatgpt: Optimizing language models\\nfor dialogue.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll L. Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\\n2022. Training language models to follow instruc-\\ntions with human feedback. In Advances in Neural\\nInformation Processing Systems 35: Annual Confer-\\nence on Neural Information Processing Systems 2022,\\nNeurIPS 2022, New Orleans, LA, USA, November 28\\n- December 9, 2022 .\\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano\\nErmon, Christopher D. Manning, and Chelsea Finn.\\n2023. Direct preference optimization: Your language\\nmodel is secretly a reward model. ArXiv preprint ,\\nabs/2305.18290.\\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\\nMichael Collins, Dipanjan Das, Slav Petrov, Gau-\\nrav Singh Tomar, Iulia Turc, and David Reitter. 2021.\\nMeasuring attribution in natural language generation\\nmodels. ArXiv preprint , abs/2112.12870.\\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Tim-\\nothy P. Lillicrap, and Gregory Wayne. 2019. Expe-\\nrience replay for continual learning. In Advances\\nin Neural Information Processing Systems 32: An-\\nnual Conference on Neural Information Processing\\nSystems 2019, NeurIPS 2019, December 8-14, 2019,\\nVancouver, BC, Canada , pages 348–358.\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\\nAlec Radford, and Oleg Klimov. 2017. Proximal\\npolicy optimization algorithms. ArXiv preprint ,\\nabs/1707.06347.\\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\\nand Jason Weston. 2021. Retrieval augmentation\\nreduces hallucination in conversation. In Findings\\nof the Association for Computational Linguistics:\\nEMNLP 2021 , pages 3784–3803.\\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\\nWei Chang. 2022. ASQA: Factoid questions meet\\nlong-form answers. In Proceedings of the 2022 Con-\\nference on Empirical Methods in Natural Language\\nProcessing , pages 8273–8288.\\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M.\\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,Dario Amodei, and Paul F. Christiano. 2020. Learn-\\ning to summarize with human feedback. In Advances\\nin Neural Information Processing Systems 33: An-\\nnual Conference on Neural Information Processing\\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\\nvirtual .\\nHao Sun, Hengyi Cai, Bo Wang, Yingyan Hou, Xi-\\naochi Wei, Shuaiqiang Wang, Yan Zhang, and Dawei\\nYin. 2023. Towards verifiable text generation with\\nevolving memory and self-reflection. ArXiv preprint ,\\nabs/2312.09075.\\nMisha Teplitskiy, Eamon Duede, Michael Menietti, and\\nKarim R Lakhani. 2022. How status of research pa-\\npers affects the way they are read and cited. Research\\nPolicy , 51(4):104484.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 10}), Document(page_content='ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\\nMelanie Kambadur, Sharan Narang, Aurélien Ro-\\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\\nScialom. 2023. Llama 2: Open foundation and fine-\\ntuned chat models. ArXiv preprint , abs/2307.09288.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\\nHajishirzi. 2023. Self-instruct: Aligning language\\nmodels with self-generated instructions. In Proceed-\\nings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers),\\nACL 2023, Toronto, Canada, July 9-14, 2023 , pages\\n13484–13508.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\\nLiang, Jeff Dean, and William Fedus. 2022. Emer-\\ngent abilities of large language models. Trans. Mach.\\nLearn. Res. , 2022.\\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\\nral text generation with unlikelihood training. In\\n8th International Conference on Learning Represen-\\ntations, ICLR 2020, Addis Ababa, Ethiopia, April\\n26-30, 2020 .', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 10}), Document(page_content='Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng,\\nand Tat-Seng Chua. 2023. Search-in-the-chain: To-\\nwards the accurate, credible and traceable content\\ngeneration for complex knowledge-intensive tasks.\\nArXiv preprint , abs/2304.14732.\\nHongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and\\nWeiqiang Jia. 2023a. Cognitive mirage: A review\\nof hallucinations in large language models. ArXiv\\npreprint , abs/2309.06794.\\nXi Ye, Ruoxi Sun, Sercan Ö Arik, and Tomas Pfis-\\nter. 2023b. Effective large language model adap-\\ntation for improved grounding. ArXiv preprint ,\\nabs/2311.09533.\\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su,\\nand Huan Sun. 2023. Automatic evaluation of at-\\ntribution by large language models. ArXiv preprint ,\\nabs/2305.06311.\\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,\\nTingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,\\nYulong Chen, Longyue Wang, Anh Tuan Luu, Wei\\nBi, Freda Shi, and Shuming Shi. 2023. Siren’s song\\nin the ai ocean: A survey on hallucination in large\\nlanguage models. ArXiv preprint , abs/2309.01219.\\nYao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman,\\nMohammad Saleh, and Peter J. Liu. 2023. Slic-hf:\\nSequence likelihood calibration with human feed-\\nback. ArXiv preprint , abs/2305.10425.\\nRui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei\\nShen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu,\\nYuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi,\\nNuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang,\\nZhangyue Yin, Rongxiang Weng, Wensen Cheng,\\nHaoran Huang, Tianxiang Sun, Hang Yan, Tao Gui,\\nQi Zhang, Xipeng Qiu, and Xuanjing Huang. 2023.\\nSecrets of RLHF in large language models part I:\\nPPO. ArXiv preprint , abs/2307.04964.\\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.\\nBrown, Alec Radford, Dario Amodei, Paul F. Chris-\\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\\nguage models from human preferences. ArXiv\\npreprint , abs/1909.08593.\\nGuido Zuccon, Bevan Koopman, and Razia Shaik. 2023.\\nChatgpt hallucinates when attributing answers.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 11}), Document(page_content='A Details about Preference Optimization\\nMethods\\nReinforcement Learning form Human Feedback\\n(RLHF) (Ouyang et al., 2022) uses reward-\\nmodel-based reinforcement learning algorithm to\\nlearn the optimal policy. It first learns a reward\\nmodel from the preference data, then uses an on-\\npolicy PPO algorithm (Schulman et al., 2017)\\nto maximize the learned reward. The reward is\\nlearned to use Bradley-Terry model (Bradley and\\nTerry, 1952), which assumes the preference score\\ncan be approximated by substituted with point-wise\\nreward. This assumption may lead to an approxi-\\nmation error when preference is deterministic. The\\nPPO algorithm is used on data sampled from gener-\\nating policy, which may have a different support or\\ndistribution drift from preference data, the learned\\nreward model inference on the out-of-distribution\\ndata may reduce the accuracy. The process of\\nRLHF needs to train reward model and on-policy\\nPPO algorithm which is complex, time-consuming,\\nand unstable.\\nDirect Preference Optimization (DPO)\\n(Rafailov et al., 2023) combines off-policy algo-\\nrithm and Bradley-Terry model to directly learn\\nthe generating policy from preference data. The\\noff-policy algorithm is based on KL-regularization\\nreward maximization from off-RL community,\\nwhich is data efficient, stable and eliminating\\nthe need for a reward model. When preference\\nis deterministic which occurs in most cases, the\\nreward of Bradley-Terry model is undefined, which\\nleads to ignoring the KL-regularization term and\\noverfitting the preference dataset.\\nIdentity-mapping Preference Optimization\\n(IPO) (Azar et al., 2023) claims when pref-\\nerences are deterministic or near deterministic,\\nDPO will lead over-fitting to the preference dataset\\nat the expense of ignoring the KL-regularation\\nterm. To optimize the objective, IPO derives an\\noff-policy loss on empirical dataset:\\nhπθyw,yl= logπθ(yw)\\nπref(yw)−logπθ(yl)\\nπref(yl),(8)\\nLIPO(θ, yw, yl) =\\x12\\nhπ\\nyw,yl−1\\n2β\\x132\\n.(9)\\nThat means IPO loss will always regularize πθto-\\nwards πrefby controlling the gap between the log-\\nlikelihood ratios logπθ(yw|x)\\nπθ(yl|x)andlogπref(yw|x)\\nπref(yl|x).Kahneman-Tversky Optimization (KTO)\\n(Kawin et al., 2023) directly maximizes the utility\\nof LLM generations instead of maximizing the\\nlog-likelihood of preferences by introducing a\\nKahneman-Tversky Optimization loss. KTO does\\nnot need preference pairs and only knowledge of\\nwhether output is desirable or undesirable for a\\ngiven input.\\nSequence Likelihood Calibration (SLiC)\\n(Zhao et al., 2023) uses calibrated likelihood of\\nmodel-generated sequences to better align with\\nreference sequences in the model’s latent space.\\nIt tries to alleviate the problem of MLE that\\ngives probability mass to sparsely observed target\\nsequences, which is used to calculate reward in\\nDPO.\\nB Details about Pre-processing\\nFor ExpertQA dataset, we remove samples whose\\n1) citations attribute to empty references; 2) doc-\\numents contain different document IDs but same\\ncontext. For EVIGSE dataset, we remove sam-\\nples whose 1) citation attribute to “None” refer-\\nences; 2) do not have reference documents. We\\nfurther normalize the “supported” label and the ci-\\ntation format for these datasets. The details of each\\ndataset we used for post-training procedure after\\npre-processing are shown in Table 7.\\nC Post-training Templates\\nThe post-training template we used follows the\\nquestion answering template used by Gao et al.\\n(2023b) since we find that preposition question\\nbefore document can result in a performance boost\\nwhen trying ICLC ITEmethod in the preliminary\\nexperiments. The concrete templates are shown in\\nTable 8.\\nD Details about the Instruction\\nThe templates employed for generating preference\\ndata are detailed in Table 9 for positive instances,\\nTable 10 for statements exhibiting hallucination\\nerrors, Table 11 for statements with synthesis er-\\nrors, and Table 12 for statements characterized by\\nomission errors.\\nE Case Study\\nIn this section, we perform a detailed case study', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 12}), Document(page_content='errors, Table 11 for statements with synthesis er-\\nrors, and Table 12 for statements characterized by\\nomission errors.\\nE Case Study\\nIn this section, we perform a detailed case study\\nand demonstrate several examples of each type of\\nerror we defined. As shown in Table 13, we classify', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 12}), Document(page_content='Dataset # Sample Avg. Query Length Avg. Response Length Avg. Statements Avg. Citations\\nEVIGSE 3508 51.03 379.05 4.32 3.19\\nExpertQA 906 106.90 999.84 7.16 5.67\\nHARGID(train) 1301 38.55 368.22 4.62 2.85\\nHARGID(dev) 615 40.43 292.46 3.63 2.54\\nTable 7: Details for our post-training data after pre-processing.\\nInput\\nWrite an accurate, engaging, and concise answer for the given question using only the provided\\ndocuments (some of which might be irrelevant) and cite them properly. Use an unbiased and\\njournalistic tone. Always cite for any factual claim. When citing several search results, use\\n[1][2][3]. Cite at least one document and at most three documents in each sentence. If multiple\\ndocuments support the sentence, only cite a minimum sufficient subset of the documents.\\nQuestion: {{question}}\\nDocument [1](Title: {{title 1}}): {{context 1}}\\nDocument [2](Title: {{title 2}}): {{context 2}}\\nDocument [3](Title: {{title 3}}): {{context 3}}\\n...\\nDocument [n](Title: {{title n}}): {{context n}}\\nAnswer:\\nOutput\\n{{output}}\\nTable 8: Post-training Template with instruction Ipost\\nit as a fabrication error since it uses an undefined\\nentity. In Table 14, we classify it as a synthesis\\nerror since it mixes up facts from document 4 and\\ndocument 5, which results in a factual error. In\\nTable 15, we classify it as a omission error since it\\nused facts from document 4 and document 5, but\\ndocument 4 is not attributed.\\nF Related Works of Retrieval\\nAugmentation of LLMs\\nRetrieval augmentation has emerged as a prominent\\ntechnique aimed at enhancing the accuracy and\\nveracity of LLMs (Gao et al., 2023c; Asai et al.,\\n2023a). Specifically, Sun et al. (2023) couples\\nLLMs with long-term and short-term memories,\\nresulting in improved claim and citation generation.\\nMeanwhile, in order to effectively incorporate ex-\\nternal knowledge into LLMs, SearChain proposes\\na global reasoning chain strategy that facilitates re-\\ntrieval augmentation generation at each node within\\nthe chain (Xu et al., 2023). In another line of re-\\nsearch, the self-reflection is leveraged for retrieval\\nverification during the retrieval-augmented genera-tion process (Li et al., 2023b; Asai et al., 2023b).\\nDespite these advancements, prior studies have not\\nadequately addressed the issue of attribution hal-\\nlucination (Zuccon et al., 2023). In contrast, we\\nfocus on making the model better answer the query\\nand align with the reference.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 13}), Document(page_content='Algorithm 2 Automatic preference data collection algorithm\\n1:Input Input Pinit, Critic Mc, Generator Mg, Retriever R, Error instructions E\\n2:Output Output augmented preference dataset Psyn\\n3:Ptmp= {}\\n4:for(q, D,S,C,Lrel,Lsup)inPinitdo\\n5: foreach statement siinSdo\\n6: ifthe referenced passages Ciare all relevant toqthen\\n7: add(q, D,S:i,C:i,Lrel,L:i,:\\nsup)toPtmp\\n8: end if\\n9: end for\\n10:end for\\n11:for(q, D,S:i,C:i,Lrel,L:i,:\\nsup)inPtmpdo\\n12: Retrieve top- mpassages Dirusing retriever Rgiven q.\\n13: Subsequently delete passage dfrom Dir, ifdiis predicted as relevant toqusing critic Mc.\\n14: foreach statement siinSand its relative attributed passages Cido\\n15: ifsiis supported by Cithen\\n16: Predicts supported forsiusing critic Mcgiven q, di, si, where di∈D\\n17: Add supported passages to the relative attributed passages Ciof statement si.\\n18: Generate ss∧r\\niusing Mggiven q,s:i−1, newCi.\\n19: Generate ss∧˜r\\niusing Mggiven q,s:i−1,Dir.\\n20: Generate s˜s∧r\\niusing Mggiven q,s:i−1, newCiand pre-defined error type e.\\n21: add(q, ss∧r\\ni, s˜s∧r\\ni, D) and (q, ss∧r\\ni, ss∧˜r\\ni, D) toPsyn.\\n22: else\\n23: Generate ss∧r\\niusing Mggiven q,s:i−1, newCi.\\n24: Generate s˜s∧r\\niusing Mggiven q,s:i−1,D−newCiand pre-defined error type e.\\n25: add(q, ss∧r\\ni, s˜s∧r\\ni, D) toPsyn.\\n26: end if\\n27: end for\\n28:end for', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 14}), Document(page_content='Input\\nTask: Your job is to write a high quality response with requirements as follows:\\nGeneral: Given Request, incomplete response and evidence, continue write a single sentence as\\nthe next sentence of the unfinished response. If text in unfinished response is “None”, you should\\nstart the response(the first sentence).\\nDetail: You should always use the facts from the evidences to propuse your response. Your\\nresponse is correct and comprehensive, fully supported by the evidence we provided. **Don’t use\\nany evidence that can be directly retrieved from the evidences we provided**. No hallucinations,\\nno factual errors, no logic errors.\\nRequest: {{request}}\\nEvidence:\\nDocument [1](Title: {{title 1}}): {{context 1}}\\nDocument [2](Title: {{title 2}}): {{context 2}}\\nDocument [3](Title: {{title 3}}): {{context 3}}\\n...\\nDocument [n](Title: {{title n}}): {{context n}}\\nUnfinished response: {{past statements}}\\nNext sentence(good):\\nOutput\\n{{output}}\\nTable 9: Positive Template', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 15}), Document(page_content='Input\\nTask: Your job is to write a low quality response with requirements as follows:\\nGeneral: Given Request, incomplete response and evidence, continue write a single sentence as\\nthe next sentence of the unfinished response. If text in unfinished response is “None”, you should\\nstart the response(the first sentence).\\nDetail: You will always ignore the evidence. On one hand, you won’t follow the evidence we\\nprovided, your response should be irrelevant to the evidence we provided. On the other hand, your\\nresponse should be relevant to the unfinished response.\\nRequest: {{request}}\\nEvidence:\\nDocument [1](Title: {{title 1}}): {{context 1}}\\nDocument [2](Title: {{title 2}}): {{context 2}}\\nDocument [3](Title: {{title 3}}): {{context 3}}\\n...\\nDocument [n](Title: {{title n}}): {{context n}}\\nUnfinished response: {{past statements}}\\nRaw sentence(good): {{positive statement}}\\nWorse sentence(bad, ignore the evidence):\\nOutput\\n{{output}}\\nTable 10: Negative, fabrication template', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 16}), Document(page_content='Input\\nTask: Your job is to write a low quality response with requirements as follows:\\nGeneral: Given Request, incomplete response and evidence, continue write a single sentence as\\nthe next sentence of the unfinished response. If text in unfinished response is “None”, you should\\nstart the response(the first sentence).\\nDetail: You should first, identify the relationships and entities in evidence; second, continue\\nwriting the next sentence of the response span with regard to the evidence. In your response, the\\nrelationships and entities should be mistakenly intermingled(you are making negative samples, we\\nneed low-quality data).\\nRequest: {{request}}\\nEvidence:\\nDocument [1](Title: {{title 1}}): {{context 1}}\\nDocument [2](Title: {{title 2}}): {{context 2}}\\nDocument [3](Title: {{title 3}}): {{context 3}}\\n...\\nDocument [n](Title: {{title n}}): {{context n}}\\nUnfinished response: {{past statements}}\\nRaw sentence(good): {{positive statement}}\\nWorse sentence(bad, entities in evidences mistakenly intermingled):\\nOutput\\n{{output}}\\nTable 11: Negative, synthesis template', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 17}), Document(page_content='Input\\nTask: Your job is to write a low quality response with requirements as follows:\\nGeneral: Given Request, unfinished response and next sentence, omit some important points from\\nthe next sentence(good) and convert it into a worse response. Your converted worse response\\nshould be consistent with the unfinished response.\\nRequest: List the ingredients needed to make a peanut butter and jelly sandwich\\nUnfinished response:\\nRaw sentence(good): To make a peanut butter and jelly sandwich, you will need peanut butter,\\njelly or jam of your choice, and bread.\\nWorse sentence(bad, omit the evidence): To make a peanut butter and jelly sandwich, you will\\nneed peanut butter and bread.\\nRequest: What are the three features of a cloud-based Database-as-a-Service (DBaaS)?\\nUnfinished response: The three main features of a cloud-based DBaaS are scalability, cost ef-\\nficiency, and backups. Scalability allows you to increase or decrease the resources used by the\\nDBaaS with ease.\\nRaw sentence(good): Cost efficiency is another important feature of a cloud-based DBaaS, as it\\nallows you to pay for only the resources you need and eliminates the need for upfront hardware\\ninvestments.\\nWorse sentence(bad, omit the evidence): Cost efficiency is another important feature of a\\ncloud-based DBaaS, as it allows you to pay for only the resources you need.\\nRequest: {{request}}\\nUnfinished response: {{past statements}}\\nRaw sentence(good): {{positive statement}}\\nWorse sentence(bad, omit the evidence):\\nOutput\\n{{output}}\\nTable 12: Negative, omission template', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 18}), Document(page_content='Question\\nWhen did the rams go to st louis?\\nDocuments\\nDocument [1](Title: History of St. Louis): 2011, with performances by Jay Leno and Aretha\\nFranklin. In January 1995, Georgia Frontiere, the owner of the National Football League team\\nknown as the Los Angeles Rams (now St. Louis Rams), announced she would move that team to\\nSt. Louis. The team replaced the St. Louis Cardinals (now Arizona Cardinals), an NFL franchise\\nthat had moved to St. Louis in 1960 but departed for Arizona in 1988. The Rams played their first\\ngame in their St. Louis stadium, the Edward Jones Dome, on October 22, 1996. Starting in the\\nearly 1980s, more rehabilitation and construction projects began, some of\\nDocument [2](Title: History of the St. Louis Rams): History of the St. Louis Rams The\\nprofessional American football franchise now known as the Los Angeles Rams played in St. Louis,\\nMissouri, as the St. Louis Rams from the 1995 through the 2015 seasons before relocating back\\nto Los Angeles where the team had played from the 1946 season to the 1994 season. The Rams\\nfranchise relocated from Los Angeles to St. Louis in 1995, which had been without a National\\nFootball League (NFL) team since the Cardinals moved to Phoenix, Arizona in 1988. The Rams’\\nfirst home game in St. Louis was at Busch Memorial Stadium against the\\nDocument [3](Title: History of the St. Louis Rams): History of the St. Louis Rams The\\nprofessional American football franchise now known as the Los Angeles Rams played in St. Louis,\\nMissouri, as the St. Louis Rams from the 1995 through the 2015 seasons before relocating back\\nto Los Angeles where the team had played from the 1946 season to the 1994 season. The Rams\\nfranchise relocated from Los Angeles to St. Louis in 1995, which had been without a National\\nFootball League (NFL) team since the Cardinals moved to Phoenix, Arizona in 1988. The Rams’\\nfirst home game in St. Louis was at Busch Memorial Stadium against the\\nDocument [4](Title: Los Angeles Rams): in 1980. After the 1994 NFL season, the Rams left\\nCalifornia and moved east to St. Louis, Missouri. Five seasons after relocating, the team won\\nSuper Bowl XXXIV in a 23–16 victory over the Tennessee Titans. They appeared again in Super\\nBowl XXXVI, where they lost 20–17 to the New England Patriots. The Rams continued to play in\\nEdward Jones Dome in St. Louis until the end of the 2015 NFL season, when the team filed notice\\nwith the NFL of its intent to pursue a relocation back to Los Angeles. The move was approved by\\na 30–2 margin at\\nDocument [5](Title: 1994 Los Angeles Rams season): 1994 Los Angeles Rams season The 1994\\nLos Angeles Rams season was the franchise’s 57th year with the National Football League and\\nthe 49th and last season in the Greater Los Angeles Area until their 2016 relocation back to\\nLos Angeles. After nearly 50 years in the Greater Los Angeles Area, owner Georgia Frontiere\\nannounced that the team would relocate to St. Louis, Missouri on January 15, 1995. While the\\nowners initially rejected the move, permission was eventually granted therefore bringing an end to\\nSouthern California’s first major professional sports franchise until 2016. The threat of relocation\\ndominated talk about\\nOutput\\n...Their first home game in St. Louis was at Busch Memorial Stadium against the Chicago Bears\\non October 22,1996 [1]....\\nTable 13: Sample containing fabrication error. In this sample, Chicago Bears does not appear in the reference\\ndocuments.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 19}), Document(page_content='Question\\nWho performed at the champions league final 2018?\\nDocuments\\nDocument [1](Title: 2016 UEFA Champions League Final): worldwide via UEFA.com from 1 to\\n14 March 2016 in four price categories: C440, C320, C160 and C70. The remaining tickets were\\nallocated to the local organising committee, UEFA and national associations, commercial partners\\nand broadcasters, and to serve the corporate hospitality programme. American singer Alicia Keys\\nperformed in the opening ceremony prior to the match, the first time it has featured a live music\\nperformance. Italian tenor Andrea Bocelli performed the UEFA Champions League Anthem. The\\n2016 UEFA Women’s Champions League Final was held two days prior, on 26 May 2016, at the\\nMapei Stadium – Città del Tricolore\\nDocument [2](Title: 2018 UEFA Champions League Final): Lipa performed at the opening\\nceremony preceding the final. Jamaican rapper Sean Paul joined her as a special guest to perform\\ntheir collaborative song, ¨No Lie¨. The 2018 UEFA Women’s Champions League Final was held two\\ndays earlier, on 24 May 2018, at the Valeriy Lobanovskyi Dynamo Stadium between Wolfsburg\\nand Lyon, Lyon emerging victorious 4–1. This was also the last time that the host city for the\\nmen’s Champions League final was also automatically assigned the Women’s Champions League\\nfinal. The annual UEFA Champions Festival was held between 24–27 May 2018 at the Kiev city\\ncentre. In late May,\\nDocument [3](Title: UEFA Champions League Anthem): the two teams are lined up, as well as at\\nthe beginning and end of television broadcasts of the matches. Special vocal versions have been\\nperformed live at the Champions League Final with lyrics in other languages, changing over to\\nthe host country’s language for the chorus. These versions were performed by Andrea Bocelli\\n(Italian) (Rome 2009, Milan 2016 and Cardiff 2017), Juan Diego Flores (Spanish) (Madrid 2010),\\nAll Angels (Wembley 2011), Jonas Kaufmann and David Garrett (Munich 2012), Mariza (Lisbon\\n2014, unlike the previous final performers, Mariza sang the main lyric of the anthem), and Nina\\nMaria Fischer and\\nDocument [4](Title: 2018 UEFA Champions League Final): 2018 UEFA Champions League Final\\nThe 2018 UEFA Champions League Final was the final match of the 2017–18 UEFA Champions\\nLeague, the 63rd season of Europe’s premier club football tournament organised by UEFA, and\\nthe 26th season since it was renamed from the European Cup to the UEFA Champions League. It\\nwas played at the NSC Olimpiyskiy Stadium in Kiev, Ukraine on 26 May 2018, between Spanish\\nside and defending champions Real Madrid, who had won the competition in each of the last two\\nseasons, and English side Liverpool. Real Madrid won the final 3–1 for their third consecutive and\\nDocument [5](Title: 2017 UEFA Champions League Final): 2017 UEFA Champions League Final\\nThe 2017 UEFA Champions League Final was the final match of the 2016–17 UEFA Champions\\nLeague, the 62nd season of Europe’s premier club football tournament organised by UEFA, and\\nthe 25th season since it was renamed from the European Champion Clubs’ Cup to the UEFA\\nChampions League. It was played at the Millennium Stadium in Cardiff, Wales on 3 June 2017,\\nbetween Italian side Juventus and Spanish side and title holders Real Madrid, in a repeat of the\\n1998 final. Real Madrid won the match 4–1 to secure their 12th title in this competition. With\\nOutput\\n...Real Madrid won the final 3-1 for their third consecutive and overall 12th title in this\\ncompetition[4]...\\nTable 14: Sample containing synthesis error. In this sample, document 4 and document 5 are mistakenly intermingled.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 20}), Document(page_content='Question\\nWhen was the 13th amendment ratified by the states?\\nDocuments\\nDocument [1](Title: Thirteenth Amendment to the United States Constitution): ratification did not\\nimply federal power to legislate on the status of former slaves. During the first week of December,\\nNorth Carolina and Georgia gave the amendment the final votes needed for it to become part of the\\nConstitution. The Thirteenth Amendment became part of the Constitution on December 6, 1865,\\nbased on the following ratifications: Having been ratified by the legislatures of three-fourths of the\\nseveral states (27 of the 36 states, including those that had been ïn rebellion ¨), Secretary of State\\nSeward, on December 18, 1865, certified that the Thirteenth Amendment had become valid, to all\\nintents and\\nDocument [2](Title: Thirteenth Amendment to the United States Constitution): Thirteenth Amend-\\nment to the United States Constitution The Thirteenth Amendment (Amendment XIII) to the\\nUnited States Constitution abolished slavery and involuntary servitude, except as punishment for a\\ncrime. In Congress, it was passed by the Senate on April 8, 1864, and by the House on January 31,\\n1865. The amendment was ratified by the required number of states on December 6, 1865. On\\nDecember 18, 1865, Secretary of State William H. Seward proclaimed its adoption. It was the first\\nof the three Reconstruction Amendments adopted following the American Civil War. Since the\\nAmerican Revolution, states had divided into states that\\nDocument [3](Title: Emancipation Proclamation): Winning re-election, Lincoln pressed the lame\\nduck 38th Congress to pass the proposed amendment immediately rather than wait for the incoming\\n39th Congress to convene. In January 1865, Congress sent to the state legislatures for ratification\\nwhat became the Thirteenth Amendment, banning slavery in all U.S. states and territories. The\\namendment was ratified by the legislatures of enough states by December 6, 1865, and proclaimed\\n12 days later. There were about 40,000 slaves in Kentucky and 1,000 in Delaware who were\\nliberated then. As the years went on and American life continued to be deeply unfair towards\\nblacks, cynicism towards\\nDocument [4](Title: Thirteenth Amendment to the United States Constitution): Enforcement,\\nand Contemporary Implications Thirteenth Amendment to the United States Constitution The\\nThirteenth Amendment (Amendment XIII) to the United States Constitution abolished slavery\\nand involuntary servitude, except as punishment for a crime. In Congress, it was passed by the\\nSenate on April 8, 1864, and by the House on January 31, 1865. The amendment was ratified\\nby the required number of states on December 6, 1865. On December 18, 1865, Secretary of\\nState William H. Seward proclaimed its adoption. It was the first of the three Reconstruction\\nAmendments adopted following the American Civil War. Since the American Revolution, states\\nhad\\nDocument [5](Title: Thirteenth Amendment to the United States Constitution): joint resolution\\nwas sent to Lincoln for his signature. Under the usual signatures of the Speaker of the House and\\nthe President of the Senate, President Lincoln wrote the word Äpprovedänd added his signature to\\nthe joint resolution on February 1, 1865. On February 7, Congress passed a resolution affirming\\nthat the Presidential signature was unnecessary. The Thirteenth Amendment is the only ratified\\namendment signed by a President, although James Buchanan had signed the Corwin Amendment\\nthat the 36th Congress had adopted and sent to the states in March 1861. When the Thirteenth\\nAmendment was submitted to the states\\nOutput\\n...The amendment was ratified by the required number of states on December 6,1865, and\\nPresident Lincoln signed the joint resolution on February 1,1865, after it was passed by Congress\\non February 7,1865 [5]...\\nTable 15: Sample containing omission error. In this sample, document 4 is not attributed.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 21}), Document(page_content='Step1: Preference Data Labeling Step2: Automatic Preference Data Collection\\nStep3: Preference Optimization  Question When was the last time vikings played on thanksgiving?\\nthe V ikings played their December 20 game vs the\\nChicago Bears at TCF Bank Stadium. (2010).\\nSince 1978, the games have traditionally included \\none game hosted by the Dallas Cowboys, \\nand its most recent contest came in 2017 and\\nknocked of f the Detroit Lions 30-23.\\nAnswer  Since 1920, the NFL has played games on Thanksgiving\\nDay[1]. The last time the V ikings played on Thanksgiving was on\\nNovember 23, 2017, having played before both on November 24,\\n2016 and November 23, 2000[2][3].Statement Since 1920, the NFL has played\\ngames on Thanksgiving Day[1].+\\n Doc [1]\\nIrrelevant\\nDocuments\\n[1]\\n [2]\\n [k]Relevant  \\nDocuments\\n[1]\\n [2]\\n [k]\\n+Positive Prompt +\\nStatement The V ikings played against Chicago Bears at TCF\\nBank Stadium, December 20, 2010.\\n+Negative Prompt +\\nStatement The V ikings played against Chicago Bears at TCF\\nBank Stadium, December 20, 2012.\\nSupported\\nUnupported\\nStatement The last time the V ikings\\nplayed on Thanksgiving was on November\\n23, 2017, having played before both on\\nNovember 24, 2016 and November 23,\\n2000[2][3].+\\nDoc [2]\\nSupported\\nRelevant  \\nDocuments\\n[1]\\n [2]\\n [k]\\n+Positive Prompt +\\nStatement  The last time the V ikings played on Thanksgiving\\nwas in 2017 and knocked of f the Detroit Lions 30-23.\\n+Negative Prompt +\\nStatement  The last time the V ikings played on Thanksgiving\\nwas in 2017 and knocked of f the Chicago Bears  30-23.\\nSupported\\nUnupported\\nDoc [3]\\n+Positive Prompt +\\nStatement  The first time the V ikings played on Thanksgiving\\nmaybe since 1978, and with the Dallas Cowboys .\\nSupportedQuestion When was the last time vikings played on thanksgiving?\\nUnsupported\\nQuestion When was the last time vikings played on thanksgiving?\\nPositive Negative\\nPositiveNegative(1)\\nNegative(2)\\nStatement Statement Statement\\nStatement Statement StatementPositive\\nNegative\\nPreference DataStatement Since 1920, the NFL has played games on\\nThanksgiving Day[1].\\nStatement The last time the V ikings played on Thanksgiving was\\non November 23, 2017, having played before both on November\\n24, 2016 and November 23, 2000[2][3].\\nPreference OptimizationFigure 2: The overall framework of the APO.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/improving attributed text generation of LLM via preferance learning.pdf', 'page': 22}), Document(page_content='Training Language Models to Generate Text with Citations via\\nFine-grained Rewards\\nChengyu Huang\\nNational University of Singapore\\ne0376956@nus.edu.sgZeqiu Wu\\nUniversity of Washington\\nzeqiuwu1@uw.edu\\nYushi Hu\\nUniversity of Washington\\nyushihu@uw.eduWenya Wang\\nNanyang Technological University\\nwangwy@ntu.edu.sg\\nAbstract\\nWhile recent Large Language Models (LLMs)\\nhave proven useful in answering user queries,\\nthey are prone to hallucination, and their re-\\nsponses often lack credibility due to missing\\nreferences to reliable sources. An intuitive solu-\\ntion to these issues would be to include in-text\\ncitations referring to external documents as ev-\\nidence. While previous works have directly\\nprompted LLMs to generate in-text citations,\\ntheir performances are far from satisfactory, es-\\npecially when it comes to smaller LLMs. In this\\nwork, we propose an effective training frame-\\nwork using fine-grained rewards to teach LLMs\\nto generate highly supportive and relevant ci-\\ntations, while ensuring the correctness of their\\nresponses. We also conduct a systematic anal-\\nysis of applying these fine-grained rewards to\\ncommon LLM training strategies, demonstrat-\\ning its advantage over conventional practices.\\nWe conduct extensive experiments on Ques-\\ntion Answering (QA) datasets taken from the\\nALCE benchmark and validate the model’s gen-\\neralizability using EXPERTQA. On LLaMA-2-\\n7B, the incorporation of fine-grained rewards\\nachieves the best performance among the base-\\nlines, even surpassing that of GPT-3.5-turbo.1\\n1 Introduction\\nDespite the recent success of Large Language Mod-\\nels (LLMs), their outputs often contain incorrect or\\nunverifiable claims (Ji et al., 2022). This reduces\\nthe usefulness of their generated content, and more\\nimportantly, hurts the user trust in LLMs. Previ-\\nous work (Guu et al., 2020; Lewis et al., 2020;\\nIzacard et al., 2022) attempts to address this issue\\nwith retrieval-augmented generation (i.e., RAG),\\nwhich makes the LLM condition its generation on\\ninformation retrieved from an external knowledge\\nsource for the task input. However, RAG cannot\\nfully solve the issue because there is no guarantee\\nthat the retrieved knowledge is relevant nor that the\\n1Our code will be released soon.\\nUser questionWhat\\xa0is\\xa0the\\xa0population\\xa0of\\xa0Greenville\\xa0county\\xa0South\\xa0Carolina?\\nRetrieved corpus\\nChatGPT response (In-Context Learning)As\\xa0of\\xa02017,\\xa0the\\xa0estimated\\xa0population\\xa0of\\xa0Greenville,\\xa0South\\xa0Carolina\\xa0is\\xa068,219,\\xa0making\\xa0it\\xa0the\\xa0sixth-largest\\xa0city\\xa0in\\xa0the\\xa0state. The\\xa0population\\xa0of\\xa0the\\xa0surrounding\\xa0area\\xa0was\\xa0400,492\\xa0as\\xa0of\\xa02010,\\xa0 making\\xa0it\\xa0the\\xa0third-largest urban\\xa0area\\xa0in\\xa0South\\xa0Carolina\\xa0 as\\xa0well\\xa0as\\xa0the\\xa0fastest\\xa0growing. Greenville\\xa0County,\\xa0where\\xa0the\\xa0city\\xa0is\\xa0located,\\xa0had\\xa0a\\xa0population\\xa0of\\xa0506,837\\xa0as\\xa0of\\xa02017,\\xa0making\\xa0it\\xa0the\\xa0most\\xa0populous\\xa0county\\xa0in\\xa0the\\xa0state.\\xa0[1][3][4]Missing citation\\nIrrelevant citationMissing citationChatGPT does NOT capture the gold answerGold answer: 451,225\\nFigure 1: An example of ChatGPT performing the task\\nof attributable generation. The model takes a question,\\nretrieved passages, and the task instruction (omitted due\\nto space limit) as the input, and generates a response\\nwith in-text citations. The response has 3 sentences, 2\\nof which do not have supporting citations. The third one\\nhas an irrelevant citation [1]. Moreover, ChatGPT does\\nnot capture the correct answer (451,225) mentioned in\\npassages [3] and [4].\\nLM-generated output is always consistent with the\\nknowledge.\\nRecent studies (Nakano et al., 2021; Gao et al.,\\n2023b) propose to have LLMs generate answers\\nwith in-text citations that provide evidence for the\\ngenerated content, often referred to as attributable\\ntext generation. The cited references provide easy\\naccess for end users to perform fact-checking\\non the model generations. Most existing work\\n(Malaviya et al., 2023; Sun et al., 2023; Li et al.,\\n2023) simply prompts LLMs (e.g., ChatGPT) to\\ninclude citations in their generation. Gao et al.\\n(2023b) conduct systematic evaluations to show\\nthat the model performance can often be far from', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 0}), Document(page_content='2023) simply prompts LLMs (e.g., ChatGPT) to\\ninclude citations in their generation. Gao et al.\\n(2023b) conduct systematic evaluations to show\\nthat the model performance can often be far from\\nsatisfactory with such prompting approaches. InarXiv:2402.04315v1  [cs.CL]  6 Feb 2024', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 0}), Document(page_content='this paper, we aim to study how we can train an\\nLM to perform better at attributable generation.\\nOne challenge in achieving this goal comes from\\nthe expensive cost of collecting supervised training\\ndata that contains gold generations interleaved with\\ncitations. Another challenge stems from the fine-\\ngrained task objective. Instead of a holistic goal of\\ngenerating the correct response to the task input, at-\\ntributable generation usually targets on two critical\\naspects: citation quality and the information cor-\\nrectness of the model response. The citation quality\\ncan be further assessed by two dimensions. The\\nfirst dimension checks if a generated sentence is re-\\nlated to and can be supported by the corresponding\\ncited reference(s). Another dimension inspects if\\nany cited reference in the generation is redundant.\\nTo address these challenges, we resort to two\\ntraining algorithms, namely rejection sampling\\n(RS) and reinforcement learning (RL), and pro-\\npose to use fine-grained rewards as training sig-\\nnals to teach an LM (e.g., LLaMA 2) to gener-\\nate attributable responses. We leverage the fine-\\ngrained automatic evaluation functions from Gao\\net al. (2023b) to provide localized (sentence-level)\\nand specialized (citation quality or response cor-\\nrectness) reward signals as fine-grained rewards.\\nFor training, we adapt from the fine-grained RL\\nalgorithm in Wu et al. (2023) and the tree-decoding\\nmechanism in Asai et al. (2023) to perform rein-\\nforcement learning and rejection sampling, respec-\\ntively, using the fine-grained rewards. To ensure\\na relatively good starting point before RS or RL\\ntraining, we initialize the LM with distillation from\\nChatGPT on a handful of sampled training prompts\\nfrom ALCE (Gao et al., 2023b).\\nWhen evaluating our trained LMs on the test\\nset of ALCE and an out-of-domain dataset EX-\\nPERTQA (Malaviya et al., 2023), our main findings\\nare as follows:\\n•Training an LM with fine-grained rewards for\\nattributable generation is much more effec-\\ntive than a holistic reward. This conclusion\\nholds for both RS and RL. To the best of our\\nknowledge, this is the first work that proposes\\ntotrain an LM with fine-grained rewards for\\nattributable generation.\\n•Training with rejection sampling is more effi-\\ncient and effective than reinforcement learn-\\ning, while combining them leads to the best\\nmodel performance that significantly sur-\\npasses ChatGPT.•We also show the generalizability of RS and\\nRL that they can produce attributable answers\\non EXPERTQA, a separate dataset that re-\\nquires intensive domain knowledge.\\n2 Problem Definition and Methods\\nWe follow Gao et al. (2023b) to define the task of\\nattributable text generation as: Given a query x\\nand a passage corpus D, generate a textual output\\nythat consists of Nsentences s1, s2, . . . , s Nand\\ncitations (such as [1], [1][2], etc.) that refer to\\npassages Ci⊆ D to provide evidence for each\\nsentence si.\\nIn this section, we introduce how to train an LM\\nwith fine-grained rewards to perform the task. As\\nthe corpus Dusually contains millions of passages,\\nwe first retrieve a small set (e.g., top-5) of relevant\\npassages as the first step before training an LM to\\ngenerate the output, conditioning on the query and\\nthe retrieved passages. The details of the retrievers\\nused for each dataset are provided in § 3.1. Figure 2\\nshows the overview of our training process. As\\nsupervision for in-text citations is hard to obtain,\\nwe first initialize an LM with distillation from a\\nproprietary LM such as ChatGPT, as described in\\n§ 2.1. We then introduce how fine-grained rewards\\nare computed and used to train the LM with RL\\nor RS in § 2.2. Finally, we also describe how to\\ntrain the LM with holistic rewards for comparison\\npurposes in § 2.4.\\n2.1 Distillation from ChatGPT\\nGao et al. (2023b) observe that the performance of\\nopen-sourced LMs like LLaMA-2 for attributable\\ngeneration is far from satisfactory. As a result, di-\\nrectly finetuning these LMs using RS or RL leads', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 1}), Document(page_content='open-sourced LMs like LLaMA-2 for attributable\\ngeneration is far from satisfactory. As a result, di-\\nrectly finetuning these LMs using RS or RL leads\\nto undesirable behaviors. Therefore, we first learn\\na better LM capable of generating in-text citations\\nby distilling knowledge from a powerful propri-\\netary model such as ChatGPT2. For each question\\nin the training set, we prompt ChatGPT with in-\\ncontext demonstrations (Brown et al., 2020) to gen-\\nerate answers that contain citations (the prompt is\\nin Appendix I). These generated answers serve as\\nweak supervisions to finetune our target LM. We\\nuse LLaMA-2-7B as the target LM and construct\\nits input as the concatenation of the task instruc-\\ntion, the query x, and the passages retrieved for x.\\nWe denote the resulting model after distillation as\\nDistillation .\\n2We use gpt-3.5-turbo-0301', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 1}), Document(page_content='Prompt:\\n……𝑦1=As ofthe2010 census\\n,thepopulation … thefas\\ntest growing [1].\\nR(x, 𝑦1)=2.0\\n𝑦t=As ofthe2010 census\\n,thepopulation … the20\\n10census [2][5] .\\nR(x, 𝑦𝑡)=1.8R(x,y)=2.4SFT\\nAsofthe 2010 census… the \\nfastest -growing [1][2] .</s>\\nScattered \\nrewards\\nUpdateSample\\nPrompt:\\n…Distillation\\nRejection Sampling\\nReinforcement Learning\\nPrompt:\\n…\\nWhat isthepopulation ofgre\\nenville county south carolina ?\\nDocument [1]…Asof2017, theestimated\\npopulation … \\nthemost populous county\\ninthestate [1][3][4].SFT\\n𝑠11=The population … \\nwas estimated tobe68,2\\n19asof2017 [1].\\n𝑠12=The estimated popul\\nation …68,219 asof2017\\n[1].R(x,concat (𝑦1, 𝑠11))=2.4\\nR(x,concat (𝑦1, 𝑠12))=2.4\\n𝑠𝑡1=…\\n𝑠𝑡2=…R(x,concat (𝑦𝑡, 𝑠𝑡1))=1.6\\nR(x,concat (𝑦𝑡, 𝑠𝑡2))=1.6\\n…Rerank…R(x,y)=2.4\\nR(x,y)=1.6Sample\\n…Asofthe2010 census, thepopulation ofGreenville County, South\\n        𝑅3:+𝑤3;𝑅3\\nCarolina was 451,225 [3][4],making itthemost populous county in   \\n           𝑅2:+𝑤2\\nthestate .Asof2017, theestimated population ofthecounty was          \\n 𝑅3;𝑅3;𝑅2\\n506,837 [3][4].The cityofGreenville, which istheseat ofGreenvill\\neCounty, had anestimated population of68,219 asof2017, makin\\ngitthesixth -\\n             𝑅3;𝑅2\\nlargest cityinthestate [1].The population ofthesurrounding area\\nwas 400,492 asof2010, making itthethird -\\nlargest urban area inSouth Carolina and thefastest -\\n           𝑅3;𝑅3;𝑅2;𝑅1:-2𝑤1\\ngrowing [1][2 ].</s>\\nGold Answers:\\n451,225, 514,213, 379,616, 320,167\\n1 hit: +𝑤1−3𝑤1=−2𝑤1Premise: [3][4]\\nHypothesis: As … \\nstate.NLI Entail +𝑤2Premise: [3]\\nHypothesis: As … \\nstate.NLI Entail +𝑤3[3] is \\nrelevantCitation Precision Reward\\nCitation Recall Reward\\nCorrectness Recall Reward𝑅1𝑅2\\nDistillation\\n+RS\\n+RL𝑅3\\nPPO\\n……\\n…Figure 2: Left: An overview of our framework. We use the same mechanism to compute the rewards for each\\nsentence in rejection sampling and each sequence in reinforcement learning, and the tokens colored in yellow are\\nthose allocated with a reward; Right: The application of our fine-grained rewards on a sample response. For each\\nreward type, we only show the detailed computation of the first token where a reward is allocated.\\n2.2 Training with Fine-grained Rewards\\nAttributable text generation aims to generate re-\\nsponses that contain correct information and high-\\nquality citations. Specifically, the task objective\\nis often decomposed into three aspects (Gao et al.,\\n2023b): information correctness, citation recall,\\nand citation precision. As a result, we use three\\nreward functions to represent these three sub-goals\\nrespectively. Our fine-grained rewards are de-\\ntailed below:\\nCorrectness R1.Correctness of a model-\\ngenerated response to a given query is usually\\nmeasured as the fraction of key information pieces\\nincluded in the response (Stelmakh et al., 2022).\\nSpecifically, in most existing factoid QA datasets\\nwhose questions require synthesizing multiple doc-\\numents to answer (Stelmakh et al., 2022; Rubin\\net al., 2022; Sun et al., 2022), each question comes\\nwith a list of short phrases or important informa-\\ntion statements that cover different aspects of the\\ncomplete answer, referred to as key information\\nlist. We can either apply an exact string match\\n(EM) or use a Natural Language Inference (NLI)\\nmodel to infer whether each item (sub-claim) in the\\nlist is covered in the model response. More details\\ncan be found in Appendix A.1. Note that although\\nin-text citations can be hard to obtain for training,\\nthe key information lists are often provided in ex-isting datasets. For each item in the list, we assign\\na reward of +w1if it is in the response, and −w1\\notherwise. We then sum up the rewards for all list\\nitems to obtain the reward for the whole response.\\nCitation Recall R2.Citation recall is defined\\nas the percentage of sentences in the model re-\\nsponse that can be supported by the corresponding\\ncited passages. We use an NLI model to determine\\nwhether each sentence is entailed by its reference\\npassages. Specifically, the cited passage(s)3are\\ntreated as the premise and the answer sentence is', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 2}), Document(page_content='cited passages. We use an NLI model to determine\\nwhether each sentence is entailed by its reference\\npassages. Specifically, the cited passage(s)3are\\ntreated as the premise and the answer sentence is\\ntreated as the hypothesis for the NLI model. We\\nassign +w2for each sentence that can be entailed\\nby the premise, and −w2otherwise. As a result,\\nR2is computed at the sentence level. See more\\ndetails in Appendix A.2.\\nCitation Precision R3.Citation Precision refers\\nto the percentage of citations generated in the re-\\nsponse that can help support the sentence (Gao\\net al., 2023b). We use the same NLI model used\\ninR2to infer whether each cited passage is nec-\\nessary to entail the sentence. We assign +w3for\\neach citation that is \"relevant\" (See Appenidx A.3),\\nand−w3otherwise. Therefore, we apply R3at the\\ncitation level.\\n3If multiple passages are cited, we concatenate them.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 2}), Document(page_content='2.2.1 Rejection Sampling (RS)\\nRS is a decoding strategy that can be used to gener-\\nate labels for model fine-tuning. Specifically, the\\nmodel is prompted with each training example in-\\nput to generate a number of output samples. Then\\naholistic reward function is used to rank these sam-\\nples and the top-1 output is used as the label for\\nmodel fine-tuning.\\nIn previous studies (Touvron et al., 2023;\\nNakano et al., 2021), RS has proven to be an effec-\\ntive approach to improving model performance. In\\nthis work, we apply a novel fine-grained RS train-\\ning approach to improve the LM’s performance\\non attributable generation after the distillation step\\ndescribed in Section 2.1.\\nWe adopt a sentence-level beam search decoding\\nalgorithm introduced in Asai et al. (2023) to rank\\nmodel outputs with our fine-grained reward func-\\ntions. At each sentence step, the LM generates K\\ndifferent continuation candidates for each sequence\\nin the beam. Then we obtain the top- Bgeneration\\nsequences based on a combined reward, where B\\nis the beam size. We compute the combined reward\\nRas the sum of R1,R2,R3for each generated\\nsequence yi=s1,...,iat a sentence step as:\\nR=P3\\nu=1Plu\\nk=1Rk\\nu (1)\\nwhere luis the number of corresponding segments\\ninyiforRu(e.g., l1= 1 since R1treats the en-\\ntire generation as a single sequence, and l2=i\\nsince R2is computed at the sentence level). Rk\\nu\\ndenotes the reward score Rufor the kthsegment\\ninyi. The whole process stops until all sequences\\nin the beam reaches the end of generation or the\\nmaximum number of steps His reached. Then the\\nhighest ranked sequence is returned for further LM\\nfine-tuning as in a standard RS process.\\n2.2.2 Reinforcement Learning (RL)\\nWe also experiment with LM training using RL af-\\nter the distillation step. Following Wu et al. (2023),\\nwe optimize the policy LM using proximal policy\\noptimization (PPO) (Schulman et al., 2017) with\\nfine-grained rewards. We adapt the token-level re-\\nward assignment in Wu et al. (2023) to our own\\nfine-grained reward functions by assigning i) the\\nsequence-level R1to the EOS token of each gen-\\nerated response; ii) the R2score for each sentence\\nto the last token of that sentence; iii) the R3score\\nfor each bracketed citation to its end token (i.e., theright bracket). If a token is assigned by multiple\\nreward functions, we take their sum to be the final\\nreward for that token.\\nFormally, given a (prompt, response) pair (x, y),\\nwe denote the token time step of the EOS token\\nofyasT1\\n1, the time step of the end token of each\\nsentence siasT2\\ni, and the time step of the end token\\nfor each citation cjasT3\\nj. We use θandθinitto\\nrepresent the current and initial policy respectively\\nand refer to Pas the probability distribution over\\nthe next token given the generation prefix gtat time\\nstept. The reward for each token atgenerated at\\ntime step tinyis:\\nrt=P3\\nu=1Plu\\nk=1(1(t=Tu\\nk)Rk\\nu)−βlogPθ(at|gt)\\nPθinit(at|gt)(2)\\nwhere l1,l2, andl3denote the number of sequences\\n(which is 1), sentences, and citations in yandβis\\nthe coefficient for the KL penalty commonly used\\nin PPO. Rk\\nuhere denotes Rufor the kthsegment\\niny.\\nWith the above reward assignment, we follow\\nthe standard PPO algorithm to optimize both the\\npolicy and value models.\\n2.3 Combining RS and RL\\nStacking RL on top of RS is shown to be effective\\n(Touvron et al., 2023) for LLM training. There-\\nfore, we experiment with combining RL and RS\\nfor model fine-tuning. Starting from Distillation ,\\nwe first apply RS to fine-tune the model, and then\\napply RL to further train it.\\n2.4 Training with Holistic Rewards\\nFor comparison purposes, we also analyze model\\nperformances when trained with holistic rewards.\\nIn this setting, each generated response is assigned\\nwith a single overall reward score. Similarly, we\\nexperiment with both RS and RL for model fine-\\ntuning. We use Rin Eq. 1 when i=Nas our\\nholistic reward.\\nRS We follow the standard RS pipeline to first\\nhave the LM generate multiple outputs and then\\nrank them by the holistic reward R. The best gen-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 3}), Document(page_content='holistic reward.\\nRS We follow the standard RS pipeline to first\\nhave the LM generate multiple outputs and then\\nrank them by the holistic reward R. The best gen-\\nerated output is then used for model fine-tuning. In\\ncontrast, our fine-grained RS samples and ranks the\\ngenerated sequences at each sentence step, which\\nallows finer and better control over the sampling\\nprocess.\\nRL We assign the holistic reward to the last token\\nof each generated sequence during training and all', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 3}), Document(page_content='other tokens receive a reward of 0. In contrast, the\\nfine-grained RL training assigns denser rewards to\\ntokens in a sequence. Similar to Eq. 2, we also add\\nthe KL penalty term to the reward for each token.\\nThis RL training process is designed in the same\\nway as in most recent work in RL from human\\npreference feedback (Menick et al., 2022; Nakano\\net al., 2021; Touvron et al., 2023).\\n3 Experiment Setup\\n3.1 Datasets\\nWe fine-tune our LM with three datasets in the\\nALCE (Gao et al., 2023b) benchmark designed\\nfor the task of attributable generation: (1) ASQA\\n(Stelmakh et al., 2022), a long-form QA dataset\\ncontaining ambiguous questions that can have dif-\\nferent interpretations and corresponding answers\\nunder different contexts; (2) QAMPARI (Rubin\\net al., 2022), a factoid QA dataset where the answer\\nto each question is a list of entries; (3) ELI5 (Fan\\net al., 2019), a long-form QA dataset with mostly\\nWhy/How/What type of questions. Most questions\\nin these three datasets require the model to syn-\\nthesize knowledge from multiple passages in order\\nto construct a complete answer. Each dataset has\\nits own passage corpus and each example comes\\nwith the question as well as a list of key informa-\\ntion pieces that should be covered in a gold answer.\\nNo citations are provided. We use GTR (Ni et al.,\\n2022) or BM25 to perform retrieval to get the top-5\\npassages from the corresponding corpus for each\\nquestion, following Gao et al. (2023b). More de-\\ntails are in Appendix B.2.\\nWe train the LM on all of the three QA datasets.\\nSpecifically, we pre-process the training examples\\nto address the imbalance of the training sets among\\nthe three datasets. This results in roughly 3k/1k\\ntrain/dev examples evenly distributed over the three\\ndatasets. Furthermore, as ALCE only provides\\ndevelopment set examples, we re-assemble and\\nobtain around 3k test set examples with roughly\\n1k instances from ASQA, QAMPARI, and ELI5\\neach. More details about data processing are in\\nAppendix B.1. We also conduct experiments to\\ntrain LMs for different datasets separately. This\\nsetting is denoted as separate and its results are in\\nAppendix D.\\nTo verify the generalizability of our framework,\\nwe test our model using a dataset unseen dur-\\ning training, EXPERTQA (Malaviya et al., 2023),\\nwhich is another long-form QA dataset whose ques-tions requires knowledge in specific domains (e.g.,\\npsychology) to answer. After filtering out a few\\nsamples that do not have human-revised answers\\n(We need to infer sub-claims from the annotated\\nanswers to compute R1, as in the case of ELI5 (See\\nAppendix A.1)), we get 2169 samples for evalua-\\ntion. More details are in Appendix B.1.\\n3.2 Evaluation Metrics\\nOur evaluation metrics are aligned with the reward\\nmodels, including Correctness Recall, Citation Re-\\ncall, and Citation Precision discussed in ALCE4.\\nWe illustrate their computations with a response\\nywithl2sentences and l3citations to a question\\nwithtgold answers (key information pieces). See\\nexamples in Figure 3.\\nCorrectness Recall Suppose ycaptures hout of\\ntkey information pieces (i.e., they have a Correct-\\nness Recall reward of +w1(See Appendix A.1)),\\nthen its Correctness Recall ish\\nt.\\nMore specifically, since the key information lists\\nare available in ASQA and QAMPARI, we com-\\npute an exact string match recall (EM Rec) that\\nchecks how many key information pieces are an\\nexact substring of generated responses. On top of\\nthat, following (Gao et al., 2023b), we compute\\nRec.-5 (i.e.,min(h,5)\\nmin(t,5)) on QAMPARI, so that the\\nrecall will be 100 if at least 5 correct answers are\\nhit. On ELI5, we only have a single long-form\\nanswer for each question. Therefore, we follow\\n(Gao et al., 2023b) to infer three sub-claims for\\neach answer with text-davinci-003, and use them\\nas the key information pieces. We then use an NLI\\nmodel to check how many sub-claims are entailed\\nby the model response (Claim Rec) .\\nAdditionally, on QAMPARI, we directly ask the\\nmodel to format its response as a list of items. To', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 4}), Document(page_content='model to check how many sub-claims are entailed\\nby the model response (Claim Rec) .\\nAdditionally, on QAMPARI, we directly ask the\\nmodel to format its response as a list of items. To\\ncheck whether the model is blindly making more\\nguesses to increase Correctness Recall, we include\\naCorrectness Precision metric on QAMPARI. It\\nmeasures the percentage of predicted items that\\nmatch a key information piece. Suppose there are t’\\ncomma-separated entries in the generated response,\\nthen the Correctness Precision will beh\\nt′.\\nCitation Recall Suppose l′\\n2out of l2sentences\\nare entailed by the cited passages (i.e., they each\\n4While ALCE also uses MAUVE (Pillutla et al., 2021) to\\nevaluate fluency, it is sensitive to output length and unstable.\\nTherefore, we do not include it as a metric in the main analysis,\\nbut we still report its results in Appendix D.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 4}), Document(page_content='have a Citation Recall reward of +w2(See Ap-\\npendix A.2)), then the Citation Recall isl′\\n2\\nl2.\\nOn QAMPARI, since the model generates a\\ncomma-separated list instead of a natural language\\nresponse, we treat each comma-separated item (for-\\nmatted as ’{question} {item}’) as a sentence.\\nCitation Precision Suppose l′\\n3out of l3citations\\nare relevant, (i.e., they each have a Citation Preci-\\nsion reward of +w3(See Appendix A.3)), then the\\nCitation Precision isl′\\n3\\nl3.\\nOn EXPERTQA, we use their metrics instead.\\nAutoAIS (Gao et al., 2023a) is a metric similar\\nto our Citation Recall that measures the percent-\\nage of sentences supported by the citations, but it\\ndiffers from ours in detailed implementation (e.g.,\\nmeasures to handle long sequences that exceed the\\nNLI model’s context window). Another metric is\\nFActscore (Min et al., 2023), which measures the\\npercentage of sentences generated by the model\\nthat are factual but not necessarily relevant to the\\nquestion.\\n3.3 Training Details\\nWe fine-tune LLaMA-2-7B (Touvron et al., 2023)\\nin the main experiments. For RS with fine-grained\\nrewards, we use a beam width ( B) of 8. We set\\na sentence-level maximum search depth ( H) of 5\\non ASQA and ELI5, and 10 on QAMPARI, since\\nQAMPARI questions have more key information\\npieces to answer on average. We sample 2 continua-\\ntions ( K) for each sequence in a single search step.\\nFor holistic RS, we set the number of sampling\\nsequences to 16 to make a fair comparison. For\\nreward computation, we use TRUE5(Honovich\\net al., 2022), a T5-11B model for NLI, to mea-\\nsure entailment for correctness recall on ELI5, and\\ncitation recall and precision across our datasets.\\nFor efficiency, we replace TRUE with a smaller\\nNLI model, namely attrscore-flan-t5-xl (Yue et al.,\\n2023), during the RLHF training stage.\\nIn terms of reward weights, we set each of w1,\\nw2, and w3to 0.2 throughout the RS and RL pro-\\ncess for LLaMA-2-7B. Other training details are\\nshown in Appendix C. We repeat each RL exper-\\niment 3 times, each with a different seed. The re-\\nsults are computed as the average of the inference\\nresults of these 3 independent runs.\\n5See https://huggingface.co/google/t5_xxl_\\ntrue_nli_mixture .3.4 Baselines\\nAs mentioned in § 2.1, we initialize the language\\nmodel (LLaMA-2-7B) with distillation using the\\ntraining examples generated from ChatGPT. Af-\\nter this step, we leverage fine-grained rewards to\\nfine-tune the model with rejection sampling ( f.g.\\nRS) or reinforcement learning ( f.g. RL ). We also\\nexperiment with combining RS and RL for model\\nfine-tuning ( f.g. RS+RL ). As discussed in § 2.4,\\nwe also report results of using holistic rewards for\\nRS (h. RS ), RL ( h. RL ) and RS+RL ( h. RS+RL ).\\nWe compare the above fine-tuned LMs with re-\\nsults from in-context learning ( ICL) (Brown et al.,\\n2020). Specifically, we prompt the LM with 2 in-\\ncontext demonstrations where each contains the\\nquestion, 5 retrieved documents, and an answer\\nwith citations. The prompt template is shown in\\nAppendix I. We report ICL results with ChatGPT\\n(GPT-3.5-turbo) or LLaMA-2-7B .\\n4 Results and Analysis\\nWe first compare all the systems (§ 4.1), and then\\nconduct ablation study on the training effect of\\neach reward type (§ 4.2). We also analyze the\\neffect of retrieval on the final model performance\\n(§ 4.3), as well as the citation errors that we observe\\n(§ 4.4). Lastly, we test our model’s generalization\\ncapability on EXPERTQA that is unseen during\\ntraining (§ 4.5).\\n4.1 Main Results\\nThe main results are summarized in Table 1 and we\\nhave the key observations as follows.\\nTraining with fine-grained rewards greatly\\nboosts performance. Adding f.g. RL training\\nto the initial model (Distillation), we observe per-\\nformance increases on every metric and dataset, ex-\\ncept for Citation Recall on QAMPARI and Claim\\nRecall on ELI5. Compared to RL, training with RS\\ngenerally leads to more performance improvement\\nacross the three datasets. However, we observe\\nthat combining RS and RL leads to the best perfor-\\nmance.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 5}), Document(page_content='generally leads to more performance improvement\\nacross the three datasets. However, we observe\\nthat combining RS and RL leads to the best perfor-\\nmance.\\nIn addition, there is a much larger performance\\nboost on ASQA and ELI5 with fine-tuning than on\\nQAMPARI. We suspect the reason behind this to\\nbe the fact that gold responses of QAMPARI are\\nlists of entities, while the other two datasets have\\nnatural language responses, making training biased\\ntowards the latter format. Appendix D shows that', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 5}), Document(page_content='ASQA QAMPARI ELI5\\nCorrectness Citation Correctness Citation Correctness Citation\\nEM Rec Rec Prec Rec.-5 Prec Rec Prec Claim Rec Rec Prec\\nICL ChatGPT 39.96 74.72 70.97 18.34 18.57 17.65 20.60 13.47 50.94 47.58\\nICL LLaMA-2-7B 34.15 14.12 15.26 8.24 9.23 7.51 7.65 7.83 14.44 8.92\\nDistillation 35.56 74.80 67.99 17.26 16.18 18.69 18.94 12.03 49.69 45.71\\nRL/RS only\\n+h.RL 34.33 75.77 70.12 17.30 16.44 16.39 18.92 11.52 51.77 49.32\\n+f.g.RL 35.99 76.30 72.38 18.39 18.81 17.82 19.07 11.60 51.29 51.09\\n+h.RS 37.96 74.86 68.48 14.62 15.21 16.71 17.98 11.60 54.10 48.95\\n+f.g.RS 40.07 76.71 74.35 16.14 18.95 18.56 19.50 11.67 58.75 55.03\\nRS + RL\\n+h.(RS+RL) 37.33 74.86 69.37 15.02 15.67 16.82 18.09 11.21 55.62 50.58\\n+f.g.(RS+RL) 40.05 77.83 76.33 16.65 19.54 19.50 20.03 11.54 60.86 60.23\\nTable 1: Main result on the test set of three QA datasets. ICL: In-Context Learning; EM Rec: String Exact Match\\nRecall; Rec.-5: Calibrated recall that is 100 if the predicted response includes at least 5 correct answers.\\nin the separate training setting, fine-tuning with\\neither RS, RL or RS+RL can give a significant\\nperformance boost for QAMPARI.\\nFine-grained rewards help smaller LLMs sur-\\npass ChatGPT. Although the performance gap\\nstill remains between LLaMA-2 and ChatGPT after\\nthe distillation step, LLaMA-2 trained with fine-\\ngrained rewards can close the gap or even outper-\\nform ChatGPT. Training with a combined RS and\\nRL leads to an average of 4.0%, 0.9%, 10.6% im-\\nprovements over ChatGPT among all evaluation\\nmetrics on ASQA, QAMPARI, and ELI5 respec-\\ntively.\\nFine-grained rewards are better than holistic\\nrewards. We compare fine-grained rewards with\\nholistic ones for model training. Table 1 shows the\\noutstanding effectiveness of fine-grained rewards\\nin all training setups (RS, RL, RS+RL). We ob-\\nserve higher performance gains with fine-grained\\nrewards in almost all metrics and all datasets. The\\nonly exception is that the citation recall on ELI5 is\\nsimilar for h. RL and f.g. RL. The performance gap\\nbetween using fine-grained and holistic rewards is\\nwider for RS compared to RL.\\n4.2 Ablation of Reward Models\\nWe conduct our ablation study to understand how\\nlearning to optimize one single task objective (an-\\nswer correctness or attribution quality) affects the\\nmodel performance. Since we show in § 4.1 that RS\\ngenerally gives better results than RL, we conduct\\nthis ablation analysis on f.g.RS. We train the LM\\non each individual QA dataset by using either the\\nrewards associated with answer correctness ( R1)or citation quality ( R2andR3). We report the eval-\\nuation results in Table 2.\\nTo show the effect of only optimizing towards\\nanswer correctness, we remove R2andR3during\\nthe sentence-level beam search. As indicated by\\nthe number of tokens (# Tokens) metric in Table 2,\\nif the model is only given the correctness reward,\\nit will increase its response length in an attempt to\\ncapture more gold answers. Citation quality is ig-\\nnored and decreases quickly. As a result, the model\\nachieves the highest correctness recall compared to\\nother systems or training settings.\\nWe also attempt to focus only on citation qual-\\nity and remove R1during the sentence-level beam\\nsearch. The models produce better results on cita-\\ntion quality on the long-form QA datasets, while\\ntheir correctness recall drops to a certain extent.\\n4.3 Retrieval Analysis\\nSince the retrieved documents may not cover every\\ngold answer (key information piece) in the first\\nplace (Gao et al., 2023b), we calibrate the model’s\\ncorrectness metrics, so that we can directly inspect\\nthe model’s ability to synthesize information and\\ncapture important answers in the given documents.\\nSpecifically, we ignore gold answers that are not\\nrecalled by the top 5 retrieved documents when\\ncalculating the model’s correctness. Numerical\\nresults are shown in Appendix E.\\nTrained LLMs are effective answer retrievers,\\nbut space for improvement exists. Our fine-\\ngrained methods (f.g. RL; f.g. RS; f.g.(RS+RL))', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 6}), Document(page_content='results are shown in Appendix E.\\nTrained LLMs are effective answer retrievers,\\nbut space for improvement exists. Our fine-\\ngrained methods (f.g. RL; f.g. RS; f.g.(RS+RL))\\nare able to capture around 70 percent of whatever\\nis already captured by the documents on ASQA\\nand QAMPARI, and this number becomes more', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 6}), Document(page_content='ASQA QAMPARI ELI5\\nCorrectness Citation Length Correctness Citation Length Correctness Citation Length\\nEM Rec Rec Prec #Tokens Rec.-5 Prec Rec Prec #Tokens Claim Rec Rec Prec #Tokens\\nICL ChatGPT 39.96 74.72 70.97 37.51 18.34 18.57 17.65 20.60 13.70 13.47 50.94 47.58 86.06\\n+f.g.RS 40.24 77.65 74.96 51.34 17.48 20.67 20.62 21.65 11.24 11.87 61.27 56.45 83.01\\nw/o.R2+R3 41.29 49.51 67.54 56.34 23.12 18.13 15.99 16.73 15.29 13.60 41.06 43.38 88.38\\nw/o.R1 39.79 79.42 75.69 55.89 12.62 17.63 20.31 21.45 10.91 11.17 62.92 58.51 84.26\\nTable 2: Ablation study results on the three QA datasets.\\nthan 90 percent on ELI5. While this shows that\\nour methods can comprehend and retrieve most\\nanswers in the documents, there is still space to\\nimprove correctness recall by capturing the remain-\\ning 30 percent of answers in the documents on\\nASQA and QAMPARI. We believe that a correct-\\nness recall that is higher than the recall of the doc-\\numents themselves is very possible, as is in the\\ncase of ELI5 (The document recall is only 9.13,\\nwhile ChatGPT’s correctness recall is 13.47), since\\nLLMs have extra parametric knowledge embedded\\nduring pretraining. More investigation on this can\\nbe included in future work.\\n4.4 Citation Error Analysis\\nWe randomly select and inspect responses gener-\\nated by f.g.(RS+RL) for 30 test examples (10 from\\nASQA/QAMPARI/ELI5 each), and observe that\\nthe most common types of citation errors the model\\nmakes are as below.6\\nMixing up of passage ids (4.76%). This refers to\\nthe case where the claim is unsupported as a result\\nof the model mixing up the passage ids. As shown\\nin Table 6, the model sometimes correctly extracts\\nand paraphrases the content from one passage, but\\nattributes them to an incorrect passage.\\nRedundant citations (28.57%). This refers to\\nthe case where passages that are irrelevant to the\\nresponse are cited. An example case is when no\\nretrieved passage is relevant to the question and\\nthe model generates the response based on its own\\nparametric knowledge (See Table 7). This error\\nis common on the long-form QA datasets ASQA\\nand ELI5 (We explicitly ask the model to attach\\nonly 1 citation to each item in the response on\\nQAMPARI following Gao et al. (2023b)), and it\\ndirectly impacts Citation Precision.\\n6There are miscellaneous errors (23.80%) that are hard to\\ncategorize.Misinterpretation of cited passages (42.86%).\\nWe also observe that the model can interpret and\\nparaphrase the facts mentioned in the documents\\nincorrectly . This error emerges frequently in QAM-\\nPARI, where the question usually involves multi-\\nhop relations between multiple entities and the\\nmodel can easily make mistakes in the reasoning\\nchain, as shown in Table 8.\\nIn addition, we notice that our LLMs’s responses\\nare more extractive after training. A significant pro-\\nportion of the generated text is directly extracted\\nfrom the documents. Consequently, when halluci-\\nnation happens, it can often be attributed to the mis-\\ninterpretation error mentioned above, which makes\\nhallucination errors easier to locate and interpret.\\n4.5 Analysis on Generalizability\\nEXPERTQA\\nAutoAIS FActscore #Sents\\nICL ChatGPT 56.98 85.83 8145\\nICL LLaMA-2-7B 19.63 82.77 10053\\nDistillation 51.33 83.46 7293\\nRL/RS only\\n+h.RL 53.64 83.82 8139\\n+f.g.RL 56.15 83.89 7322\\n+h.RS 57.88 83.47 8436\\n+f.g.RS 63.49 83.85 7450\\nRS+RL\\n+h.(RS+RL) 59.42 83.71 8012\\n+f.g.(RS+RL) 66.12 83.78 6256\\nTable 3: Result on EXPERTQA. # Sents: Same as\\nMalaviya et al. (2023), we treat each sentence in the\\nresponse as a unit to compute AutoAIS and FActscore.\\nWe report the total number of sentences summed up\\nfrom the responses to the 2169 samples.\\nTo demonstrate the generalizability of our\\nmodel, we also evaluate it on a separate dataset,\\nEXPERTQA (Malaviya et al., 2023). We adopt\\nthe evaluation metrics used in EXPERTQA that\\nmeasure citation quality and factuality and the\\nresults are shown in Table 3.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 7}), Document(page_content='The capability of attributable generation is gen-\\neralizable. Similar to the previous three datasets,\\nthe model trained with fine-grained rewards gives\\nthe highest AutoAIS scores (Gao et al., 2023a),\\na metric measuring the percentage of sentences\\nthat are supported by citations. This indicates the\\nrobustness and transferability of the model’s attri-\\nbution capability trained with fine-grained rewards.\\nTrained LLMs have high factual precision.\\nWhile we do not use factuality as a reward model,\\nthe FActscore (Min et al., 2023) metric shows that\\nmore than 80 % of the sentences that our meth-\\nods generate are factual, which is close to that of\\nChatGPT.\\n5 Related Work\\nRetrieval-Augmented Language Models. Many\\nprior works on RALMs exist for tasks such as\\nquestion-answering and fact-checking. Aside from\\nthe works on kNN-LM (Khandelwal et al., 2020;\\nZhong et al., 2022), most of them involve (1) doc-\\nument selection and (2) document reading. Many\\nworks attempted to improve document selection by\\ntraining a stronger retriever or applying additional\\nfiltering mechanisms. Methods such as REALM\\n(Guu et al., 2020), RAG (Lewis et al., 2020), and\\nAtlas (Izacard et al., 2022) train the retrievers and\\nLMs jointly in an end-to-end setting. Lin et al.\\n(2022) fintunes the LMs and retrievers in two sep-\\narate stages that complement each other. Others\\nmade efforts to train an extra NLI model (Yoran\\net al., 2023) or only the LLMs themselves (Luo\\net al., 2023; Asai et al., 2023) to filter the retrieved\\ndocuments. Other lines of works explore effec-\\ntive retrieve on-the-fly methods (Parisi et al., 2022;\\nSchick et al., 2023; Jiang et al., 2023; Ram et al.,\\n2023; Gao et al., 2023b) and also post-hoc retrieval\\nfor answer attribution or revision (Gao et al., 2023b;\\nMalaviya et al., 2023; He et al., 2022).\\nMost previous works do not include explicit\\ncitations or quotes in their models’ responses.\\nHowever, the ability to generate citations can not\\nonly reduce hallucination but also increase verifia-\\nbility on a sentence-level granularity. Such ability\\nbecomes crucial in factoid, generation-based,\\nlong-form QA, where answers to the questions\\ntypically need to present multiple perspectives, and\\neach perspective needs to be grounded (Stelmakh\\net al., 2022; Rubin et al., 2022; Fan et al., 2019).\\nAmong those prior methods that do provide\\ncitations, they either are too reliant on searchengines and difficult to reproduce (Nakano et al.,\\n2021), only consider simple questions requiring a\\nsingle source (Menick et al., 2022), do not achieve\\nsatisfactory results especially when comparing\\nsmaller LLMs with ChatGPT (Gao et al., 2023b;\\nMalaviya et al., 2023; Asai et al., 2023), or use\\nexcessively sophisticated mechanism that incur\\nhigh inference cost (Sun et al., 2023; Ye et al.,\\n2023). The performance of existing commercial\\nsystems like BingChat is neither satisfactory in\\nterms of their citation recall as demonstrated by\\nMalaviya et al. (2023).\\nTraining with Reward Models. Recent works\\nuse reward models to train LLMs for various tasks.\\nIn the context of RALMs, Menick et al. (2022)\\ntrains a preference-based reward model and then\\nuses Reinforcement Learning from Human Prefer-\\nences (RLHP) to train its model to quote knowl-\\nedge sources. Nakano et al. (2021) trains similar\\nreward models, but they also use them for best-of-n\\nsampling during inference on top of RLHP. Zhou\\net al. (2023) uses tree search for QA tasks and uti-\\nlizes an LM value function to generate scores for\\neach solution trajectory. A common characteristic\\nof these reward models is that they only measure\\nthe overall quality of a response. A more natural\\nand informative reward model for citations is one\\nthat is fine-grained since citations can be scattered\\nthroughout a piece of text. Inspired by Wu et al.\\n(2023), we divide a holistic reward into more spe-\\ncific ones using both reward density and category.\\n6 Conclusion\\nIn this work, we systematically analyze common\\nLLM training methods for attributable text gener-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 8}), Document(page_content='cific ones using both reward density and category.\\n6 Conclusion\\nIn this work, we systematically analyze common\\nLLM training methods for attributable text gener-\\nation. We show that training with fine-grained re-\\nwards gives the strongest performance boost and it\\nalso enables small LLMs to outperform ChatGPT.\\nWe propose a number of directions to explore.\\nFirstly, our retrieval analysis shows room to further\\nimprove LLMs’ correctness recall on QA datasets,\\nand more work can be done to enhance LLMs’\\nreading comprehension and synthesis ability given\\nthe retrieved documents. Secondly, according to\\nTouvron et al. (2023), iteratively applying imitation\\nlearning and reinforcement learning for multiple\\nrounds can continuously increase the model’s per-\\nformance. While this is not the main focus of our\\nwork, it will be intriguing to see how far a smaller\\nLLM can go in our task setting using this approach.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 8}), Document(page_content='Limitations\\nThere are several limitations of our work. Firstly,\\nour correctness reward model on ELI5 may not\\nreflect the correctness recall fully, since the sub-\\nclaims inferred by text-davinci-003 can be incor-\\nrect or incomplete when compared with the orig-\\ninal text answer. This can cause inconsistencies\\nbetween the training objectives, evaluation metrics,\\nand actual correctness. Secondly, our method still\\nrequires an initial distillation step with ChatGPT.\\nThis can possibly hinder accessibility when larger,\\nmore capable LLMs are not available. A possible\\nresolution to this is to iteratively use In-Context\\nLearning and beam search sampling to bootstrap\\nresponses of high quality, and then apply behav-\\nioral cloning and reinforcement learning to update\\nthe model weights, which can be a future direction\\nto explore.\\nReferences\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\\nHannaneh Hajishirzi. 2023. Self-rag: Learning to\\nretrieve, generate, and critique through self-reflection.\\narXiv preprint arXiv:2310.11511, 2023 .\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nDaniel Ziegler Aditya Ramesh, Jeffrey Wu, Clemens\\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\\nClark, Christopher Berner, Sam McCandlish, Alec\\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\\nLanguage models are few-shot learners. In Advances\\nin Neural Information Processing Systems .\\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\\nof quantized llms. arXiv preprint arXiv:2305.14314 .\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\\nier, Jason Weston, and Michael Auli. 2019. Eli5:\\nLong form question answering. In Association for\\nComputational Linguistics (ACL) , pages 3558—-\\n3567.\\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent\\nZhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and\\nKelvin Guu. 2023a. Rarr: Researching and revising\\nwhat language models say, using language models.\\nInProceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers) , page 16477–16508.Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\\n2023b. Enabling large language models to generate\\ntext with citations. In Proceedings of the 2023 Con-\\nference on Empirical Methods in Natural Language\\nProcessing , pages 6465—-6488.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\\nand Mingwei Chang. 2020. Realm: Retrieval aug-\\nmented language model pre-training. In Proceedings\\nof International Conference on Machine Learning,\\n2020 , pages 33–40.\\nHangfeng He, Hongming Zhang, and Dan Roth. 2022.\\nRethinking with retrieval: Faithful large language\\nmodel inference. arXiv preprint arXiv:2301.00303,\\n2022 .\\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\\nScialom, Idan Szpektor, Avinatan Hassidim, and\\nYossi Matias. 2022. True: Re-evaluating factual con-\\nsistency evaluation. In North American Chapter of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies (NAACL-HLT) , pages\\n3905–3920.\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\\nand Weizhu Chen. 2021. Lora: Low-rank adap-\\ntation of large language models. arXiv preprint\\narXiv:2106.09685, 2021 .\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\\nYu, Armand Joulin, Sebastian Riedel, and Edouard\\nGrave. 2022. Atlas: Few-shot learning with re-\\ntrieval augmented language models. arXiv preprint\\narXiv:2208.03299, 2022 .\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\\nMadotto, and Pascale Fung. 2022. Survey of halluci-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 9}), Document(page_content='arXiv:2208.03299, 2022 .\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\\nMadotto, and Pascale Fung. 2022. Survey of halluci-\\nnation in natural language generation. ACM Comput-\\ning Surveys , 55(12):1––38.\\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing\\nSun, Qian Liu, Jane Dwivedi-Yu, and Yiming Yang.\\n2023. Active retrieval augmented generation. arXiv\\npreprint arXiv:2305.06983, 2023 .\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2020. Generalization\\nthrough memorization: Nearest neighbor language\\nmodels. In International Conference on Learning\\nRepresentations .\\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-\\ncient memory management for large language model\\nserving with pagedattention. In Proceedings of the\\nACM SIGOPS 29th Symposium on Operating Systems\\nPrinciples .', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 9}), Document(page_content='Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rock-\\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\\nRetrieval-augmented generation for knowledge-\\nintensive nlp tasks. In Advances in Neural Infor-\\nmation Processing Systems .\\nXinze Li, Yixin Cao, Liangming Pan, Yubo Ma, and\\nAixin Sun. 2023. Towards verifiable generation: A\\nbenchmark for knowledge-aware language model at-\\ntribution. arXiv preprint arXiv:2310.05634 .\\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia\\nShi, Maria Lomeli, Rich James, Pedro Rodriguez,\\nJacob Kahn, Gergely Szilvasy, Mike Lewis, Luke\\nZettlemoyer, and Scott Yih. 2022. Ra-dit: Re-\\ntrievalaugmented dual instruction tuning. arXiv\\npreprint arXiv:2310.01352, 2022 .\\nHongyin Luo, Yung-Sung Chuang, Yuan Gong, Tian-\\nhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, He-\\nlen Meng, and James Glass. 2023. Sail: Search-\\naugmented instruction learning. arXiv preprint\\narXiv:2305.15225, 2023 .\\nChaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth\\nSieber, Mark Yatskar, and Dan Roth. 2023. Ex-\\npertqa: Expert-curated questions and attributed an-\\nswers. arXiv preprint arXiv:2309.07852, 2023 .\\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John\\nAslanides, Francis Song, Martin Chadwick, Mia\\nGlaese, Susannah Young, Lucy Campbell-Gillingam,\\nGeoffrey Irving, and Nat McAleese. 2022. Teaching\\nlanguage models to support answers with verified\\nquotes. arXiv preprint arXiv:2203.11147, 2022 .\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\\nFactscore: Fine-grained atomic evaluation of factual\\nprecision in long form text generation. arXiv preprint\\narXiv:2305.14251v1 .\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders,\\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\\nKrueger, Kevin Button, Matthew Knight, Benjamin\\nChess, and John Schulman. 2021. Webgpt: Browser-\\nassisted question answering with human feedback.\\narXiv preprint arXiv:2112.09332, 2021 .\\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Her-\\nnandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,\\nKeith Hall, Ming-Wei Chang, and Yinfei Yang. 2022.\\nLarge dual encoders are generalizable retrievers. In\\nProceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing , pages\\n9844—-9855.\\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\\nTool augmented language models. arXiv preprint\\narXiv:2205.12255, 2022 .Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\\nDmytro Okhonko, Samuel Broscheit, Gautier Izacard,\\nPatrick Lewis, Barlas O ˘guz, Edouard Grave, Wen\\ntau Yih, and et al. 2021. The web is your oyster\\n- knowledge-intensive nlp against a very large web\\ncorpus. arXiv preprint arXiv:2112.09924, 2021 .\\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\\nHarchaoui. 2021. Mauve: Measuring the gap be-\\ntween neural text and human text using divergence\\nfrontiers. In Advances in Neural Information Pro-\\ncessing Systems .\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2020. Exploring the limits\\nof transfer learning with a unified text-to-text trans-\\nformer. The Journal of Machine Learning Research\\n(JMLR) , 21(140).\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. In Transactions of the Association for\\nComputational Linguistics, 2023 .\\nSamuel Joseph Amouyal Ohad Rubin, Ori Yoran, Tomer\\nWolfson, Jonathan Herzig, and Jonathan Berant.\\n2022. Qampari: An open-domain question an-\\nswering benchmark for questions with many an-\\nswers from multiple paragraphs. arXiv preprint\\narXiv:2205.12665, 2022 .\\nMichael Santacroce, Yadong Lu, Han Yu, Yuanzhi', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 10}), Document(page_content='swering benchmark for questions with many an-\\nswers from multiple paragraphs. arXiv preprint\\narXiv:2205.12665, 2022 .\\nMichael Santacroce, Yadong Lu, Han Yu, Yuanzhi\\nLi, and Yelong Shen. 2023. Efficient rlhf: Re-\\nducing the memory usage of ppo. arXiv preprint\\narXiv:2309.00754, 2023 .\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\\nCancedda, and Thomas Scialom. 2023. Toolformer:\\nLanguage models can teach themselves to use tools.\\narXiv preprint arXiv:2302.04761, 2023 .\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\\nAlec Radford, and Oleg Klimov. 2017. Proxi-\\nmal policy optimization algorithms. arXiv preprint\\narXiv:1707.06347, 2017 .\\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and\\nMing-Wei Chang. 2022. Asqa: Factoid ques-\\ntions meet long-form answers. arXiv preprint\\narXiv:2204.06092, 2022 .\\nHaitian Sun, William Cohen, and Ruslan Salakhutdinov.\\n2022. Conditionalqa: A complex reading comprehen-\\nsion dataset with conditional answers. In Proceed-\\nings of the 60th Annual Meeting of the Association\\nfor Computational Linguistics , page 3627–3637.\\nHao Sun, Hengyi Cai, Bo Wang, Yingyan Hou, Xi-\\naochi Wei, Shuaiqiang Wang, Yan Zhang, and Dawei\\nYin. 2023. Towards verifiable text generation with\\nevolving memory and self-reflection. arXiv preprint\\narXiv:2312.09075 .', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 10}), Document(page_content='Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, and et al. 2023. Llama 2: Open foun-\\ndation and fine-tuned chat models. arXiv preprint\\narXiv:2307.09288, 2023 .\\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane\\nSuhr, Prithviraj Ammanabrolu, Noah A Smith, Mari\\nOstendorf, and Hannaneh Hajishirzi. 2023. Fine-\\ngrained human feedback gives better rewards for lan-\\nguage model training. In Advances in Neural Infor-\\nmation Processing Systems .\\nXi Ye, Ruoxi Sun, Sercan Ö. Arik, and Tomas Pfis-\\nter. 2023. Effective large language model adap-\\ntation for improved grounding. arXiv preprint\\narXiv:2311.09533 .\\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan\\nBerant. 2023. Making retrieval-augmented language\\nmodels robust to irrelevant context. arXiv preprint\\narXiv:2310.01558, 2023 .\\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su,\\nand Huan Sun. 2023. Automatic evaluation of at-\\ntribution by large language models. arXiv preprint\\narXiv:2305.06311 .\\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-\\ning language models with memory augmentation.\\nInProceedings of the 2022 Conference on Empir-\\nical Methods in Natural Language Processing , pages\\n5657–5673.\\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman,\\nHaohan Wang, and Yu-Xiong Wang. 2023. Lan-\\nguage agent tree search unifies reasoning acting\\nand planning in language models. arXiv preprint\\narXiv:2310.04406, 2023 .A Details on Fine-grained Rewards\\nA.1 Correctness Recall Reward\\nIn ASQA, each answer (key information piece) cor-\\nresponds to a separate interpretation of the question\\nand is usually a single phrase. In QAMPARI, each\\nquestion has a list of gold answers, and each an-\\nswer is also a single phrase. For these two datasets,\\nwe check how many answers from the gold answer\\nset (key information list) are exact substrings of the\\nmodel response. In ELI5 and EXPERTQA, since\\nonly a single long-form answer is available for each\\nquestion, we infer three sub-claims from the gold\\nlong-form answers with text-davinci-003, and use\\nthese claims as the key information pieces. We then\\nuse an NLI model to check how many sub-claims\\nare entailed by the model response.\\nIn QAMPARI, since users usually may not want\\nthe entire list, we do not penalize a missing answer\\nif at least 5 answer entries have been captured.\\nAs there is no clear boundary for an answer unit\\nin long-form response, we adopt a sequence-level\\ngranularity for R1, meaning that the reward is com-\\nputed once and assigned to a single token for the\\nentire response sequence.\\nSpecifically, if the response yto a question from\\ndataset dstring matches (ASQA; QAMPARI) or\\nentails (ELI5; EXPERTQA) hitems out of tgold\\nanswer items, then the reward is\\nR1\\n1=n\\nw1h−w1(t−h), d̸=qampari\\nw1h−w1max (min (t,5)−h,0), d=qampari\\n(3)\\nA.2 Citation Recall Reward\\nGiven a sentence si, its set of cited passages Ci,\\nand an NLI model σthat checks whether a set of\\ncited documents entail a sentence, sihas a Citation\\nRecall reward of +w2(i.e., Ri\\n2= +w2) if and\\nonly if it is entailed by the concatenation of the\\ncited passages, meaning that σ(concat (Ci), si) =\\n1. Otherwise, the reward is −w2(i.e.,Ri\\n2=−w2).\\nOn long-form QA datasets (ASQA; ELI5; EX-\\nPERTQA), this reward is computed for each sen-\\ntence in the response. On QAMPARI, since the\\nmodel response is a comma-separated list, we treat\\neach comma-separated item (formatted as ’{ques-\\ntion} {item}’) in the response as a sentence.\\nA.3 Citation Precision Reward\\nWe use the definition in (Gao et al., 2023b). Given\\na sentence si, its set of cited passages Ci, and an', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 11}), Document(page_content='NLI model σ, a citation that refers to passage cj∈\\nCihas a Citation Precision reward of +w3(i.e.,\\nRj\\n3= +w3) if and only if σ(concat (Ci), si) = 1\\nand either of the following conditions holds\\n•cjexplicitly entails s with σ(cj, s) = 1\\n•cjimplicitly helps other passages entail s with\\nσ(concat (Ci− {cj}), s) = 0\\nOtherwise, the reward is −w3(i.e.,Rj\\n3=−w3).\\nB Datasets and Metrics\\nB.1 Datasets\\nASQA The authors of ASQA have not released\\nits test split. Therefore, we randomly shuffle and\\nthen divide its original train split samples into two\\ngroups so that one group is used for training and\\nthe other is used for validation. We use its original\\ndev split as our test split. This gives 3,853/500/948\\nsamples for train/dev/test respectively.\\nQAMPARI We randomly sample 4,000 in-\\nstances from the original train split of QAMPARI\\nand take its entire development and test splits, giv-\\ning 4,000/1,000/1,000 samples for train/dev/test\\nusage.\\nELI5 The original ELI5 dataset is much\\nlarger than ASQA. To ensure the balance be-\\ntween different datasets, we randomly sample\\n3,728/1,000/1,000 instances from the train/dev/test\\nsplits of ELI5. We also include in the train split\\n264 samples from WebGPT (Nakano et al., 2021),\\nsince these samples’ answers contain citations\\nand are of relatively high quality. This gives\\n3,992/1,000/1,000 samples in the train/dev/test\\nsplits.\\nEXPERTQA We remove a few samples that\\ndo not have human-revised answers since we\\nneed annotated answers to infer sub-claims for\\ncorrectness recall computation, and this gives us\\nthe remaining 2,169 samples. Since some groups\\nof samples in EXPERTQA are evaluated in a\\nclosed-book setting or use a different retrieval\\nmechanism, we manually retrieve the top 5\\ndocuments from Sphere (Piktus et al., 2021) again\\nfor each sample to ensure consistency with other\\ndatasets. We use EXPERTQA for testing only.\\nThe above gives around 4k/0.5k/1k samples\\nfor train/valid/test splits on ASQA and approxi-\\nmately 4k/1k/1k samples on QAMPARI and ELI5.They are used directly in the separate setting. In\\nthecombined setting, we further sample around\\n1k/334/1k instances from each filtered dataset to\\nget 2,992/1,002/3,000 samples for joint training\\nand inference.\\nB.2 Retrieval\\nWe use GTR (Ni et al., 2022) to retrieve the top-5\\npassages from the 2018-12-20 Wikipedia snapshot\\non ASQA and QAMPARI, and use BM25 to re-\\ntrieve the top-5 passages from Sphere (Piktus et al.,\\n2021) on ELI5 and EXPERTQA respectively.\\nB.3 Metrics\\nWe use the metrics from ALCE (Gao et al., 2023b)\\nto evaluate our model generations’ correctness and\\ncitation quality. Their computations are illustrated\\nin Figure 3.\\nMAUVE (Pillutla et al., 2021) is an additional\\nmetric in ALCE that is used to evaluate fluency,\\nwhich we do not include in the main analysis since\\nit is sensitive to output length and unstable (Gao\\net al., 2023b). We only report it in Appenidx\\nD. It is computed on the model’s generated re-\\nsponses against the provided gold responses. Fol-\\nlowing (Gao et al., 2023b), we remove citations\\nin the model’s generations, append the question to\\nboth the gold and generated responses, and trun-\\ncate them to the first 100 words before computing\\nMAUVE.\\nC Additional Training Details\\nEfficiency For memory efficiency, we fine-tune\\nLoRA (Hu et al., 2021) in the distillation and RS\\nfine-tuning steps, and QLoRA (Dettmers et al.,\\n2023) for RLHF. Moreover, in RLHF training, we\\nuse an architecture similar to Hydra-PPO (San-\\ntacroce et al., 2023) with a slight modification that\\nseparates the weights of the reward model from the\\nmain model. We use vLLM (Kwon et al., 2023) to\\naccelerate inference.\\nContext and generation length We set the max-\\nimum source length to be 1,024 for T5-large7and\\n1,200 for LLaMA-2-7B since its prompts contains\\ninstructions. We set the maximum generation\\nlength to 200 for both models to control the con-\\nciseness of answers.\\n7In addition to LLaMA-2-7B, we also apply our method\\nto T5-large. Its results are in Appendix D.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 12}), Document(page_content='Answer:\\nStanley Kubrick  [1], \\nAnthony Mann [ 2], \\nDalton Trumbo [ 2], \\nBrian De Palma [ 4], \\nJoseph L. Mankiewicz [ 5].\\nGold:\\nStanley Kubrick \\nKevin Billington  X \\nKirk Douglas X \\nAnthony Newley X\\nAndr  ̆00e9 de Toth X\\nRec. -5 =min(1,5)\\nmin(5,5)=1\\n5\\nPrec  = 1\\n5\\nAnswer:\\nI’m Coming Out\" is a song \\nrecorded by Diana Ross [1][2]. \\nIt was written and produced \\nby Chic  members Bernard \\nEdwards and Nile Rodgers [ 1].\\nGold:\\nDiana Ross\\nAmerie X\\nEM Rec = 1\\n2\\nAnswer:\\nThere are several books that are considered the \\nbest for child and youth psychiatry, including the \\nTextbook of Child and Adolescent Psychiatry [2], \\nRutter ’s Child and Adolescent Psychiatry  [5], and \\nConcise Guide to Child and Adolescent \\nPsychiatry [ 2].\\nClaim Rec = 2\\n3Gold Claims:\\n1. \"Textbook of Child and Adolescent Psychiatry\" \\nis recommended …\\n2. \"Rutter ’s Child and Adolescent Psychiatry\" has \\nbeen described as \"the best \\ntextbook …\\n3. … such as \"Bipolar Disorder in Youth: \\nPresentation, Treatment and Neurobiology\".Entailed  by Ans \\nEntailed  by AnsAnswer:\\nThe city of Greenville, which is the seat of Greenville County, had an estimated population of 68,219  \\nas of 2017 , making it the sixth -largest city in the state [ 1]. The population of the surrounding area  \\nwas 400,492 as of 2010 , making it the third -largest urban area in South Carolina and the fastest -\\ngrowing [ 1][2].Question: Who sang the song \\ni’m coming out?Question: Who is a director \\nof a film produced by Kirk \\nDouglas?Question: Which books are the best for child and \\nyouth psychiatry?Correctness Citation Recall\\nThe city … estimated population of 68,219  as \\nof 2017 , making … [1]. Entailed  by [1]\\nCitation Rec =  2\\n2= 1Citation Precision\\nThe city … estimated population of 98,219 \\nas of 2017 , making … [1]    . The population … \\nwas 400,492 as of 2010 , making … [1]   [2] X.\\nThe population … was 400,492 as of 2010 , \\nmaking … [1][2]. Entailed  by [1][2]\\nCitation Prec =  2\\n3Question: What is the population of greenville  county south carolina ?\\nNot Entailed  by AnsASQAQAMPARI ELI5Figure 3: Left: Examples of how the Correctness metrics are computed for ASQA (EM Rec), QAMPARI (Rec.-5,\\nPrec), and ELI5 (Claim Rec) respectively; Right: An example of how the Citation Recall and Citation Precision are\\ncomputed.\\nDistillation We instruction-tune our model for\\n10 epochs with Adam Optimizer. We use a learning\\nrate of 5e-5 and a linear learning rate scheduler\\nfor T5-large. We use a learning rate of 2e-4 and a\\ncosine learning rate scheduler with a warm-up ratio\\nof 0.03 for LLaMA-2-7B. The best checkpoints are\\nselected using the dev set for RS fine-tuning or RL.\\nRS fine-tuning When fine-tuning the model, we\\nuse the same hyperparameters as in the distillation\\nstep, except that we change the number of training\\nepochs to 3.\\nLoRA We set the rank to 64 and αto 16. The\\nadapters are applied to the Q and V projection lay-\\ners in the transformer unit. We use the same set of\\nhyperparameters for every adapter.\\nRL Similar to Wu et al. (2023), we set γ= 1\\nandλ= 0.95. We use Adam Optimizer. The\\nlearning rate is set to 1e-5 for T5-large and 3e-5 for\\nLLaMA-2-7B, and a linear scheduler is used for\\nboth models. Additionally, we sample 4 response\\nsequences from the model for each training sample,\\nwith the temperature set to 0.7 and top k set to\\n20. On T5-large, we train the model for 80,000\\nepisodes in total. On LLaMA-2-7B, for efficiency\\nreasons, we set the number of episodes to 48,000.\\nWe use 100 warm-up steps across every setting.\\nThe KL coefficient βis set to 0.3.\\nD Complete Main Experiments\\nTo demonstrate the general applicability of our\\nframework, we also apply our method to T5-large\\n(Raffel et al., 2020). We provide the complete main\\nexperiment results in Table 4, including results on\\nT5-large and on LLaMA-2-7B checkpoints. These\\nnew models are trained in the separate setting,which means they are trained on each individual\\ndataset instead of the mixture. We also include an\\nadditional baseline Self-RAG (Asai et al., 2023).', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 13}), Document(page_content='new models are trained in the separate setting,which means they are trained on each individual\\ndataset instead of the mixture. We also include an\\nadditional baseline Self-RAG (Asai et al., 2023).\\nNote that RL experiments for the separate train-\\ning settings are run only once, since we observe\\nlittle variation when repeating them with different\\nseeds.\\nE Retrieval Analysis\\nAs can be seen from Table 5, around 70 percent of\\nanswers present in the retrieved documents can be\\ncaptured by our LLMs on ASQA and QAMPARI.\\nOn ELI5, this coverage increases to more than 90\\npercent.\\nF Training Curves\\nWe show the f.g.RL training curves of LLaMA-\\n2-7B across the 3 independent runs in figure 4.\\nWhile correctness recall fluctuates, there is a steady\\nincrease in citation recall and citation precision.\\nG Citation Error Analysis\\nWe show examples for each type of citation error\\nin Table 6, 7, and 8.\\nH Additional Experiment on\\nEXPERTQA\\nWe also evaluate EXPERTQA with our own met-\\nrics. The results are shown in Table 9. Similar\\nto ELI5, our method outperforms ChatGPT on ci-\\ntation recall and precision, but still lags behind\\non claim recall. The reasons can be complicated,\\nwhile the most probable ones are the same as those\\nfor ELI5, including insufficient emphasis on cor-\\nrectness reward and incorrect sub-claims derivation\\nby text-davinci-003.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 13}), Document(page_content='ASQA QAMPARI ELI5\\nCorrectness Citation Fluency Correctness Citation Correctness Citation Fluency\\nEM Rec Rec Prec MAUVE Rec.-5 Prec Rec Prec Claim Rec Rec Prec MAUVE\\nICL ChatGPT 39.96 74.72 70.97 76.49 18.34 18.57 17.65 20.60 13.47 50.94 47.58 43.13\\nICL LLaMA-2-7B 34.15 14.12 15.26 85.31 8.24 9.23 7.51 7.65 7.83 14.44 8.92 43.53\\nSelf-RAG 7B* 30.0 67.8 66.9 74.3 - - - - - - - -\\nSelf-RAG 13B* 31.7 71.3 70.3 71.6 - - - - - - - -\\nT5 (Separate Setting)\\nDistillation 33.60 71.98 65.15 66.81 15.98 14.30 15.48 17.96 9.03 48.32 43.29 51.04\\n+f.g.RL 33.75 74.24 68.30 61.11 18.10 14.56 15.75 17.97 9.63 52.04 47.25 40.31\\nLLaMA-2-7B (Separate Setting)\\nDistillation 34.84 69.30 62.20 59.93 17.66 16.42 16.14 17.93 12.77 49.37 43.73 48.53\\n+h.RL 36.80 71.57 68.87 77.79 17.30 17.08 16.80 18.56 12.40 49.95 45.83 54.68\\n+f.g.RL 36.94 71.86 69.75 77.21 18.86 17.66 16.95 18.36 12.23 49.76 47.10 47.23\\n+h.RS 37.41 77.84 71.28 68.71 16.06 18.01 17.29 18.13 11.30 55.93 51.35 42.75\\n+f.g.RS 40.24 77.65 74.96 83.10 17.48 20.67 20.62 21.65 11.87 61.27 56.45 53.37\\n+h.(RS+RL) 37.72 78.47 73.19 68.65 17.58 20.74 21.25 21.61 11.30 56.67 53.69 59.79\\n+f.g.(RS+RL) 41.33 79.71 77.90 91.14 15.76 21.60 21.36 21.92 11.47 63.45 59.96 51.32\\nLLaMA-2-7B (Combined Setting)\\nDistillation 35.56 74.80 67.99 62.08 17.26 16.18 18.69 18.94 12.03 49.69 45.71 36.03\\n+h.RL 34.33 75.77 70.12 61.79 17.30 16.44 16.39 18.92 11.52 51.77 49.32 45.46\\n+f.g.RL 35.99 76.30 72.38 69.99 18.39 18.81 17.82 19.07 11.60 51.29 51.09 40.64\\n+h.RS 37.96 74.86 68.48 79.04 14.62 15.21 16.71 17.98 11.60 54.10 48.95 47.91\\n+f.g.RS 40.07 76.71 74.35 87.04 16.14 18.95 18.56 19.50 11.67 58.75 55.03 45.03\\n+h.(RS+RL) 37.33 74.86 69.37 81.91 15.02 15.67 16.82 18.09 11.21 55.62 50.58 47.76\\n+f.g.(RS+RL) 40.05 77.83 76.33 89.08 16.65 19.54 19.50 20.03 11.54 60.86 60.23 40.52\\nTable 4: Complete main result on the three QA datasets. In-Context: In-Context Learning; * indicates results that\\nare reported by the original paper; - indicates numbers not reported by the original paper. Our results on ChatGPT\\nslightly deviate from those reported by Gao et al. (2023b), since they use sampling when decoding the responses\\nduring inference, and their evaluation on QAMPARI and ELI5 is run on the development split, while ours is run on\\nthe test split.\\nFigure 4: Training curves of LLaMA-2-7B with f.g.RL in the combined setting, measured with the development set\\nacross 3 independent runs. The shaded region indicates the standard error across these runs.\\nI Prompts\\nThe prompts for ASQA, QAMPARI, and ELI5 are\\nshown in Table 10, Table 11, and Table 12 respec-\\ntively. Our prompts are consistent with those used\\nby Gao et al. (2023b). Note that during training,\\nwe remove the in-context demonstrations from the\\nprompts.\\nJ Examples\\nWe provide an example for ASQA, QAMPARI,\\nELI5, and EXPERTQA each, presented in Ta-\\nble 13, 14, 15, and 16 respectively. The responses\\nare generated by the f.g.(RS + RL) checkpoints\\ntrained in the combined setting.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 14}), Document(page_content='ASQA QAMPARI ELI5\\nD.R.: 57.65 D.R.: 20.53 D.R.: 9.13\\nC.R. C.R.-5 C.R. C.P. C.R.\\nICL ChatGPT 73.05 68.81 67.57 16.86 93.28\\nICL LLaMA-2-7B 65.24 55.16 54.59 8.99 91.15\\nDistillation 67.68 69.95 68.78 15.59 92.58\\nRL/RS only\\n+h.RL 66.10 69.89 68.73 15.85 92.47\\n+f.g.RL 68.62 71.19 69.84 18.02 92.34\\n+h.RS 70.87 63.90 63.03 14.74 92.68\\n+f.g.RS 74.28 67.27 66.16 18.41 92.73\\nRS+RL\\n+h.(RS+RL) 70.19 64.67 63.79 15.25 92.41\\n+f.g.(RS+RL) 74.08 67.87 66.75 18.87 92.57\\nTable 5: Retrieval Analysis. D.R.: Correctness Re-\\ncall@5 of the retrieved documents; C.: Calibration with\\nthe answers in the retrieved documents; R.: Correctness\\nRecall; R.-5: Correctness Recall that is 100 if there are\\nat least 5 hits; P.: Correctness Precision\\nInstruction: Provide a · · ·\\nQuestion: Who is a director of a film produced by\\nKirk Douglas?\\nDocument [1](Title: Kirk Douglas): began producing\\nfilms as varied as \"Paths of Glory\" (1957) and · · ·\\nDocument [2](Title: Kirk Douglas): executive producer\\nas well, raising the $12 million production cost, making\\n· · ·Douglas initially selected Anthony Mann to direct, but\\nreplaced him early on with Stanley Kubrick, with whom · · ·\\nWhen the film was released, Douglas gave full credit to its\\nscreenwriter, Dalton Trumbo, who was on the · · ·\\nDocument [3] · · ·\\nAnswers: Stanley Kubrick [1], Anthony Mann [2],\\nDalton Trumbo [2], Brian De Palma [4], Joseph L.\\nMankiewicz [5].\\nTable 6: An example from QAMPARI test set demon-\\nstrating the mixing up of passage ids error. Stanley\\nKubrick is only mentioned in [2], but is erroneously\\nattributed to [1].Instruction: Write an · · ·\\nQuestion: What exactly is ’spin’ in quantum mechan-\\nics?\\nDocument [1](Title: quantum mechanics - Does spin\\nhave anything to do with a rate of change? - Physics Stack\\nExchange): quantum mechanics - Does spin have anything to\\ndo · · ·what exactly does it mean, 1/2\\nDocument [2](Title: quantum mechanics - Does spin have\\nanything to do with a rate of change? - Physics Stack\\nExchange): quantum mechanics -Does spin have anything\\nto do · · ·why do we relate spin with angular momentum\\noperator in quantum mechanics?\\nDocument [3] · · ·\\nAnswers: In quantum mechanics, spin refers to the\\nintrinsic angular momentum of particles, such as electrons,\\nwhich can have either a spin up or a spin down state [1][2].\\nSpin is · · ·\\nTable 7: An example from ELI5 test set demonstrating\\nthe redundant citations error. None of the documents\\nfrom [1] to [5] mention anything about spin states. The\\nLLM retrieves its parametric knowledge when answer-\\ning the question, but it still inserts the citations.\\nInstruction: Provide a · · ·\\nQuestion: Michael Kiske is part of what music group?\\nDocument [1] · · ·\\nDocument [2] · · ·\\nDocument [3](Michael Kiske): · · ·In 2006, Michael Kiske\\nreleased his third full-length solo album under the name of\\n\"Kiske\". The album contained songs in a mellow, acoustic\\nsinger-songwriter style and was a step away from the sound\\nof his previous solo albums. · · ·\\nDocument [4] · · ·\\nAnswers: Helloween [1], Unisonic [1], SupaRed [3],\\nKiske [3], Ill Prophecy [5].\\nTable 8: An example from QAMPARI test set demon-\\nstrating the misinterpretation of cited passages error.\\n\"Kiske\" is actually the name of Michael Kiske’s album,\\nbut is incorrectly interpreted by the LLM as the name\\nof a music group where Michael Kiske is a member.\\nEXPERTQA\\nCorrectness Citation Fluency\\nClaim Rec Rec Prec MAUVE\\nICL ChatGPT 19.27 47.79 47.30 48.68\\nICL LLaMA-2-7B 10.32 10.09 7.79 34.27\\nDistillation 15.28 49.03 46.22 40.63\\n+f.g.RL 15.53 49.73 51.11 45.92\\n+f.g.RS 15.48 59.46 57.58 44.67\\n+f.g.(RS+RL) 15.23 58.94 59.80 42.13\\nTable 9: Result on EXPERTQA.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 15}), Document(page_content='Instruction: Write an accurate, engaging, and concise answer for the given question using only the\\nprovided search results (some of which might be irrelevant) and cite them properly. Use an unbiased and\\njournalistic tone. Always cite for any factual claim. When citing several search results, use [1][2][3]. Cite\\nat least one document and at most three documents in each sentence. If multiple documents support the\\nsentence, only cite a minimum sufficient subset of the documents.\\nQuestion: Who played galen in planet of the apes?\\nDocument [1](Title: Planet of the Apes): {text 1}\\nDocument [2](Title: Planet of the Apes (1968 film)): {text 2}\\nDocument [3](Title: Planet of the Apes (1968 film)): {text 3}\\nDocument [4](Title: Planet of the Apes): {text 4}\\nDocument [5](Title: Planet of the Apes): {text 5}\\nAnswer:In the 1968 film Planet of the Apes, Galen was played by Wright King [2]. And in the tv series\\nPlanet of the Apes, Galen was played by Roddy McDowall [1].\\nQuestion: Which is the most rainy place on earth?\\nDocument [1](Title: Cherrapunji): {text 1}\\nDocument [2](Title: Cherrapunji): {text 2}\\nDocument [3](Title: Mawsynram): {text 3}\\nDocument [4](Title: Earth rainfall climatology): {text 4}\\nDocument [5](Title: Going to Extremes): {text 5}\\nAnswer:Several places on Earth claim to be the most rainy, such as Llor ˘00f3, Colombia, which reported\\nan average annual rainfall of 12,717 mm between 1952 and 1989, and L ˘00f3pez de Micay, Colombia,\\nwhich reported an annual 12,892 mm between 1960 and 2012 [3]. However, the official record is held by\\nMawsynram, India with an average annual rainfall of 11,872 mm [3], although nearby town Sohra, India,\\nalso known as Cherrapunji, holds the record for most rain in a calendar month for July 1861 and most rain\\nin a year from August 1860 to July 1861 [1].\\nQuestion: {question}\\nDocument [1](Title: title 1): {text 1}\\nDocument [2](Title: title 2): {text 2}\\nDocument [3](Title: title 3): {text 3}\\nDocument [4](Title: title 4): {text 4}\\nDocument [5](Title: title 5): {text 5}\\nAnswer:\\nTable 10: Prompt and demonstrations for ASQA. We remove the demonstrations in training.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 16}), Document(page_content='Instruction: Provide a list of accurate answers for the given question using only the provided search results\\n(some of which might be irrelevant) and cite them properly. Always cite one and only one document for\\neach answer. Separate answers by commas. For questions that have more than 5 answers, write at least 5\\nanswers.\\nQuestion: Which film has Gong Li as a member of its cast?\\nDocument [1](Title: Gong Li): {text 1}\\nDocument [2](Title: Gong Li): {text 2}\\nDocument [3](Title: Gong Li): {text 3}\\nDocument [4](Title: Zhang Yimou): {text 4}\\nDocument [5](Title: Gong Li): {text 5}\\nAnswer:The Story of Qiu Ju [1], Farewell My Concubine [2], Flirting Scholar [2], The Monkey King 2\\n[3], Mulan [3], Saturday Fiction [3], Coming Home [3].\\nQuestion: Glenn Ford was a member of cast in which film?\\nDocument [1](Title: Glenn Ford): {text 1}\\nDocument [2](Title: Glenn Ford): {text 2}\\nDocument [3](Title: CBS Thursday Night Movie): {text 3}\\nDocument [4](Title: The Trouble with Girls (film) ): {text 4}\\nDocument [5](Title: Trouble in the Glen): {text 5}\\nAnswer:So Ends Our Night [1], Heaven with a Barbed Wire Fence [1], Happy Birthday to Me [2], The\\nGreatest Gift [2], The Gift [2], The Brotherhood of the Bell [3].\\nQuestion: {question}\\nDocument [1](Title: title 1): {text 1}\\nDocument [2](Title: title 2): {text 2}\\nDocument [3](Title: title 3): {text 3}\\nDocument [4](Title: title 4): {text 4}\\nDocument [5](Title: title 5): {text 5}\\nAnswer:\\nTable 11: Prompt and demonstrations for QAMPARI. We remove the demonstrations in training.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 17}), Document(page_content='Instruction: Write an accurate, engaging, and concise answer for the given question using only the\\nprovided search results (some of which might be irrelevant) and cite them properly. Use an unbiased and\\njournalistic tone. Always cite for any factual claim. When citing several search results, use [1][2][3]. Cite\\nat least one document and at most three documents in each sentence. If multiple documents support the\\nsentence, only cite a minimum sufficient subset of the documents.\\nQuestion: What’s the difference between Shia vs. Sunni Islam?\\nDocument [1](Title: The Sunni vs Shia Divide - Explained - Globaloi): {text 1}\\nDocument [2](Title: What ˘2019s the difference between Sunni and Shia Islam? ˘2013 Macrosnaps): {text\\n2}\\nDocument [3](Title: Difference between Sunni and Shia Muslims | Sunni vs Shia Muslims): {text 3}\\nDocument [4](Title: What is the difference between Shia and Sunni Islam? - Islam Stack Exchange):\\n{text 4}\\nDocument [5](Title: What is the difference between Sunni and Shia Islam? | Patrick Syder Travel): {text 5}\\nAnswer:The main difference between Shia and Sunni Muslim is related to ideological heritage and\\nissues of leadership [1]. This difference is first formed after the death of the Prophet Muhammad in 632\\nA.D. [1][2]. The ideological practice of the Sunni branch strictly follows Prophet Muhammad and his\\nteachings, while the Shia branch follows Prophet Muhammad’s son-in-law Ali [2]. Nowadays, Sunni and\\nShia are the major branches of Islam [3].\\nQuestion: How do student loans affect getting a mortgage?\\nDocument [1](Title: Student Loans ˘2013 How do they work? | The Financial Review): {text 1}\\nDocument [2](Title: How Does Student Loan Debt Affect Buying a Home? | Experian): {text 2}\\nDocument [3](Title: Studentloanify - How your student loans affect your home mortgage prospects):\\n{text 3}\\nDocument [4](Title: How do student loans affect your credit score? | Student Loan Planner): {text 4}\\nDocument [5](Title: Does Student Loan Debt Affect Getting A Mortgage?): {text 5}\\nAnswer:When applying for a mortgage, student loans can affect the debt to income ratio, which is a key\\nfactor in determining the amount that an individual can afford to pay for the mortgage [1]. While student\\nloan repayments do not appear in an individual’s credit history and do not affect credit scores, lenders do\\nconsider the amount of an individual’s student loan repayments when assessing their mortgage application\\n[1][2][3]. Some 83% of non-homeowners say student loan debt is preventing them from buying a home,\\naccording to the National Association of Realtors [2]. It is important to note that student loans do not\\nprevent an individual from getting a mortgage [1].\\nQuestion: {question}\\nDocument [1](Title: title 1): {text 1}\\nDocument [2](Title: title 2): {text 2}\\nDocument [3](Title: title 3): {text 3}\\nDocument [4](Title: title 4): {text 4}\\nDocument [5](Title: title 5): {text 5}\\nAnswer:\\nTable 12: Prompt and demonstrations for ELI5. We remove the demonstrations in training.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 18}), Document(page_content='Instruction: Write an ···\\nQuestion: Who sang the song i’m coming out?\\nDocument [1](Title: I’m Coming Out): I’m Coming Out \"I’m Coming Out\" is a song recorded by\\nAmerican singer Diana Ross. It was written and produced by Chic members Bernard Edwards and Nile\\nRodgers, and released in August 22, 1980 as the second single from Ross’ self-titled tenth album \"Diana\"\\n(1980). In 1979, Ross commissioned Chic founders Nile Rodgers and Bernard Edwards to create material\\nfor a new album after taking her daughters to see the band in concert, following the success of Ross’s final\\nAshford & Simpson-composed and produced LP,\" The Boss. \"Rodgers got the idea for \"I’m Coming Out\"\\nafter noticing three different drag\\nDocument [2](Title: I’m Coming Out): the Loren Dawson remix. The original single version (which is\\nsimilar to the original) can be found on both CD singles and the original motion picture soundtrack to\\n\"Maid in Manhattan\". Ross’ version appears on the film’s soundtrack album as well. A music video was\\nproduced to promote the single. I’m Coming Out \"I’m Coming Out\" is a song recorded by American\\nsinger Diana Ross. It was written and produced by Chic members Bernard Edwards and Nile Rodgers,\\nand released in August 22, 1980 as the second single from Ross’ self-titled tenth album \"Diana\" (1980).\\nIn 1979, Ross commissioned Chic\\nDocument [3](Title: I’m Coming Out): \"I’m Coming Out\" and its resulting popularity has led to Ross’s\\nsong often being sampled, most notably by Stevie J, who sampled the song for rapper The Notorious\\nB.I.G.’s 1997 song \"Mo Money Mo Problems\". There’s also been a cover by Marcia Hines. It was\\nfeatured as a cover for the video game \"Karaoke Revolution\" as well. In more recent times, the song\\nwas sampled in Keyshia Cole’s remake of \"I’m Coming Out\" (featuring Iggy Azalea) for the original\\nsoundtrack of the 2014 movie The Other Woman, starring Cameron Diaz, Kate Upton and Leslie Mann.\\nThe song is sampled in\\nDocument [4](Title: I’m Coming Out): \"Break Your Heart Right Back\" from Ariana Grande’s sophomore\\nalbum \"My Everything\" (2014). This song was also covered for the soundtrack to the 2016 movie\\n\"Trolls\". In 1997, German comedian Hella von Sinnen sampled \"I’m Coming Out\" for her single \"Mein\\nComing Out\", using the term \"coming out\" in the context of coming out of the closet, or revealing one’s\\nhomosexuality. In 2003, American singer Amerie covered \"I’m Coming Out\" for the soundtrack to the\\nfilm \"Maid in Manhattan\". Her version was released as a single in select European countries and Australia.\\nThe final single version to be released was\\nDocument [5](Title: Coming Out (album)): \"Poinciana (The Song of the Tree)\" featured a solo by Michael\\nBrecker. \"Coming Out\" debuted on Billboard’s Top Pop Album chart on September 18, 1976 and peaked\\nat No. 48. The album was re-released on CD in March 1993. Manhattan Transfer Musicians Production\\nComing Out (album) Coming Out is the third album by The Manhattan Transfer, released August 19,\\n1976 on Atlantic Records. On the advice of their manager Aaron Russo, the group downplayed the\\n\"nostalgic\" tenor of their previous breakthrough album in favor of more contemporary songs. However,\\nthe throwback single \"Chanson d’Amour\" became a breakout single in Europe\\nAnswer: \"I’m Coming Out\" is a song recorded by Diana Ross [1][2]. It was written and produced by Chic\\nmembers Bernard Edwards and Nile Rodgers [1].\\nGold: \"I’m Coming Out\" is a song recorded by American singer Diana Ross. It was written and produced\\nby Chic members Bernard Edwards and Nile Rodgers, and released on August 22, 1980, as the second\\nsingle from Ross’ self-titled tenth album Diana (1980). In 2003, American singer Amerie covered \"I’m\\nComing Out\" for the soundtrack to the film \"Maid in Manhattan\". Her version was released as a single in\\nselect European countries and Australia.\\nGold Answers: [Diana Ross, Amerie]\\nTable 13: ASQA example', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 19}), Document(page_content='Instruction: Provide a ···\\nQuestion: Who is a director of a film produced by Kirk Douglas?\\nDocument [1](Title: Kirk Douglas): began producing films as varied as \"Paths of Glory\" (1957) and\\n\"Spartacus\" (1960). In those two films, he collaborated with the then-relatively-unknown director Stanley\\nKubrick taking lead roles in both films. Douglas has been praised for helping to break the Hollywood\\nblacklist by having Dalton Trumbo write \"Spartacus\" with an official on-screen credit, although this has\\nbeen disputed by others. He produced and starred in \"Lonely Are the Brave\" (1962), considered a classic,\\nand \"Seven Days in May\" (1964), opposite Burt Lancaster, with whom he made seven films. In 1963, he\\nstarred in the Broadway play \"One Flew Over the\\nDocument [2](Title: Kirk Douglas): executive producer as well, raising the $12 million production cost,\\nmaking it one of the most expensive films made up to that time. Douglas initially selected Anthony Mann\\nto direct, but replaced him early on with Stanley Kubrick, with whom he previously collaborated in \"Paths\\nof Glory\". When the film was released, Douglas gave full credit to its screenwriter, Dalton Trumbo, who\\nwas on the Hollywood blacklist, and thereby effectively ended it. About that event, he said, \"I’ve made\\nover 85 pictures, but the thing I’m most proud of is breaking the blacklist.\" However the film’s producer\\nEdward Lewis and\\nDocument [3](Title: Spartacus (film)): credibility. Trumbo was publicly given credit for two major films.\\nOtto Preminger made public that Trumbo wrote the screenplay for his film \"Exodus\", and Kirk Douglas\\npublicly announced that Trumbo was the screenwriter of \"Spartacus\". Further, President John F. Kennedy\\npublicly ignored a demonstration organized by the American Legion and went to see the film. After David\\nLean turned down an offer to direct, \"Spartacus\" was to be directed by Anthony Mann, then best known\\nfor his Westerns such as \"Winchester ’73\" and \"The Naked Spur\". Douglas fired Mann at the end of the\\nfirst week of shooting, in which\\nDocument [4](Title: Home Movies (film)): directing films. Kirk Douglas plays a film instructor loosely\\nmodeled on the director himself. Keith Gordon is one of his pupils who films everything that happens at\\nhome. Many of the events that happen to Gordon’s character were modeled on events from De Palma’s\\nown adolescence, particularly rivalry with a more favored brother, a mother prone to dramatic outbursts,\\nand a philandering father. Home Movies (film) Home Movies is a 1979 independent film directed by\\nBrian De Palma and starring Kirk Douglas, Nancy Allen, Vincent Gardenia, Keith Gordon, Theresa\\nSaldana, and Gerrit Graham. De Palma had been teaching film at\\nDocument [5](Title: Kirk Douglas): in the picture,\" adding that \"he’s as bright a person as I’ve met in the\\nacting profession.\" She says that his \"pragmatic approach to acting\" would later be a \"philosophy that\\nended up rubbing off on me.\" Between 1970 and 2008, Douglas made nearly 40 movies and appeared on\\nvarious television shows. He starred in a western, \"There Was a Crooked Man...\" (1970), alongside Henry\\nFonda. The film was produced and directed by Joseph L. Mankiewicz. In 1973, he directed his first film,\\n\"Scalawag\". Also in 1973, Douglas appeared in a made-for-TV musical version of \"Dr. Jekyll and Mr.\\nHyde\".\\nAnswer: Stanley Kubrick [1], Anthony Mann [2], Dalton Trumbo [2], Brian De Palma [4], Joseph L.\\nMankiewicz [5].\\nGold: Stanley Kubrick, Kevin Billington, Kirk Douglas, Anthony Newley, Andr ˘00e9 de Toth\\nGold Answers: [[Stanley Kubrick], [Kevin Billington], [Kirk Douglas], [Anthony Newley], [Andr ˘00e9 de\\nToth]]\\nTable 14: QAMPARI example', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 20}), Document(page_content='Instruction: Write an ···\\nQuestion: What exactly is ’spin’ in quantum mechanics?\\nDocument [1](Title: quantum mechanics - Does spin have anything to do with a rate of change? - Physics Stack Exchange): quantum mechanics - Does spin have\\nanything to do with a rate of change? - Physics Stack Exchange the electron. Not the answer you’re looking for? Browse other questions tagged quantum-mechanics\\noperators quantum-spin rotation spinors or ask your own question. How do you rotate spin of an electron? Did the Goudsmit-Uhlenbeck analysis of spin consider\\nrelativity? Is there an Ehrenfest-like result for the expectation value of orbital angular momentum? Can spin be related to a shift in angle? What does spin 0 mean\\nexactly? Does String Theory explain spin? How can it be derived that particles described by the Dirac equation must have spin 1/2? When we say electron spin is 1/2,\\nwhat exactly does it mean, 1/2\\nDocument [2](Title: quantum mechanics - Does spin have anything to do with a rate of change? - Physics Stack Exchange): quantum mechanics - Does spin have\\nanything to do with a rate of change? - Physics Stack Exchange your own question. Did the Goudsmit-Uhlenbeck analysis of spin consider relativity? Can spin be\\nrelated to a shift in angle? What does spin 0 mean exactly? Does String Theory explain spin? How can it be derived that particles described by the Dirac equation\\nmust have spin 1/2? When we say electron spin is 1/2, what exactly does it mean, 1/2 of what? Determine spin-state with B=Bxin theSzbase why do we relate\\nspin with angular momentum operator in quantum mechanics?\\nDocument [3](Title: Search results for ‘Biology‘ - PhilArchive): Search results for ‘Biology‘ - PhilArchive biased to satisfy the collective goals of the ensemble.\\nUsing this postulate, we mathematically derive the correct spin probability distribution without using quantum mechanical formalism (operators and Born ˘2019s\\nrule) and exactly reproduce the quantum mechanical spin correlation in entangled pairs. Using our concept, we also mathematically derive the form of quantum\\nmechanical wave function of free particle which is conventionally a postulate of quantum mechanics. Thus, we prove that the origin of quantum mechanical results\\nlies in the will (or consciousness) of the objects biased by the collective goal of ensemble or universe. This biasing by the group on individuals\\nDocument [4](Title: The Upside to Quantum Mechanics): The Upside to Quantum Mechanics Questions About Quantum Mechanics Which is named Quantum\\nTunneling. The quantum simulator is installed as part of the QDK. There ˘2019s no macroscopic property that ˘2019s equivalent to quantum spin. The traditional\\nmanner of operating the Hopfield network is to continue picking neurons at random and updating them by thinking about the connected neurons, together with their\\nweights. Basically, the concept is that, when you have a look at a system (such as an electron that ˘2019s spinning in two directions at exactly the same time) the\\nelectron custom essays is going to be made to quit spinning in two directions simultaneously, pick\\nDocument [5](quantum mechanics - How exactly is fusion possible? - Physics Stack Exchange): quantum mechanics - How exactly is fusion possible? - Physics\\nStack Exchange Pauli’s Exclusion Principle Can two electrons have the same momentum and spin directions? Nature of the quantum degeneracy pressure\\nHow does the Pauli-exclusion principle work if space is infinitely divisible? What exactly is meant by a quantum state in QM? Why a half-integer spin implies\\n˘201cantisymmetry ˘201d on the wave function? What is the physical (i.e. non-mathematical) cause off the difference between a system of identical bosons and\\nfermions? How do two exactly the same (in space) wavefunctions of two non-distiguishable free particles contribute to Pauli’s exclusion principle? How do I derive\\nPauli’s exclusion principle with path integrals?', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 21}), Document(page_content='Pauli’s exclusion principle with path integrals?\\nAnswer: In quantum mechanics, spin refers to the intrinsic angular momentum of particles, such as electrons, which can have either a spin up or a spin down state\\n[1][2]. Spin is not related to a rate of change, but rather is a fundamental property of particles that can be described by quantum mechanical operators and the Pauli\\nexclusion principle [1][5]. While there are various interpretations of quantum mechanics, including the idea that particles described by the Dirac equation must have\\nspin 1/2 [2], the origin of quantum mechanical results is not necessarily related to the will or consciousness of the objects, as suggested in document [3].\\nGold: Spin is the quantum mechanical analogue to rotational angular momentum. Rotational angular momentum is a measure of how strongly an object is rotating on\\nits own axis. The Earth’s revolution about its own axis (the one that causes days) is an example of rotational angular momentum.\\nThe reason I said that spin is an \"analogue\" to rotational angular momentum is that it’s not an idea that really makes sense in QM. In order for us to\\nsee how an object is rotating, we have to be able to clearly see what shape it is. If you can’t see a shape, rotation is sort of meaningless, since... well, you can’t\\nactually tell if something is rotating if it has no shape.\\nQuantum mechanical objects don’t have well-defined shapes. They’re sort of fuzzy and nebulous, so you can’t really pin them down like you’d want.\\nWe hence can’t look at, say, an electron and go \"oh, it’s rotated by 180 degrees after two seconds, so its angular momentum is X.\" Electrons can’t really rotate, it\\ndoesn’t make sense.\\nHowever, electrons do exhibit certain properties that make it *seem like* they’re rotating. For example, when a charged (non-quantum mechanical\\nobject) rotates, it makes a magnetic field. Electrons also make a magnetic field in the exact same way. So, they seem to be acting like they’re rotating, even though we\\ncan’t really pin down what that means exactly.\\nWe thus call it \"spin.\" It’s... like they’re rotating, hence \"spin,\" but it’s understood by physicists to not mean the exact same thing.\\nSpin is \"quantized,\" which means that there are only certain values that it can have. If you measure an electron’s spin, you will only either get\\n˘045b/2 (clockwise), or - ˘045b/2 (counterclockwise)ˆ (1). ˘045b is the \"reduced [Planck constant](_URL_0_),\" which shows up a lot in physics.\\nˆ (1. note that when I say \"measure,\" what’s implied is that we’re choosing an axis upon which to measure. For example, if you wanted to measure\\nthe Earth’s angular momentum, you would measure it along the Earth’s axis. The axis you choose to measure on for a quantum mechanical object is irrelevant\\nthough.)\\nGold Answer Claims:\\nSpin in quantum mechanics is analogous to rotational angular momentum.\\nSpin is not a concept that makes sense in quantum mechanics as objects do not have well-defined shapes.\\nSpin is quantized and can only have certain values when measured.\\nTable 15: ELI5 example', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 21}), Document(page_content='Instruction: Write an · · ·\\nQuestion: Which books are the best for child and youth psychiatry?\\nDocument [1](Title: job details - Consulting Psychiatrist - Department of Psychiatry, Ambulatory mental health program,\\nBC Children’s Hospital in Vancouver): job details - Consulting Psychiatrist - Department of Psychiatry, Ambulatory mental\\nhealth program, BC Children’s Hospital in Vancouver themes. Through the affiliation with the BCHRI, the Department has\\nestablished research strengths in neuroimaging, genetics, metabolism and clinical research. The Department is actively involved\\nin supporting best practice through multiple provincial networks for which it provides infrastructure support (Eating Disorders,\\nHealth Literacy, Youth Concurrent Disorders and Child & Youth Mental Health and Substance Use), through its robust and\\naward winning Child & Youth Mental Health Literacy Program and Child Health BC. Interested candidates will have obtained\\nfellowship standing in the Royal College of Physicians & Surgeons of Canada in Psychiatry along with certification in Child and\\nAdolescent Psychiatry subspecialty\\nDocument [2](Title: Textbook of Child and Adolescent Psychiatry - Google Books): Textbook of Child and Adolescent\\nPsychiatry - Google Books the Journal of the American Academy of Child and Adolescent Psychiatry. She has published more\\nthan 60 articles, book chapters, and books, including Concise Guide to Child and Adolescent Psychiatry, and Helping Parents,\\nYouth, and Teachers Understand Medications for Behavioral and Emotional Problems, published by American Psychiatric\\nPublishing, Inc. She is the principal author of the American Academy of Child and Adolescent Psychiatry Practice parameters\\nfor the assessment and treatment of children, adolescents, and adults with attention-deficit/hyperactivity disorder and a co-author\\nof the American Academy of Child and Adolescent Psychiatry practice parameters for the use of stimulant medications. Title\\nTextbook\\nDocument [3](Title: Journal of the American Academy of Child and Adolescent Psychiatry Page 5): Journal of the American\\nAcademy of Child and Adolescent Psychiatry Page 5 in evidence-based treatments for youth depression in recent decades,\\noverall treatment effects are modest at best, with 30-50% of youth being non-responders. Practice parameters consistently\\nrecommend systematic assessment and routine monitoring of depressive symptoms, or measurement-based care (MBC), to\\nenhance youth depression treatment. However, the literature offers few guidelines on how to use assessment results to inform\\ncare decisions or detect real and clinically meaningful change. (Source: Journal of the American Academy of Child and\\nAdolescent Psychiatry) Source: Journal of the American Academy of Child and Adolescent Psychiatry - February 27, 2019\\nCategory: Psychiatry Authors: Freda F. Liu, Molly C.\\nDocument [4](Title: Browse In Child and Adolescent Psychiatry | Oxford Medicine Online - Oxford Medicine): Browse In\\nChild and Adolescent Psychiatry | Oxford Medicine Online - Oxford Medicine Child and Adolescent Psychiatry x Download\\ncomplete list of books in this Child and Adolescent Psychiatry Collection (.pdf) (.xls) Bipolar Disorder in Youth: Presentation,\\nTreatment and Neurobiology Stephen M. Strakowski, Melissa P. DelBello, and Caleb M. Adler (eds) Bipolar disorders affect up\\nto 3% of the world’s population and are the 6th leading cause of disability worldwide. As the disorder presents differently in\\nyouth as it does in adults, ... More Bipolar disorders affect up to 3% of the world’s population and are the 6th leading cause of\\ndisability worldwide. As the disorder presents differently in youth as it does\\nDocument [5](Title: Rutter’s Child and Adolescent Psychiatry -ORCA): Rutter’s Child and Adolescent Psychiatry -ORCA\\nnew look.\" -Professor Sir Michael Rutter, extract from Foreword. Reviews of previous editions: \"This book is by far the best', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 22}), Document(page_content='new look.\" -Professor Sir Michael Rutter, extract from Foreword. Reviews of previous editions: \"This book is by far the best\\ntextbook of Child & Adolescent Psychiatry written to date.\" -Dr Judith Rapoport, NIH \"The editors and the authors are to be\\ncongratulated for providing us with such a high standard for a textbook on modern child psychiatry. I strongly recommend this\\nbook to every child psychiatrist who wants a reliable, up-to-date, comprehensive, informative and very useful textbook. To my\\nmind this is the best book of its kind available today.\" -Journal of Child Psychology and Psychiatry\\nAnswer: There are several books that are considered the best for child and youth psychiatry, including the Textbook of Child and\\nAdolescent Psychiatry [2], Rutter’s Child and Adolescent Psychiatry [5], and Concise Guide to Child and Adolescent Psychiatry\\n[2].\\nGold: The books recommended for child and youth psychiatry include \"Textbook of Child and Adolescent Psychiatry\" which\\nis noted in the Journal of the American Academy of Child and Adolescent Psychiatry and \"Rutter’s Child and Adolescent\\nPsychiatry\", which was described as \"the best textbook of Child & Adolescent Psychiatry written to date\" and a \"very useful\\ntextbook\" by Dr. Judith Rapoport at NIH and the Journal of Child Psychology and Psychiatry, respectively . Other resources\\ncould be found in the category of Child and Adolescent Psychiatry in Oxford Medicine Online, which includes a list of books\\nsuch as \"Bipolar Disorder in Youth: Presentation, Treatment and Neurobiology\" .\\nGold Answer Claims:\\n\"Textbook of Child and Adolescent Psychiatry\" is recommended for child and youth psychiatry.,\\n\"Rutter’s Child and Adolescent Psychiatry\" has been described as \"the best textbook of Child & Adolescent Psychiatry written to\\ndate\".,\\nOxford Medicine Online has a list of books in the category of Child and Adolescent Psychiatry, such as \"Bipolar Disorder in\\nYouth: Presentation, Treatment and Neurobiology\".\\nTable 16: EXPERTQA example', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/training language models to generate text with citations via fine grained rewards.pdf', 'page': 22}), Document(page_content='Effective Large Language Model Adaptation for Improved\\nGrounding and Citation Generation\\nXi Ye♢∗Ruoxi Sun♠Sercan Ö. Arık♠Tomas Pfister♠\\n♢The University of Texas at Austin♠Google Cloud AI\\n♢xiye@cs.utexas.edu\\n♠{ruoxis,soarik,tpfister}@google.com\\nAbstract\\nLarge language models (LLMs) have achieved\\nremarkable advancements in natural language\\nunderstanding and generation. However, one\\nmajor issue towards their widespread deploy-\\nment in the real world is that they can gener-\\nate \"hallucinated\" answers that are not factual.\\nTowards this end, this paper focuses on im-\\nproving LLMs by grounding their responses\\nin retrieved passages and by providing cita-\\ntions. We propose a new framework, AGREE ,\\nAdaptation for GRounding Enhanc Ement, that\\nimproves the grounding from a holistic per-\\nspective. Our framework tunes LLMs to self-\\nground the claims in their responses and pro-\\nvide accurate citations to retrieved documents.\\nThis tuning on top of the pre-trained LLMs\\nrequires well-grounded responses (with cita-\\ntions) for paired queries, for which we intro-\\nduce a method that can automatically construct\\nsuch data from unlabeled queries. The self-\\ngrounding capability of tuned LLMs further\\ngrants them a test-time adaptation (TTA) ca-\\npability that can actively retrieve passages to\\nsupport the claims that have not been grounded,\\nwhich iteratively improves the responses of\\nLLMs. Across five datasets and two LLMs,\\nour results show that the proposed tuning-\\nbased AGREE framework generates superior\\ngrounded responses with more accurate cita-\\ntions compared to prompting-based approaches\\nand post-hoc citing-based approaches.\\n1 Introduction\\nRecent advancements in large language models\\n(LLMs) have yielded demonstrably groundbreak-\\ning capabilities in natural language processing\\n(NLP) (Brown et al., 2020; Chowdhery et al.,\\n2022). Their ability to understand, generate, and\\nmanipulate text at unprecedented scales and depths\\nhas established them as a transformative force\\nwithin the burgeoning field of artificial intelligence,\\n∗Work done during an internship at Google Cloud AI.poised to significantly impact our increasingly data-\\ndriven world. Despite their widely spread adoption,\\none prominent issue of LLMs is that in certain\\nscenarios they hallucinate: they generate plausible-\\nsounding but nonfactual information (Maynez et al.,\\n2020; Ji et al., 2023; Menick et al., 2022), limiting\\ntheir the applicability in real-world settings. To\\nmitigate hallucinations, solutions generally rely\\non grounding the claims in LLM-generated re-\\nsponses to supported passages by providing an at-\\ntribution report (Rashkin et al., 2023; Bohnet et al.,\\n2022; Gao et al., 2023a) or adding citations to the\\nclaims (Liu et al., 2023; Gao et al., 2023b; Huang\\nand Chang, 2023).\\nThere has been a growing amount of interest in\\nmaking LLM-generated responses more trustwor-\\nthy by grounding and adding citations. One line\\nof work uses instruction tuning (Kamalloo et al.,\\n2023) or in-context learning (Gao et al., 2023b)\\nto instruct LLMs to generate grounded responses\\nwith citations to retrieved passages, following the\\nretrieval-augmented generation (Chen et al., 2017;\\nGuu et al., 2020; Lewis et al., 2020) framework.\\nAs LLMs are required to perform this challenging\\ntask from just instructions and few-shot demon-\\nstrations, such directions often lead to mediocre\\ngrounding quality (Gao et al., 2023b). Another line\\nof work is on post-hoc citing (Gao et al., 2023a;\\nChen et al., 2023), which links support passages\\nto the claims in responses using a natural language\\ninference (NLI) model. This paradigm heavily re-\\nlies on LLMs’ parametric knowledge and might not\\nextend well to less-known knowledge (Sun et al.,\\n2023).\\nWe propose a new learning-based framework,\\nAGREE ,Adaptation of LLMs for GRounding\\nEnhanc Ement. As shown in Fig. 1, our frame-\\nwork fine-tunes LLMs to generate citations, as op-\\nposed to prompting or relying on an external NLI\\nmodel used in a post-hoc way. At the training', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 0}), Document(page_content='Enhanc Ement. As shown in Fig. 1, our frame-\\nwork fine-tunes LLMs to generate citations, as op-\\nposed to prompting or relying on an external NLI\\nmodel used in a post-hoc way. At the training\\nphase, AGREE collects well-grounded responsesarXiv:2311.09533v3  [cs.CL]  2 Apr 2024', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 0}), Document(page_content='Learning-Based: AGREE(Ours)\\nPrompting-Based Post-Hoc CitingTraining Adaptation (Section 4.1)\\nFinetuningCorpusUnlabeled Queries??Base LLMAdapted LLMNLI ModelTraining DataAutomated data constructionBase LLM\\nQueryBase LLMPrompting the LLM to include citations in the responseAdd citations post-hoc using an NLI modelCorpusNLI ModelResponse without citationsResponse [citation]Response [citation]Test-Time Adaptation (Section 4.2)Adapted LLMCorpusResponse [citation]Base LLMPassagesInstructions  Few-shot ExamplesQueryQueryResponse [citation]Finetune the base LLM to generate citations Given a base LLM and unlabeled queries, construct synthetic data of responses with citations using an NLI modelThe adapted LLM iteratively retrieves from the corpus to refine the responseQueryPassagesFigure 1: Our framework, AGREE , combines tuning (Section 4.1) and test time adaptation (Section 4.2) for better\\nattribution and citation generation.\\nfor unlabelled queries automatically from a base\\nLLM with the help of an NLI model. Next, the\\ncollected data are used for supervising LLMs to\\ngenerate grounded responses based on the retrieved\\npassages as well as include citations in their re-\\nsponses. As a test-time approach, we propose an\\niterative inference strategy that allows LLMs to\\nseek for additional information based on the self-\\ngrounding evaluation so as to refine its response.\\nThe tuning and test-time adaptation together enable\\nLLMs to effectively and efficiently ground their\\nresponses in the corpus. We apply AGREE frame-\\nwork to adapt an API-based LLM, text-bison , and\\nan open LLM, llama-2-13b , with training data col-\\nlected using unlabelled queries from three datasets.\\nWe conduct evaluation on both in-domain and out-\\nof-distribution datasets, comparing the proposed\\nAGREE framework against competitive in-context\\nlearning and post-hoc citing baselines. The exper-\\nimental results highlight that AGREE framework\\nsuccessfully improves grounding, in citation recall\\n& precision, compared to the baselines by a sub-\\nstantial margin (generally more than 20%). We\\nfind LLMs can learn to add accurate citations to\\ntheir responses with our carefully designed tuning\\nmechanisms. Furthermore, the improvements in\\ngrounding quality achieved by tuning using certain\\ndatasets can generalize well across domains. To\\nsummarize, our main contributions include:\\n• A learning-based approach that adapts a base\\nLLM to include accurate citations in its re-\\nsponse, leveraging automatically created data;\\n•A test-time adaptation (TTA) method that iter-atively improves responses of LLMs based on\\nthe citation information;\\n•Extensive experiments on two LLMs over five\\ndatasets demonstrating the effectiveness of the\\nproposed AGREE framework for improving\\ngrounding and citation generation.\\n2 Related Work\\nHallucination is a prevalent issue for generative\\nlanguage models on many tasks (Maynez et al.,\\n2020; Raunak et al., 2021; Dziri et al., 2021; Ji\\net al., 2023; Ye and Durrett, 2022; Tang et al., 2023;\\nHuang and Chang, 2023). It has been evaluated\\nin different ways, investigating the grounding in\\ngenerated responses (Bohnet et al., 2022; Rashkin\\net al., 2023; Min et al., 2023; Yue et al., 2023).\\nVarious approaches have been proposed to mit-\\nigate hallucination and improve the factuality of\\nLLM-generated responses. Among these, our work\\nparticularly focuses on providing citations to at-\\ntributable information source (Liu et al., 2023; Gao\\net al., 2023b). Unlike existing work that largely\\nrelies on zero-shot prompting or few-shot prompt-\\ning (Kamalloo et al., 2023; Gao et al., 2023b) or\\nuse an additional NLI model (Gao et al., 2023a;\\nChen et al., 2023) to add citations, we propose a\\nlearning-based approach that tunes LLMs to gener-\\nate better-grounded responses supported with cita-\\ntions.\\nMore broadly, recent work also investigates\\nmethods for improving factuality of LLMs with-\\nout using external knowledge, including inference-\\ntime intervention (Li et al., 2023b; Chuang et al.,', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 1}), Document(page_content='2023), cross-exam (Cohen et al., 2023; Du et al.,\\n2023), self-verify (Dhuliawala et al., 2023), or re-\\ninforcement learning (Tian et al., 2024; Wu et al.,\\n2023). Our work differs from them in providing\\ncitations to external knowledge in the responses.\\nAdditionally, there is past work that also uses ex-\\nternal knowledge (e.g., knowledge base) to re-\\nduce hallucination by injecting knowledge into\\nprompts (Elaraby et al., 2023; Peng et al., 2023).\\nWhile the external knowledge used for generating\\na response can possibly serve as a coarse and gen-\\neral reference, these approaches also do not offer\\ngranular, sentence-level citations as in our work.\\nLastly, the proposed framework is a form of a\\nretrieval augmented generation approach. While\\npast work has explored using retrieval to improve\\nLLM generation quality (Chen et al., 2017; Lewis\\net al., 2020; Guu et al., 2020; Izacard and Grave,\\n2020; Shi et al., 2023) or factuality (Shuster et al.,\\n2021; Jiang et al., 2023; Pan et al., 2023), our ap-\\nproach further enables LLMs to generate citations\\nand self-generated citations to guide retrieval.\\n3 Problem & Background\\nOur proposed framework aims to adapt a pre-\\ntrained LLM MBtoMAthat is able to provide\\ngrounded responses with citations. Given a text\\nquery Qand a corpus D={di}consisting of text\\npassages, the adapted LLM MAis required to gen-\\nerate a response Ato the query that is factually\\ngrounded in the corpus Das well as providing cita-\\ntionsCtogether with its response.\\nFollowing past work (Liu et al., 2023; Gao et al.,\\n2023b), we segment LLMs’ output into statements\\nby sentences and require each of the sentences to\\ncite a set of passages from the corpus. Specifically,\\nlets1, . . . , s nbe the statements in the answer A=\\ns1, . . . , s n. The citations C={E1, . . . , E n}links\\neach statement sito a set of evidence passages\\nEi⊂ D.\\nRecall that our adaptation aims to provide bet-\\nter grounded responses. With citations C, we can\\nquantify the grounding quality of a response Aby\\na grounding score G:\\nG(A,C) =1\\nnX\\niϕ(concat( Ei), si),\\nwhere ϕis an NLI model that assesses whether the\\nconcatenated passage concat(E i)supports si. The\\ngrounding score Gessentially averages how well\\neach sentence is supported by its citations.4 AGREE Framework\\nThe proposed AGREE framework takes a holistic\\nperspective for grounding, proposing a model tun-\\ning approach that adapts the base LLM to include\\ncitations in its responses, and introducing a test-\\ntime adaptation (TTA) mechanism that leverages\\nthe citation information for actively retrieving from\\nthe corpus and iteratively refining the responses.\\n4.1 Tuning LLMs\\nWe tune the LLM to self-ground the claims in their\\nresponses by providing citations to retrieved docu-\\nments. Our method is able to grant LLMs such an\\nability using only a collection of unlabeled queries\\n{Q}and an NLI model ϕ. As we are using un-\\nlabeled queries without reference responses, we\\nformulate the adaptation task as tuning LLMs to\\nachieve better grounding without heavily deviating\\nfrom the original generations (such an approach\\nof preservation has also been adopted in recent\\nwork (Gao et al., 2023a)). Conceptually, we adapt\\nMBtoMAso that the answers generated by the\\nadapted LLM MAshould satisfy the grounding\\nconstraints (with grounding score > τG) while max-\\nimizing the scores with respect to the base LLM\\nMB:\\nmax E(A,C)∼MA(·|Q,D)MB(A|Q,D) 1{G(A,C)≥τG}.\\n(1)\\nIn practice, we adopt a data-centric approach for\\noptimizing MA. For a given question, we opt to\\nuse the maximally-grounded response sampled\\nfrom the base LLM to construct the tuning data.\\nWe will detail the process in the following of this\\nsection.\\nData generation As shown in Fig 2, given the\\nquery, we first sample responses {A}from the\\nbase LLM MB(· |Q,D)using instruction fol-\\nlowing (see Appendix A for details). For each A=\\ns1, . . . , s nwe create citations C={Ei}using the\\nNLI model, ϕ, to link a sentence sito the maxi-\\nmally supported passage ei= max d∈Dϕ(d, si)if', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 2}), Document(page_content='lowing (see Appendix A for details). For each A=\\ns1, . . . , s nwe create citations C={Ei}using the\\nNLI model, ϕ, to link a sentence sito the maxi-\\nmally supported passage ei= max d∈Dϕ(d, si)if\\nthe passage eiactually support si(i.e.,ϕ(ei, si)>\\nτ).1Otherwise, we do not add a citation to\\nsi, and siis an unsupported statement. That is:\\nEi={ei}if ϕ(ei, si)> τ else {}.We use Uto\\n1In practice, we only present 5 passages retrieved from D\\nto the LLM for generating initial responses, and only generate\\ncitations to this set of retrieved passages. We use TRUE (Hon-\\novich et al., 2022), a T5-11B NLI model. Please refer to\\nAppendix A for more details.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 2}), Document(page_content='querypassagesBase LLMresponseresponse…………response [citation]response [citation]NLI Model0.300.65FinetuneQuery: why did the panic of 1837 in the united states have an effect on texasPassages: [1] (Panic of 1837): The Panic of 1837 was a financial crisis in the United States ... Speculative lending practices in the West, a sharp decline in cotton prices. [..]Response:  The Panic of 1837 in the United States had an effect on Texas. Texas was a major exporter of cotton. And the panic caused a sharp decline in cotton prices [1]……Adapted LLMClaims Not Supported: Texas was a major exporter of cotton.samplingadd citationssentence1sentence2……[1] passage1[2] passage2……\\nNLI: 0.90\\nNLI: 0.20\\nsentence1. sentence2 [1]. sentence3 [4] ……score𝒢\\nResponse with CitationsUnsupported Statementssentence1score𝒢Concrete Example of Tuning DataOverview of Tuning Process\\nData GenerationFigure 2: Illustration of the tuning process. We sample responses from the base model, use an NLI model to add\\ncitations to the sampled responses, and tune the base model with the best-grounded response. We also show a\\nconcrete example of tuning data on the right.\\ndenote the set of unsupported statements that can-\\nnot find citations. This allows us to evaluate the\\ngrounding of Aas in Section 3. Now, we can\\nchoose the best response A∗from{A}based on\\nthe grounding scores to form a grounded response,\\ni.e.,A∗= arg maxAG(A, C).\\nWe then use {Q, A∗, C∗}(C∗as the citations as-\\nsociated with A∗) to teach the base LLM to gener-\\nate grounded responses with citations. In addition\\nto citations, we also instruct the LLM to clearly\\nstate the unsupported statements U∗, as shown\\nin Fig. 2. We note that the tuning of framework\\ndoes not force all training responses to be perfectly\\ngrounded. Instead, we supervise the LLM itself\\nto identify unsupported statements. This allows\\nthe LLM to generate more flexibly and guide the\\nretrieval process with its knowledge.2\\nSupervised fine-tuning We have introduced how\\nwe construct supervision to instruct the LLM to\\nadd citations and state unsupported statements in\\nits response. To effectively tune the LLM, we ver-\\nbalize the entire process in natural language. We\\ndenote the verbalized natural language description\\nasVERB( A∗,C∗, U∗)(see Fig. 2 for a concrete ex-\\nample).3The natural language formalization also\\nallows us to conveniently tune the LLM with stan-\\ndard language modeling objectives:\\nMA= arg max\\nMX\\nQM(VERB( A∗,C∗, U∗)|Q,D).(2)\\nWe note that this actual objective, Eq (2), max-\\nimizes the log probability of generating the best-\\n2Please refer to Appendix A for more details on the tuning\\nmethod.\\n3Please refer to Appendix E for more examples of tuning\\ndata.\\nqueryquery related passages  RetrieverAdapted LLMRetriever\\nresponse [citation]unsupported statementssupplementing passages working passages Figure 3: Illustration of the test-time adaptation mecha-\\nnism. The adapted LLM retrieves from the corpus based\\non self-generated citation information to refine its re-\\nsponse in an iterative way.\\ngrounded answer A∗that is selected from the gener-\\nation of the base model. As A∗is sampled from the\\nbase model, such an objective avoids significant de-\\nviations from the original generations, which aligns\\nwith the goal of the conceptual objective (Eq (1)).\\nMulti-dataset training We use multiple ex-\\nisting datasets to construct the adaptation data\\nused to tune the pre-trained LLM, including\\nNatural Questions (NQ) (Kwiatkowski et al.,\\n2019), FEVER (Thorne et al., 2018), and Strat-\\negyQA (Geva et al., 2021). We choose these as\\nthey contain diverse text, and the answers to the\\ncorresponding queries require different types of rea-\\nsoning processes: NQ provides diverse queries nat-\\nurally asked by real human users; FEVER places a\\nparticular emphasis on fact verification; and Strate-\\ngyQA requires multi-hop reasoning with implicit\\nstrategy. It is worthwhile to note that AGREE only\\nuses queries, leaving out ground-truth answers, to\\nimprove LLMs.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 3}), Document(page_content='Algorithm 1 Iterative TTA\\n1:procedure ITERATIVE INFERENCE (Q,D,MA,k,B)\\ninput: A query Q, text corpus D, the adapted LLM MA, the number of passages kthatMAcan take as input, the budget for LLM calls B\\n2: relevant _psgs = []\\n3: ▷retrieve passages using the query\\n4: working _psgs :=RETRIEVE (Q,D)[:k]\\n5: ▷keep track of seen passages to avoid presenting duplicate passages to the LLM\\n6: seen _psgs := []\\n7: whileiter= 1 : Bdo\\n8: ▷Use the LLM to generate an answer Afor the query Qbased on the working psgs D. Additionally obtain the cited passages and unsupported the\\nsentences.\\n9: A,cited _psgs ,unsup _sents :=MA(Q,working _psgs )\\n10: ▷add cited passages to the list of relevant passages and de-duplicate the list\\n11: relevant _psgs :=DEDUPLICATE (relevant _psgs +cited _psgs)\\n12: ▷update the seen passages to include the working passages of this iteration\\n13: seen _psgs :=seen _psgs +working _psgs\\n14: ifunsup _sents is not None then\\n15: ▷retrieve additional information related to the unsupported statements\\n16: supplementing _psgs :=RETRIEVE (unsup _sents, D)\\n17: else\\n18: ▷include more query-related passages to acquire more complete information\\n19: supplementing _psgs :=RETRIEVE (Q,D)\\n20: ▷update the working passage to include supplementing passages that have not been presented to the LLM before\\n21: working _psgs :=DEDUPLICATE (relevant _psgs +SETDIFF(supplementing _psgs, seen _psgs))[:k]\\n22: return A,cited _psgs\\n4.2 Test-time adaptation\\nWe introduce a novel test-time adaptation (TTA)\\nmethod for the inference procedure, overviewed\\nin Fig. 3. Our framework is a form of retrieval\\naugmented generation framework – at the core of\\nour approach lies the adapted LLM that is able to\\nanswer a query based on a set of given passages\\nretrieved from the corpus, and, more importantly,\\nself-ground its response to add citations to the pas-\\nsages as well as to find unsupported statements\\nneeding further investigation. With these capabil-\\nities, the adapted LLM can iteratively construct a\\nset of relevant passages from the large corpus D\\nand refine its response to the query.\\nThe detailed procedure of TTA is shown in Al-\\ngorithm (1). Given a query Qand the corpus D,\\nwe first retrieve based on the query to obtain an\\ninitial set of working passages. Next, we employ\\nthe following procedure iteratively until we con-\\nsume all the budget Bof invoking LLM calls. At\\neach iteration, the LLM generates a response to the\\nquery based on the working passages, adds cita-\\ntions to its response, and finds out any unsupported\\nstatements that do not have citations (ln 9). Then,\\nwe add the cited passages to the list of relevant\\npassages. Lastly, at each iteration, we update the\\nworking passages – if there are unsupported state-\\nments, we include additional information retrieved\\nbased on the unsupported statements (ln 15), other-\\nwise, we include more passages that are retrieved\\nbased on the query to acquire more complete in-\\nformation (ln 17). We only include passages that\\nare new and haven’t been presented to the LLMDataset Type Corpus #\\nTrain\\nNQ Factoid QA Wiki 2500\\nStrategyQA Multi-htop QA Wiki 1000\\nFever Fact Checking Wiki 1000\\nIn-Distribution Test\\nNQ Factoid QA Wiki 700\\nStrategyQA Factoid QA Wiki 460\\nOut-of-Distribution Test\\nASQA Ambiguous QA Wiki 948\\nQAMPARI Multi-answer QA Wiki 1000\\nEnterprise Customer Support QA Enterprise 580\\nTable 1: Statistics used for adaptation and test datasets.\\nIn addition to in-domain test datasets, we also investi-\\ngate the generalization to out-of-distribution datasets\\nthat exhibit different reasoning processes or different\\ncorpus types.\\nyet (ln 19). Note that at each iteration, we let the\\nLLM to re-generate a response based on the current\\nworking passages instead of editing from previous\\none, which we observed lead to better fluency.\\nThe design of our proposed TTA enables effi-\\ncient and flexible inference. We rely on the LLM\\nto generate citations itself, which has the advantage\\nof reduced overhead of invoking an additional NLI', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 4}), Document(page_content='The design of our proposed TTA enables effi-\\ncient and flexible inference. We rely on the LLM\\nto generate citations itself, which has the advantage\\nof reduced overhead of invoking an additional NLI\\nmodel in a post-hoc way. Also, as we iteratively\\nrefine the answer, such a process can be streamed\\nand flexibly controlled by setting a budget in de-\\nployment.\\n5 Experiments\\n5.1 Setup\\nEvaluation datasets We conduct comprehensive\\nevaluation on 5 datasets. Recall that we train\\nAGREE on multiple datasets including NQ, Strate-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 4}), Document(page_content='NQ StrategyQA ASQA QAMPARI Enterprise\\nem-rec rec pre acc rec pre em-rec rec pre rec-5 rec pre rec pre\\nBase model: text-bison-001\\nICLC ITE 47.6 52.1 56.3 74.5 13.6 27.8 39.5 47.3 49.8 20.3 22.7 24.5 30.2 40.5\\nPOSTSEARCH 45.1 29.7 28.7 75.5 20.1 20.1 38.4 19.2 19.2 22.5 16.2 16.2 15.9 15.9\\nPOSTATTR 45.1 31.5 31.5 75.5 18.4 18.4 35.1 38.0 38.0 22.5 18.5 18.5 20.1 20.1\\nAGREE W/OTTA 50.0 67.9 73.1 74.1 33.4 50.5 39.5 65.9 70.5 20.1 60.1 64.5 55.8 67.1\\nAGREE W/ TTA 53.1 70.1 75.0 74.9 39.2 57.9 40.9 73.2 77.0 20.9 62.9 67.1 57.2 68.6\\nBase model: llama-2-13b\\nICLC ITE 45.8 42.8 41.6 65.5 20.6 33.1 35.2 38.2 39.4 21.0 10.2 10.4 30.6 38.8\\nPOSTSEARCH 35.9 17.5 17.5 64.3 8.7 8.7 25.0 23.6 23.6 12.0 27.5 27.5 13.4 13.4\\nPOSTATTR 35.9 26.0 26.0 64.3 12.5 12.5 25.0 33.6 33.6 12.0 28.9 28.9 18.7 18.7\\nAGREE W/OTTA 47.9 50.5 56.6 65.0 25.5 35.0 35.7 50.2 55.3 17.1 40.4 43.6 50.6 53.8\\nAGREE W/ TTA 51.0 62.0 66.0 64.6 30.2 37.2 39.4 64.0 66.8 17.9 51.4 53.4 50.4 55.4\\nTable 2: Answer accuracy and grounding (measured by citation quality) of AGREE and baselines across 5 datasets.\\nOur approach achieves substantially better citation grounding (measured by citation recall) and citation precision\\ncompared to the baselines.\\ngyQA, and Fever. In addition to the two in-domain\\ntest sets, NQ and StrategyQA (we leave out the\\nnon-QA dataset, FEVER), we further test the gen-\\neralization of adapted LLMs on 3 out-of-domain\\ndatasets, including ASQA (Stelmakh et al., 2022),\\nQAMPARI (Amouyal et al., 2022), and an Enter-\\nprise dataset.4In particular, ASQA and QAMPARI\\ncontain questions of ambiguous answers and multi-\\nple answers. The Enterprise dataset is a proprietary\\ndataset which requires provided answers that are\\ngrounded in customer service passages. Such an\\nevaluation suite allows assessing the generalization\\ncapability of the adapted LLMs for OOD question\\ntypes (ASQA and QAMPARI) as well as to an en-\\ntirely different corpus (Enterprise).\\nModels We demonstrate AGREE framework with\\ntwo LLMs, text-bison and LlaMA-2-13B (Touvron\\net al., 2023). We use GTR-large (Ni et al., 2021)\\nas our retriever, and use TRUE (Honovich et al.,\\n2022) as the NLI model.\\nBaselines We evaluate the effectiveness AGREE\\nin two settings, invoking LLMs once, without TTA;\\nand invoking LLMs multiple times, with the pro-\\nposed TTA.5We compare with three baselines from\\nrecent work, including one prompting-based ap-\\nproach and two post-hoc citing approaches, de-\\nscribed below.\\n4We use FEVER to create tuning data, but do not use it\\nfor evaluations. As we use LLMs in a zero-shot setting, the\\nLLMs do not always answer with the specific labels defined in\\nFEVER, which might introduce inaccuracies in the evaluation\\nof answer correctness.\\n5We set the budget Bfor LLM calls used in TTA to be 4.Few-shot In-Context Learning (ICLC ITE):\\nFollowing Gao et al. (2023b), we prompt LLMs\\nwith few-shot examples (Gao et al., 2023b), each\\nconsisting of a query, a set of retrieved passages,\\nand an answer with inline citations. The LLMs can\\ntherefore learn from the in-context examples and\\ngenerated citations in the responses. It is worth-\\nwhile to note that ICLC ITEis a RAG baseline that\\nalso uses retrieved passages .\\nPost-hoc search (POSTSEARCH ): Follow-\\ning Gao et al. (2023b), given a query, we first\\ninstruct LLMs to answer the query without pas-\\nsages, and then add citations in a post-hoc way via\\nsearching. We link each claim in the response to\\nthe most relevant passage retrieved from a set of\\nquery-related passages. This baseline only uses the\\nretriever but not the NLI model.\\nPost-hoc Attribution (POSTATTR): Follow-\\ning Gao et al. (2023a), instead of citing the most\\nrelevant passage, for each claim, we retrieve a set\\nofkpassages from the corpus, and then use the\\nNLI model, ϕ, to link to the passage that maxi-\\nmally supports the claim. We note both baselines\\nin the post-hoc citing paradigm only rely on LLMs’\\nparametric knowledge.6\\nMetrics We mainly focus on improving the\\ngrounding quality of generated responses, re-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 5}), Document(page_content='in the post-hoc citing paradigm only rely on LLMs’\\nparametric knowledge.6\\nMetrics We mainly focus on improving the\\ngrounding quality of generated responses, re-\\nflected by the quality of citations. Following past\\nwork (Gao et al., 2023b), we report the citation\\nrecall (rec) and citation precision (pre) on all the\\nevaluation datasets. We note that citation recall\\n6Please refer to Appendix B for more details on experi-\\nmental setup.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 5}), Document(page_content='aggregates how well each sentence is supported\\nby the citation to the corpus, which is essentially\\nthe grounding score G. Therefore, we prioritize\\non the evaluation of citation recall.\\nWe also report the correctness of the generated\\noutputs. For NQ, we report exact match recall (em-\\nrec; whether the short answers are substrings in the\\nresponse). For StrategyQA, we report the accuracy\\n(acc). For ASQA and QAMPARI, we use subsets\\nfrom Gao et al. (2023b), and report the exact match\\nrecall (em-rec) for ASQA and recall-5 (rec-5, con-\\nsidering recall to be 100% if the prediction includes\\nat least 5 correct answers) for QAMPARI. For the\\nEnterprise dataset, we only report the citation qual-\\nity as there are no ground truth answers for this\\ndataset, and citation quality reflects whether the\\nmodel can provide accurate information.\\n5.2 Results and analyses\\nTuning is effective for superior grounding: Ta-\\nble 2 summarizes the results obtained using our\\nAGREE framework and compares with the base-\\nlines. As suggested by the results, across 5 datasets,\\nAGREE can generate responses that are better\\ngrounded in the text corpus and provide accurate\\ncitations to its response, substantially outperform-\\ning all the baselines. When tuned with high-quality\\ndata, LLMs can effectively learn to self-ground\\ntheir response without needing an additional NLI\\nmodel. On the other hand, ICLC ITE, which solely\\nrelies on in-context learning, cannot generate cita-\\ntions as accurately as a tuned LLM, as suggested\\nby the large gap on citation precision between\\nICLC ITEand AGREE . We also observe simi-\\nlar findings as suggested by Gao et al. (2023b):\\nPOSTCITEoften leads to poor citation quality –\\nwithout being conditioned on passages, the re-\\nsponse from POSTCITEoften cannot be paired with\\npassages that lead to high citation recall for the gen-\\nerated claims.\\nThe performance improvements can general-\\nize: Recall that we adapt the base LLM only us-\\ning in-domain training sets (NQ, StrategyQA, and\\nFEVER), and directly test the model on out-of-\\ndistribution (OOD) test set (ASQA, QAMPARI,\\nEnterprise). The results suggest that the improve-\\nments obtained from training on in-domain datasets\\ncan effectively generalize to OOD datasets that con-\\ntain different question types or use different types\\nof corpus. This is a fundamental advantage of the\\nproposed approach – AGREE can generalize to atarget domain in the zero-shot setting without need-\\ning any samples from the target domain, which is\\nneeded for ICLC ITE.\\nTTA improves both grounding and answer cor-\\nrectness: The comparison between AGREE with-\\nout and with TTA highlights the effectiveness of the\\nproposed iterative TTA strategy. We observe im-\\nprovements in terms of both better grounding and\\naccuracy. For instance, TTA improves llama-2 an-\\nswer correctness by 3.1 and 3.7 on NQ and ASQA,\\nrespectively. Such improvements can be attributed\\nto the fact that our TTA allows the LLMs to ac-\\ntively collect relevant passages to construct better\\nanswers following the self-grounding guidance.\\nDiscussions on answer correctness: In general,\\nAGREE w/ TTA can achieve better correctness com-\\npared to ICLC ITE.AGREE w/o TTA achieves similar\\nanswer correctness with ICLC ITE, as both meth-\\nods are conditioned on the same set of passages.\\nAs a result, the quality of passages heavily inter-\\nvenes on the correctness of the answers. Unlike\\nAGREE andICLC ITE,POSTATTR purely relies\\non the parametric knowledge of the LLMs to an-\\nswer the query. As a result, POSTATTR generally\\nachieves inferior answer correctness compared to\\nAGREE andICLC ITEon these two LLMs, espe-\\ncially on the less capable LLM, llama-2-13b , that\\nhas less accurate knowledge compared to bison .\\nMoreover, on the Enterprise dataset which contains\\nmore domain-specific information, POSTATTR ut-\\nterly fails to recall attributable information from\\nLLMs’ parametric knowledge.\\nResults with different LLMs: Our approach\\nsuccessfully adapts both text-bison-001 and', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 6}), Document(page_content='terly fails to recall attributable information from\\nLLMs’ parametric knowledge.\\nResults with different LLMs: Our approach\\nsuccessfully adapts both text-bison-001 and\\nllama-2-13b .llama is generally less capable com-\\npared to bison , underperforming bison in terms\\nof answer correctness and citation quality. Still,\\nAGREE also consistently outperforms the baseline,\\ngenerating more grounded answers as well as pro-\\nviding more precise citations. This highlights that\\nthe proposed tuning-based adaptation approach is\\nmodel-agnostic and is effective across LLMs of\\nvarying capabilities.\\nComputational efficiency: AGREE framework\\nfine-tunes the base LLM to enable self-grounding\\nwithout needing for additional in-context examples\\nor NLI models. As a result, our framework is able\\nto achieve strong citation performance without ex-\\npensive inference cost. Table 4 shows the compari-\\nson between the computation cost, measured by the', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 6}), Document(page_content='NQ StrategyQA ASQA QAMPARI Enterprise\\nem-rec rec pre acc rec pre em-rec rec pre rec-5 rec pre rec pre\\nBase model: text-bison-001\\nICLC ITE 47.6 52.1 56.3 74.5 13.6 27.8 39.5 47.3 49.8 20.3 22.7 24.5 30.2 40.5\\nAGREEMulti-dataset\\nW/OTTA 50.0 67.9 73.1 74.1 33.4 50.5 39.5 65.9 70.5 20.1 60.1 64.5 55.8 67.1\\nAGREENQ-only\\nW/OTTA 49.4 62.3 69.1 74.1 33.0 45.5 38.4 56.0 64.5 19.1 43.7 49.5 40.5 59.2\\nBase model: llama-2-13b\\nICLC ITE 45.8 42.8 41.6 65.5 20.6 33.1 35.2 38.2 39.4 21.0 10.2 10.4 30.6 38.8\\nAGREEMulti-dataset\\nW/OTTA 47.9 50.5 56.6 65.0 25.5 35.0 35.7 50.2 55.3 17.1 40.4 43.6 50.6 52.8\\nAGREENQ-only\\nW/OTTA 48.1 47.4 53.6 62.1 25.0 30.2 35.0 44.0 51.2 15.7 33.1 38.0 44.7 49.2\\nAGREEDistill\\nW/OTTA 47.9 59.1 65.1 64.4 30.5 41.1 35.2 58.5 65.2 17.9 52.5 52.7 48.1 55.9\\nTable 3: Analysis on the impact of training data. Training with multiple datasets ( AGREEMulti-dataset) leads to better\\ngrounding (citation recall) and better citation precision across datasets, compared to training using the NQ dataset\\n(AGREENQ-only). The citation quality of a less capable model llama-2-13b can also benefit from tuning using outputs\\nfrom a more capable model ( text-bison-001 ).\\n# Tok: LLM # Tok: NLI (T5-11B)\\nICLC ITE 2800 −\\nPOSTATTR 360 3520\\nAGREE w/o TTA 1210 −\\nAGREE w/ TTA 4840 −\\nTable 4: The average computation cost (for one query)\\nof different methods measured by the number of to-\\nkens processed by the LLM and the NLI model (based\\non a T5-11B architecture). AGREE w/o TTA is able to\\nachieve better citation quality compared to ICLC ITE,\\ndespite consuming less than half of the tokens needed\\nfor ICLC ITE.\\nnumber of tokens processed by the LLM and the\\nNLI model, needed for one query of our methods\\nand that of the baselines. Compared to ICLC ITE,\\nAGREE w/o TTA uses much fewer tokens due to not\\nusing additional in-context examples, but achieves\\nsignificantly better citation quality (see Table 2).\\nPOSTATTR does not use retrieved passages in the\\nprompts and hence requires less computation on the\\nLLM compared to our framework, but it requires\\nadditional overhead of extensively invoking the\\nNLI model (which has 11B parameters – see Ap-\\npendix A for details) to verify the each of the claims\\nbased on each of the retrieved passages. The cita-\\ntion performance of POSTATTR also substantially\\nlags ICLC ITEandAGREE .AGREE w/ TTA requires\\nmore computation compared to AGREE w/o TTA , but\\nis able to achieve both better citation quality and\\nimprovements in answer correctness.\\nThe impact of training with multiple datasets:\\nAGREE uses multiple datasets spanning factoid QA,\\nmulti-hop reasoning, and fact-checking to construct\\ndata for adapting the base model. We expect such\\na combination can grant the adapted model bettergeneralization to different types of questions and\\ndifferent text distributions. We conduct an analysis\\nto investigate the benefits of using multiple datasets\\nfor tuning. Table 3 shows the performance of our\\napproach trained using multi-datasets and a counter-\\npart that is trained only on NQ data ( AGREENQ-only).\\nThe results suggest that training using NQ leads to\\ninferior citation quality compared to training on the\\ncombination of three datasets across all the datasets.\\nThe performance gap is especially significant on\\ndatasets other than NQ. Moreover, training only on\\nNQ also leads to inferior answer correctness across\\nall the datasets. Nevertheless, training only on NQ\\ncan still improve performance compared to solely\\nrelying on in-context learning (ICLC ITE).\\nAGREE as a distillation approach: Our work\\nmainly focuses on improving the base LLM’s\\ngrounding capability in a self-improving way. That\\nis, we use the samples generated by the base LLM\\nitself to adapt the base LLM, as opposed to distill-\\ning from proprietary models which may impose\\nconstraints on the deployment of adapted model.\\nNevertheless, we conduct an analysis to investigate\\nthe effectiveness as a distillation approach in a data-\\ncentric way by using data generated from a more', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 7}), Document(page_content='Nevertheless, we conduct an analysis to investigate\\nthe effectiveness as a distillation approach in a data-\\ncentric way by using data generated from a more\\ncapable model to enhance the grounding data for\\na less capable model. We use the data generated\\nbytext-bison-001 to tune llama-2-13b .7As shown\\nin the last row of Table 3, AGREEDistillachieves\\nbetter citation quality compared to AGREE on\\nllama-2-13b , as it is trained on better grounded re-\\nsponses produced by a more capable model. How-\\never, llama-2 fine-tuned on data generated by bison\\nstill can’t level the performance on bison as con-\\n7Please refer to Appendix B for details.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 7}), Document(page_content='Example from NQ\\nQuery: where did the world‘s largest recorded wave occur?\\nWithout TTA\\nPassage [1]: (retrieved using the query)\\n(Title: 1958 Lituya Bay earthquake and megatsunami) Lituya Bay has a\\nhistory of megatsunami events, but the 1958 event was the first for which\\nsufficient data was captured...\\nAGREE w/o TTA :The world’s largest recorded wave occurred in Lituya\\nBay,Alaska. The data was captured in a 1958 megatsunami event [1].\\nWith TTA\\nPassage [2]: (retrieved using prior unsupported statement )\\n(Title: Megatsunami) In 1958, a 7.8 strike-slip earthquake in southeast\\nAlaska caused 90 million tonnes of rock and ice to drop into the deep\\nwater at the head of Lituya Bay. The block fell almost vertically and hit\\nthe water with sufficient force to ...\\nPassage [3]: (retrieved using prior unsupported statement )\\n(Title: Tsunami) ...Their existence was confirmed in 1958, when a giant\\nland slide in Lituya Bay, Alaska, caused the highest wave ever recorded,\\nwhich had a height of 524 metres....\\nAGREE w/ TTA :The world’s largest recorded wave occurred in Lituya\\nBay, Alaska [3]. The wave was caused by a giant landslide that occurred\\nin 1958 [2]. The landslide displaced 90 million...[2].\\nExample from ASQA\\nQuery: Who played the weasley brothers in harry potter?\\nWithout TTA\\nPassage [1]: (retrieved using the query)\\n(Title: James and Oliver Phelps) James Andrew Eric and Oliver Martyn\\nJohn Phelps are identical twin British actors, best known for playing\\nFred and George Weasley in the \"Harry Potter\" film series.\\nAGREE w/o TTA :The Weasley brothers in Harry Potter were played by\\nidentical twin actors James and Oliver Phelps [1].\\nWith TTA\\nPassage [2]: (additional passages retrieved using the query)\\n(Title: Chris Rankin) ... Chris Rankin Christopher William \"Chris\"\\nRankin is a New Zealand-born British actor who is best known for playing\\nPercy Weasley in the \"Harry Potter\" film franchise.\\nAGREE w/ TTA :James and Oliver Phelps played Fred and George Weasley\\nin the Harry Potter film series [1]. Chris Rankin played Percy Weasley in\\nthe Harry Potter film franchise [2].\\nFigure 4: Output examples of the proposed AGREE\\nframework with text-bison-001 as the base model.\\nTTA is able to improve the response by retrieving more\\nrelevant information to precisely support a statement\\n(see top) or finding more passages to generate a more\\ncomplete response (see bottom).\\nstrained by the gap between the capabilities of these\\ntwo LLMs.\\nQualitative analyses: We qualitatively analyze\\nthe advantages of the proposed AGREE framework\\ncompared to ICLC ITE, the strongest among the\\nbaselines. We observe that on both text-bison-001\\nand llama-2-13b ,ICLC ITEachieves inferior cita-\\ntion quality due to failure in following the cita-\\ntion format (e.g., adding citations after the peri-\\nods, violating the instructions), linking a statement\\nto a relevant but un-attributable passage (as indi-\\ncated by poor citation precision), and introducing\\nmore auxiliary information not mentioned in the\\nretrieved passages (as indicated by citation recall).\\nOurAGREE framework mitigates these issues by\\ntuning on well-grounded responses certified by the\\nNLI model. We also provide example outputs in\\nFig. 4 comparing the outputs of AGREE with and\\nwithout proposed TTA and observe that TTA canhelp find more supporting passages by active re-\\ntrieving using unsupported statements (top) or it-\\neratively find more passages to construct a more\\ncomplete response (bottom).\\n6 Conclusion\\nWe introduce a novel framework, AGREE , that\\nadapts LLM for improved grounding. The pro-\\nposed framework tunes a pre-trained LLM to self-\\nground its response in retrieved passages using au-\\ntomatically collected data. The integrated capabil-\\nity for grounding their responses further enables\\nthe LLM to improve the responses at test time. Our\\nevaluations across five datasets demonstrate the\\nbenefits of the proposed learning-based approach\\ncompared to approaches that solely rely on prompt-\\ning or the parametric knowledge of LLMs.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 8}), Document(page_content='evaluations across five datasets demonstrate the\\nbenefits of the proposed learning-based approach\\ncompared to approaches that solely rely on prompt-\\ning or the parametric knowledge of LLMs.\\n7 Limitations and future work\\nAGREE employs an automated data creation that\\nrelies on an NLI model, instead of humans. Thus,\\nthe citation quality is dependent on the NLI model.\\nAs suggested in Gao et al. (2023b); Honovich et al.\\n(2022), one issue might be favoring “fully support”\\nand cannot effectively detect “partially support\".\\nThus, the adapted LLMs may favor adding “fully\\nsupport\" citations. One solution is to curate a set of\\nhuman-annotated citations for “partially support\",\\nwhich we defer to future work. Also, our evalua-\\ntion follows prior work (Rashkin et al., 2023; Gao\\net al., 2023a) and uses the NLI model to quantify\\nthe grounding and citation quality. Therefore, our\\nwork can encounter the same issue as past work:\\ngrounding and citation quality evaluation is limited\\nby the capabilities of the NLI model.\\nAGREE uses created grounded responses to\\nLLMs via supervised fine-tuning, as we demon-\\nstrate it leads to strong empirical results. It is\\nalso possible to treat grounding as a preference\\nand RLHF (Ouyang et al., 2022) to tune LLMs,\\nwhich we leave to future work. AGREE tuning\\nincurs additional cost that is a one-time require-\\nment for adapting the LLM. Considering the sub-\\nstantial grounding improvements, we believe this\\nwould be acceptable for most applications, espe-\\ncially for those with high-reliability requirements.\\nFuture work can possibly explore training a sepa-\\nrate universal improved grounding model beyond\\ntask-specific adaptation.\\nWe have mainly considered open domain ques-\\ntion answering datasets focusing on information', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 8}), Document(page_content='seeking tasks in English. Generalization to other\\nlong form generation tasks and other languages can\\nbe important future work directions.\\nLastly, adding citations to LLM-generated re-\\nsponses in AGREE might carry a shared risk with\\nrelated research – a seemingly plausible but incor-\\nrect citation could potentially make an unsupported\\nstatement more convincing to users.\\nAcknowledgments\\nThanks to anonymous reviewers for their helpful\\nfeedback, as well as to Jinsung Yoon, Andreas\\nTerzis, Yanfei Chen, Ankur Taly, Lucas Zhang,\\nand Tina Pang for their help with various aspects\\nof this work.\\nReferences\\nSamuel Joseph Amouyal, Tomer Wolfson, Ohad Rubin,\\nOri Yoran, Jonathan Herzig, and Jonathan Berant.\\n2022. Qampari: An open-domain question answer-\\ning benchmark for questions with many answers from\\nmultiple paragraphs.\\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni,\\nDaniel Andor, Livio Baldini Soares, Jacob Eisenstein,\\nKuzman Ganchev, Jonathan Herzig, Kai Hui, et al.\\n2022. Attributed question answering: Evaluation and\\nmodeling for attributed large language models. arXiv\\npreprint arXiv:2212.08037 .\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\\nClark, Christopher Berner, Sam McCandlish, Alec\\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\\nLanguage models are few-shot learners.\\nAnthony Chen, Panupong Pasupat, Sameer Singh, Hon-\\ngrae Lee, and Kelvin Guu. 2023. Purr: Efficiently\\nediting language model hallucinations by denois-\\ning language model corruptions. arXiv preprint\\narXiv:2305.14908 .\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to answer open-\\ndomain questions. In Association for Computational\\nLinguistics (ACL) .\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha\\nTsvyashchenko, Joshua Maynez, Abhishek BaindoorRao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vin-\\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben-\\nton C. Hutchinson, Reiner Pope, James Bradbury, Ja-\\ncob Austin, Michael Isard, Guy Gur-Ari, Pengcheng\\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\\nGarcía, Vedant Misra, Kevin Robinson, Liam Fe-\\ndus, Denny Zhou, Daphne Ippolito, David Luan,\\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz, Er-\\nica Oliveira Moreira, Rewon Child, Oleksandr Polo-\\nzov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele\\nCatasta, Jason Wei, Kathleen S. Meier-Hellstern,\\nDouglas Eck, Jeff Dean, Slav Petrov, and Noah\\nFiedel. 2022. Palm: Scaling language modeling with\\npathways. ArXiv , abs/2204.02311.\\nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon\\nKim, James Glass, and Pengcheng He. 2023. Dola:\\nDecoding by contrasting layers improves factu-\\nality in large language models. arXiv preprint\\narXiv:2309.03883 .\\nRoi Cohen, May Hamri, Mor Geva, and Amir Glober-\\nson. 2023. Lm vs lm: Detecting factual errors via\\ncross examination. ArXiv , abs/2305.13281.\\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and\\nJason Weston. 2023. Chain-of-verification reduces\\nhallucination in large language models. ArXiv ,\\nabs/2309.11495.\\nYilun Du, Shuang Li, Antonio Torralba, Joshua B.\\nTenenbaum, and Igor Mordatch. 2023. Improving\\nfactuality and reasoning in language models through\\nmultiagent debate. ArXiv , abs/2305.14325.\\nNouha Dziri, Andrea Madotto, Osmar Zaïane, and', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 9}), Document(page_content='Tenenbaum, and Igor Mordatch. 2023. Improving\\nfactuality and reasoning in language models through\\nmultiagent debate. ArXiv , abs/2305.14325.\\nNouha Dziri, Andrea Madotto, Osmar Zaïane, and\\nAvishek Joey Bose. 2021. Neural path hunter: Re-\\nducing hallucination in dialogue systems via path\\ngrounding. In Proceedings of the 2021 Conference\\non Empirical Methods in Natural Language Process-\\ning, pages 2197–2214, Online and Punta Cana, Do-\\nminican Republic. Association for Computational\\nLinguistics.\\nMohamed Elaraby, Mengyin Lu, Jacob Dunn, Xuey-\\ning Zhang, Yu Wang, and Shizhu Liu. 2023. Halo:\\nEstimation and reduction of hallucinations in open-\\nsource weak large language models. arXiv preprint\\narXiv:2308.11764 .\\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-\\ncent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,\\nand Kelvin Guu. 2023a. RARR: Researching and\\nrevising what language models say, using language\\nmodels. In Proceedings of the Annual Conference of\\nthe Association for Computational Linguistics (ACL) .\\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\\n2023b. Enabling large language models to generate', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 9}), Document(page_content='text with citations. In Proceedings of the Conference\\non Empirical Methods in Natural Language Process-\\ning (EMNLP) .\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\\nDan Roth, and Jonathan Berant. 2021. Did aristotle\\nuse a laptop? a question answering benchmark with\\nimplicit reasoning strategies. Transactions of the\\nAssociation for Computational Linguistics , 9:346–\\n361.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Mingwei Chang. 2020. Retrieval augmented\\nlanguage model pre-training. In International confer-\\nence on machine learning , pages 3929–3938. PMLR.\\nHangfeng He, Hongming Zhang, and Dan Roth. 2022.\\nRethinking with retrieval: Faithful large language\\nmodel inference. arXiv preprint arXiv:2301.00303 .\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\\nYejin Choi. 2019. The curious case of neural text\\ndegeneration. arXiv preprint arXiv:1904.09751 .\\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\\nScialom, Idan Szpektor, Avinatan Hassidim, and\\nYossi Matias. 2022. TRUE: Re-evaluating factual\\nconsistency evaluation. In Proceedings of the 2022\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies , pages 3905–3920, Seattle,\\nUnited States. Association for Computational Lin-\\nguistics.\\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. 2022. LoRA: Low-rank adaptation of\\nlarge language models. In International Conference\\non Learning Representations .\\nJie Huang and Kevin Chen-Chuan Chang. 2023. Cita-\\ntion: A key to building responsible and accountable\\nlarge language models. ArXiv , abs/2307.02185.\\nGautier Izacard and Edouard Grave. 2020. Leveraging\\npassage retrieval with generative models for open\\ndomain question answering.\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\\nMadotto, and Pascale Fung. 2023. Survey of halluci-\\nnation in natural language generation. ACM Comput.\\nSurv. , 55(12).\\nZhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing\\nSun, Li-Yu Daisy Liu, Jane Dwivedi-Yu, Yiming\\nYang, Jamie Callan, and Graham Neubig. 2023.\\nActive retrieval augmented generation. ArXiv ,\\nabs/2305.06983.\\nEhsan Kamalloo, Aref Jafari, Xinyu Crystina Zhang,\\nNandan Thakur, and Jimmy Lin. 2023. Ha-\\ngrid: A human-llm collaborative dataset for gener-\\native information-seeking with attribution. ArXiv ,\\nabs/2307.16883.Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\\nton Lee, Kristina Toutanova, Llion Jones, Matthew\\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\\nral questions: A benchmark for question answering\\nresearch. Transactions of the Association for Compu-\\ntational Linguistics , 7:452–466.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. 2020. Retrieval-augmented generation\\nfor knowledge-intensive nlp tasks. Advances in Neu-\\nral Information Processing Systems , 33:9459–9474.\\nJunyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and\\nJi-Rong Wen. 2023a. HaluEval: A large-scale hal-\\nlucination evaluation benchmark for large language\\nmodels. In Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing .\\nKenneth Li, Oam Patel, Fernanda Viégas, Hanspeter\\nPfister, and Martin Wattenberg. 2023b. Inference-\\ntime intervention: Eliciting truthful answers from a\\nlanguage model.\\nNelson F. Liu, Tianyi Zhang, and Percy Liang. 2023.\\nEvaluating verifiability in generative search engines.\\nArXiv:2304.09848.\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\\nRyan McDonald. 2020. On faithfulness and factu-\\nality in abstractive summarization. In Proceedings\\nof the 58th Annual Meeting of the Association for', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 10}), Document(page_content='Joshua Maynez, Shashi Narayan, Bernd Bohnet, and\\nRyan McDonald. 2020. On faithfulness and factu-\\nality in abstractive summarization. In Proceedings\\nof the 58th Annual Meeting of the Association for\\nComputational Linguistics , pages 1906–1919, On-\\nline. Association for Computational Linguistics.\\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\\nJohn Aslanides, Francis Song, Martin Chadwick,\\nMia Glaese, Susannah Young, Lucy Campbell-\\nGillingham, Geoffrey Irving, and Nathan McAleese.\\n2022. Teaching language models to support answers\\nwith verified quotes. ArXiv , abs/2203.11147.\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\\nFactscore: Fine-grained atomic evaluation of factual\\nprecision in long form text generation. In Proceed-\\nings of the Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) .\\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Her-\\nnandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,\\nKeith B. Hall, Ming-Wei Chang, and Yinfei Yang.\\n2021. Large dual encoders are generalizable retriev-\\ners.ArXiv , abs/2112.07899.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\\nCarroll L. Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex\\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\\nLuke E. Miller, Maddie Simens, Amanda Askell, Pe-\\nter Welinder, Paul Francis Christiano, Jan Leike, and', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 10}), Document(page_content='Ryan J. Lowe. 2022. Training language models to\\nfollow instructions with human feedback. ArXiv ,\\nabs/2203.02155.\\nLiangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan\\nLuu, William Yang Wang, Min-Yen Kan, and Preslav\\nNakov. 2023. Fact-checking complex claims with\\nprogram-guided reasoning. In Proceedings of the\\n61th Annual Meeting of the Association for Compu-\\ntational Linguistics (ACL 2023) , Toronto, Canada.\\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\\nYu, Weizhu Chen, et al. 2023. Check your facts and\\ntry again: Improving large language models with\\nexternal knowledge and automated feedback. arXiv\\npreprint arXiv:2302.12813 .\\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\\nLora Aroyo, Michael Collins, Dipanjan Das, Slav\\nPetrov, Gaurav Singh Tomar, Iulia Turc, and David\\nReitter. 2023. Measuring attribution in natural lan-\\nguage generation models. Computational Linguistics ,\\npages 1–64.\\nVikas Raunak, Arul Menezes, and Marcin Junczys-\\nDowmunt. 2021. The curious case of hallucinations\\nin neural machine translation. In Proceedings of\\nthe 2021 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies , pages 1172–1183,\\nOnline. Association for Computational Linguistics.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\\nWen tau Yih. 2023. Replug: Retrieval-augmented\\nblack-box language models. ArXiv , abs/2301.12652.\\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\\nand Jason Weston. 2021. Retrieval augmentation\\nreduces hallucination in conversation. In Findings\\nof the Association for Computational Linguistics:\\nEMNLP 2021 , pages 3784–3803, Punta Cana, Do-\\nminican Republic. Association for Computational\\nLinguistics.\\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\\nWei Chang. 2022. ASQA: Factoid questions meet\\nlong-form answers. In Proceedings of the 2022 Con-\\nference on Empirical Methods in Natural Language\\nProcessing , pages 8273–8288, Abu Dhabi, United\\nArab Emirates. Association for Computational Lin-\\nguistics.\\nKai Sun, Y . Xu, Hanwen Zha, Yue Liu, and Xinhsuai\\nDong. 2023. Head-to-tail: How knowledgeable are\\nlarge language models (llm)? a.k.a. will llms replace\\nknowledge graphs? ArXiv , abs/2308.10168.\\nLiyan Tang, Tanya Goyal, Alexander R. Fabbri, Philippe\\nLaban, Jiacheng Xu, Semih Yavuz, Wojciech Kry ´s-\\nci´nski, Justin F. Rousseau, and Greg Durrett. 2023.\\nUnderstanding factual errors in summarization: Er-\\nrors, summarizers, datasets, error detectors. In Pro-\\nceedings of ACL .James Thorne, Andreas Vlachos, Christos\\nChristodoulopoulos, and Arpit Mittal. 2018.\\nFEVER: a large-scale dataset for fact extraction\\nand VERification. In Proceedings of the 2018\\nConference of the North American Chapter of\\nthe Association for Computational Linguistics:\\nHuman Language Technologies, Volume 1 (Long\\nPapers) , pages 809–819, New Orleans, Louisiana.\\nAssociation for Computational Linguistics.\\nKatherine Tian, Eric Mitchell, Huaxiu Yao, Christo-\\npher D Manning, and Chelsea Finn. 2024. Fine-\\ntuning language models for factuality. In The Twelfth\\nInternational Conference on Learning Representa-\\ntions .\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, et al. 2023. Llama 2: Open founda-\\ntion and fine-tuned chat models. arXiv preprint\\narXiv:2307.09288 .\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\\nChaumond, Clement Delangue, Anthony Moi, Pier-\\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\\net al. 2019. Huggingface’s transformers: State-of-\\nthe-art natural language processing. arXiv preprint\\narXiv:1910.03771 .\\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane\\nSuhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari\\nOstendorf, and Hannaneh Hajishirzi. 2023. Fine-\\ngrained human feedback gives better rewards for lan-\\nguage model training. In Thirty-seventh Conference', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 11}), Document(page_content='Ostendorf, and Hannaneh Hajishirzi. 2023. Fine-\\ngrained human feedback gives better rewards for lan-\\nguage model training. In Thirty-seventh Conference\\non Neural Information Processing Systems .\\nXi Ye and Greg Durrett. 2022. The unreliability of ex-\\nplanations in few-shot prompting for textual reason-\\ning. In Proceedings of the Conference on Advances\\nin Neural Information Processing Systems (NeurIPS) .\\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su,\\nand Huan Sun. 2023. Automatic evaluation of at-\\ntribution by large language models. arXiv preprint\\narXiv:2305.06311 .', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 11}), Document(page_content='A Details of Data Generation\\nRecall that we create the tuning data by first sam-\\npling responses from the base LLM and then using\\nthe NLI model to create citations and identify un-\\nsupported statements. We provide the details on\\nthe process in the following of this section.\\nNLI model We use TRUE NLI (Honovich et al.,\\n2022) as, to the best of our knowledge, it is the\\nstate-of-the-art NLI model for evaluating whether\\na passage supports a claim, and it is commonly\\nused in the recent line of work on attributed QA\\nand evaluating grounding (Bohnet et al., 2022; He\\net al., 2022; Li et al., 2023a).8TRUE is trained\\non data collected from 6 datasets of diverse tasks\\ncovering NLI, paraphrase detection, and fact verifi-\\ncation, which leads to its strong performance across\\ndiverse types of text. Furthermore, the citation per-\\nformance evaluated by TRUE highly aligns with\\nhuman evaluation (Gao et al., 2023b).\\nCorpus & retriever As mentioned before,\\nour framework is an instantiation of retrieval-\\naugmented framework. For the datasets using\\nWikipedia as the corpus (NQ, StrategyQA, ASQA,\\nand Qampari), we use the 2018-12-20 Wikipedia\\nsnapshot as the corpus and set up the retriever using\\nGTR-large (Ni et al., 2021).\\nTask: You will be given a question and some search results.\\nPlease answer the question in 3-5 sentences, and make sure\\nyou mention relevant details in the search results. You may\\nuse the same words as the search results when appropriate.\\nNote that some of the search results may not be relevant,\\nso you are not required to use all the search results, but\\nonly relevant ones.\\n<Question>\\nSearch Results:\\n[<Index>] <Title>\\n<Text>\\n[...]\\nAnswer:\\nFigure 5: Zero-shot prompt template for sampling initial\\nresponses from the base LLM.\\nSampling initial responses We sample initial\\nresponses from the base LLM using instruction fol-\\nlowing in a zero-shot fashion . Given a query, we\\npresent the base LLM with query and 5 retrieved\\n8https://huggingface.co/google/t5_xxl_true_\\nnli_mixturepassages appended after an instruction that requires\\nthe base LLM to answer the query based on the\\npassages; see Fig. 5 for the template of the zero-\\nshot prompt. We note that we opt to use a zero-\\nshot prompt as opposed to a task-specific few-shot\\nprompt since 1) this can avoid biasing the gener-\\nation with the few-shot in-context examples, and\\n2) this matches the expected scenario for deploy-\\ning the adapted LLM to handle new queries in a\\nzero-shot fashion.\\nFor text-bison-001 , we sample 4 responses using\\na temperature of 0.5. For llama-2-13b , we sample 4\\nresponses using nuclear sampling (Holtzman et al.,\\n2019) with p=0.95.\\nAdding citations and identifying unsupported\\nstatements After obtaining the initial response\\n{A}from the base LLM. We break each response\\nAinto sentences into s1, . . . , s i. For each si, we\\nfind the maximally supported passage ei(scored\\nbyϕ(ei, si)) that the base LLM has seen during\\ngenerating the initial responses. We link eitosiif\\nϕ(ei, si)>0.7to encourage more precise citations.\\nFor a sentence siif there does notexist an eisuch\\nthatϕ(ei, si)>0.5(the decision boundary for\\nentailment), we add sito the unsupported statement\\nsetU.\\nVerbalizing We show the template for verbaliz-\\ning the data used to tune the LLM in Fig. 6. As\\nshown in the figure, we verbalize the citations in\\nenclosed box brackets that are added at the end of\\nsentences (before periods) like [n], and verbalize\\nunsupported statements after the responses.\\nB Details of Experimental Setup\\nDetails of Finetuning For tuning, we use LORA\\ntuning (Hu et al., 2022) in experiments on both\\ntext-bison-001 and llama-2-13b . For bison , we use\\nAPI to perform tuning.9and follow all the default\\nhyper-parameters except for training steps. We set\\n10% data created as development data and choose\\nto use a training step of 1000 (chosen from 500,\\n1000, and 2000). For llama-2 , we use the hugging-\\nface transformers (Wolf et al., 2019) chat-version\\ncheckpoint.10We find the chat-version achieves', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 12}), Document(page_content='to use a training step of 1000 (chosen from 500,\\n1000, and 2000). For llama-2 , we use the hugging-\\nface transformers (Wolf et al., 2019) chat-version\\ncheckpoint.10We find the chat-version achieves\\nbetter performance than the base checkpoint in our\\npreliminary investigation. We set lora_r to be 32,\\nand only choose to use a learning rate of 1e-5 (cho-\\nsen from 1e-4 and 1e-5) using the development set.\\n9https://cloud.google.com/vertex-ai/docs/generative-\\nai/models/tune-text-models-supervised\\n10https://huggingface.co/meta-llama/Llama-2-13b-chat-hf', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 12}), Document(page_content='Input\\nTask: You will be given a question and some search results.\\nYou are required to perform the following steps.\\nFirst, please answer the question in 3-5 sentences, and\\nmake sure you mention relevant details in the search results.\\nYou may use the same words as the search results when\\nappropriate. Note that some of the search results may not\\nbe relevant, so you are not required to use all the search\\nresults, but only relevant ones. If you use the provided\\nsearch results in your answer, add [n]-style citations.\\nNext, review your response and find the unsupported sen-\\ntences that do not have citations.\\n<Question>\\nSearch Results:\\n[<Index>] <Title>\\n<Text>\\n[...]\\nOutput\\nAnswer: <Response with citations>\\nSentences Not Supported by Citations: <Unsupported\\nstatements>\\nFigure 6: Verbalization template for creating the training\\ndata for adapting the base LLM.\\nWe fine-tune llama-2 on two A100 (40GB) GPU\\nfor 4 epochs.\\nDetails of Decoding Our evaluation uses the\\nofficial implementation from ALCE (Gao et al.,\\n2023b), we use the same data split and prompt\\ntemplate from ALCE. We use temperature 0.25 for\\nevaluation on both bison and llama . We use one\\nsample for evaluation since adapted LLMs tend to\\ngenerate better-grounded response exhibiting less\\nvariation.\\nDetails of Distillation For distillation, we di-\\nrectly tune llama-2-13b using the data created with\\ntext-bison-001 . We also set lora_r to be 32, use a\\nlearning rate of 1e-5, and fine-tune for 4 epochs.\\nC Comparison to ICLC ITEon More\\nCapable LLMs\\nTable 5 compares A GREE using llama-2-13B as the\\nbase model against ICLC ITEon more capable mod-\\nels. We directly use the results from ALCE (Gao\\net al., 2023b). Our framework is able to substan-\\ntially shorten the gap between a small llama-2\\nmodel and much more capable LLMs.ASQA QAMPARI\\nem-rec rec pre rec-5 rec pre\\nBase model: llama-2-13b\\nAGREE W/OTTA 35.7 50.2 55.3 17.1 40.4 43.6\\nAGREE W/ TTA 39.4 64.0 66.8 17.9 51.4 53.4\\nBase model: llama-2-70b\\nICLC ITE 41.5 62.9 61.3 21.8 15.1 15.6\\nBase model: ChatGPT-0301\\nICLC ITE 40.4 73.6 72.5 20.8 20.5 20.9\\nTable 5: Comparing AGREE onllama-2-13B against\\nICLC ITEonllama-2-70B and ChatGPT-0301 . We di-\\nrectly quote results from ALCE.\\nD License of Datasets\\nThe licenses datasets used in our work include:\\n•NQ (Kwiatkowski et al., 2019) under Creative\\nCommons Share-Alike 3.0 license.\\n•StrategyQA (Geva et al., 2021) under MIT\\nLicense.\\n•Fever (Thorne et al., 2018) under Creative\\nCommons Share-Alike license.\\n•Ambiguous QA (Stelmakh et al., 2022) under\\nCreative Commons Share-Alike 3.0 license.\\n•Qampari (Amouyal et al., 2022) under Cre-\\native Commons Zero v1.0 Universal license.\\nE Additional Examples of Tuning Data\\nPlease see Fig. 7 and Fig. 8 for concrete examples\\nof tuning data.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 13}), Document(page_content='Input\\nTask: You will be given a question and some search results.\\nYou are required to perform the following steps.\\nFirst, please answer the question in 3-5 sentences, and\\nmake sure you mention relevant details in the search results.\\nYou may use the same words as the search results when\\nappropriate. Note that some of the search results may not\\nbe relevant, so you are not required to use all the search\\nresults, but only relevant ones. If you use the provided\\nsearch results in your answer, add [n]-style citations.\\nNext, review your response and find the unsupported sen-\\ntences that do not have citations.\\nQuestion:\\nwhat kind of sentence contains an independent clause and\\na dependent clause?\\nSearch Results:\\n[1]Dependent clause\\n... A \"complex sentence\" contains an independent clause\\nand at least one dependent clause. A sentence with two or\\nmore independent clauses plus (one or more) dependent\\nclauses is referred to as a \"compound-complex sentence\".\\n(Every clause contains a subject and predicate.)...\\n[2]Sentence clause structure\\n... A complex sentence has one or more dependent clauses\\n(also called subordinate clauses). Since a dependent clause\\ncannot stand on its own as a sentence, complex sentences\\nmust also have at least one independent clause...\\n[3]Sentence clause structure\\nA set of words with no independent clause may be an\\n\"incomplete sentence\", also called a \"sentence fragment\".\\nA sentence consisting of at least one dependent clause and\\nat least two independent clauses may be called a \"complex-\\ncompound sentence\" or \"compound-complex sentence\".\\nSentence 1 is an example of a simple sentence...\\n[4]Topic sentence\\nUsing a complex sentence is a great way to refer to the con-\\ntent of the paragraph above (dependent clause) and then\\nbring in the content of the new paragraph (the independent\\nclause). Here is a typical example: The beginning, depen-\\ndent, clause probably refers to the content of a preceding\\nparagraph that presented the ant as a community-focused\\nworker...\\n[5]Sentence clause structure\\nThis simple sentence has one independent clause which\\ncontains one subject, \"I\", and one verb, \"run\". This simple\\nsentence has one independent clause which contains one\\nsubject, \"girl\", and one predicate, \"ran into her bedroom\"...\\nOutput\\nAnswer:\\nA complex sentence contains an independent clause and at\\nleast one dependent clause [1]. A dependent clause cannot\\nstand on its own as a sentence, so complex sentences must\\nalso have at least one independent clause [2].\\nSentences Not Supported by Citations:\\nNone.\\nFigure 7: An example of tuning data created from NQ.Input\\nTask: You will be given a question and some search results.\\nYou are required to perform the following steps.\\nFirst, please answer the question in 3-5 sentences, and\\nmake sure you mention relevant details in the search results.\\nYou may use the same words as the search results when\\nappropriate. Note that some of the search results may not\\nbe relevant, so you are not required to use all the search\\nresults, but only relevant ones. If you use the provided\\nsearch results in your answer, add [n]-style citations.\\nNext, review your response and find the unsupported sen-\\ntences that do not have citations.\\nQuestion:\\nCould Robert Wadlow hypothetically see Frankenstein’s\\nmonster’s bald spot from above?\\nSearch Results:\\n[1]Frankenstein’s monster\\nIn this version, Frankenstein gives the monster the brain\\nof his mentor, Doctor Waldman, while his body is made\\nfrom a man who killed Waldman while resisting a vacci-\\nnation...In the 2004 film \"Van Helsing ,\" the monster is\\nshown in a modernized version of the Karloff design. He\\nis tall, has a square bald head, gruesome scars, and pale\\ngreen skin...\\n[2]Rorschach (character)\\nRorschach is 5’6\"tall and weighs 140 pounds, and, as\\nWalter Kovacs (his\"disguise\"), he appears as a red-haired,\\nexpressionless, man who always carries with him a sign\\nthat reads \"THE END IS NIGH\"...\\n[3]Glenn Strange\\nDuring the wave of monster-related merchandising in the', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 14}), Document(page_content='expressionless, man who always carries with him a sign\\nthat reads \"THE END IS NIGH\"...\\n[3]Glenn Strange\\nDuring the wave of monster-related merchandising in the\\nlate 1950s and 1960s, Glenn Strange’s iconic image often\\nwas used for the monster on toys, games, and parapher-\\nnalia, most often from his appearance in the Abbott and\\nCostello film...\\n[4]John Bloom (actor)\\nJohn Bloom was an American actor. Career. Standing\\nat 7’ 4\"(224 cm) tall, he is most famous for his roles as\\nthe Frankenstein Monster in the low-budget horror movie\"\\nDracula vs. Frankenstein \"(he remains the tallest actor to\\never portray the character)\"s...\\n[5]Model figure\\nThere were also cheap plastic models for the popular mar-\\nket of movie monsters, comic book heroes, and movie and\\ntelevision characters in 1:8 size (about 9 inches or 23 cm\\nin height)...\\nOutput\\nAnswer:\\nYes, Robert Wadlow could hypothetically see Franken-\\nstein’s monster’s bald spot from above. Frankenstein’s\\nmonster is described as having a square bald head [1].\\nRobert Wadlow was 8 feet 11 inches tall. Frankenstein’s\\nmonster was portrayed by John Bloom, who was 7 feet 4\\ninches tall [4].\\nSentences Not Supported by Citations:\\nRobert Wadlow was 8 feet 11 inches tall.\\nFigure 8: An example of tuning data created from Strat-\\negyQA.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/effective LLM adaption for improved grounding and citation generation.pdf', 'page': 14}), Document(page_content='Citation-Enhanced Generation for LLM-based Chatbots\\nWeitao Li, Junkai Li, Weizhi Ma, Yang Liu\\nDept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China\\nliwt23@mails.tsinghua.edu.cn ,liuyang2011@tsinghua.edu.cn\\nAbstract\\nLarge language models (LLMs) exhibit pow-\\nerful general intelligence across diverse sce-\\nnarios, including their integration into chat-\\nbots. However, a vital challenge of LLM-\\nbased chatbots is that they may produce hal-\\nlucinated content in responses, which signif-\\nicantly limits their applicability. Various ef-\\nforts have been made to alleviate hallucination,\\nsuch as retrieval augmented generation and re-\\ninforcement learning with human feedback, but\\nmost of them require additional training and\\ndata annotation. In this paper, we propose a\\nnovel post-hoc Citation- Enhanced Generation\\n(CEG ) approach combined with retrieval ar-\\ngumentation. Unlike previous studies that fo-\\ncus on preventing hallucinations during gen-\\neration, our method addresses this issue in a\\npost-hoc way. It incorporates a retrieval mod-\\nule to search for supporting documents relevant\\nto the generated content, and employs a natural\\nlanguage inference-based citation generation\\nmodule. Once the statements in the generated\\ncontent lack of reference, our model can re-\\ngenerate responses until all statements are sup-\\nported by citations. Note that our method is a\\ntraining-free plug-and-play plugin that is capa-\\nble of various LLMs. Experiments on various\\nhallucination-related datasets show our frame-\\nwork outperforms state-of-the-art methods in\\nboth hallucination detection and response re-\\ngeneration on three benchmarks. Our codes\\nand dataset will be publicly available.\\n1 Introduction\\nLarge Language Models (LLMs) have experienced\\nrapid development in recent years, which show\\npowerful general intelligence in various scenar-\\nios (Yue et al., 2023; Singhal et al., 2023). Current\\nLLM-based chatbots, epitomized by ChatGPT and\\nGPT-4, demonstrate impressive capabilities across\\ndistinct domains in communicating with humans.\\nThere is a growing consensus that LLM-based chat-\\nbots can be the next generation of information ac-\\nquisition methodology.\\nFigure 1: An illustration of our method, which adds\\ncitations for the generated content. If there are hallu-\\ncinations in the generated content, we prompt LLM to\\nregenerate a new response.\\nHowever, a critical and unsolved challenge of\\nLLM-based chatbots is the hallucination prob-\\nlem (Ji et al., 2023), which indicates these chatbots\\nmay generate hallucinated content in responses ran-\\ndomly. As the underlying mechanisms of hallucina-\\ntions remain unclear, this problem has substantial\\nconstraints on the deployment of LLM-based chat-\\nbots in various sensitive scenarios, such as health-\\ncare and education, where reliability is paramount.\\nPrevious approaches have attempted to mit-\\nigate this issue through retrieval augmenta-\\ntion (Borgeaud et al., 2022; Izacard et al., 2022) and\\nvalue alignment (RLHF) (Ouyang et al., 2022; Tou-\\nvron et al., 2023) in response generation, but these\\noften require additional training and extensive data\\nannotation. For example, InstructGPT (Ouyang\\net al., 2022) utilize RLHF to alleviate hallucina-arXiv:2402.16063v3  [cs.CL]  4 Mar 2024', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 0}), Document(page_content='tions in model output, but needs extra training.\\nGao et al. (2023a) attempt to reduce hallucination\\nthrough adding retrieved related documents and\\ncitations before generation, while the pre-hoc way\\nof incorporating citations may potentially harm the\\nmodel performance, resulting in poor response re-\\nsults with hallucinations.\\nIn this work, we propose a novel method to al-\\nleviate hallucination in LLMs, which leverages re-\\ntrieval augmentation and Natural Language Infer-\\nence (NLI) technologies to implement Citation-\\nEnhanced Generation (CEG) in a post-hoc way.\\nFigure 1 is an illustration. Differing from previous\\nstudies, the retrieval augmentation module of the\\nCEG framework works after generation (post-hoc),\\nand CEG prompts the model to regenerate the an-\\nswer when necessary. This approach is effective\\nand easy to use, which can reduce the hallucination\\nin the model’s output for various LLMs. We con-\\nduct experiments on distinct hallucination-related\\nbenchmarks, including detection and response re-\\ngeneration, where our method achieved state-of-\\nthe-art performance. Further analyses demonstrate\\nthe usefulness of each module on CEG.\\nIn summary, the main contributions of our work\\ncan be summarized as follows:\\n•We are the first to propose the use of citation\\nto alleviate hallucination in a post-hoc way\\nwith regeneration.\\n•We design a novel post-hoc citation-enhanced\\ngeneration framework combined with retrieval\\naugmentation and NLI to avoid hallucinations,\\nwhich is flexible for existing LLMs.\\n•Experimental results show that our CEG\\nframework achieves the best performance on\\nthree hallucination-related benchmarks.\\n2 Related Work\\n2.1 Hallucination Control in LLMs\\nGenerative AI has achieved significant advance-\\nments, while still facing the hallucination problem.\\nExisting strategies can be categorized into major\\ntwo types: mitigation during training and mitiga-\\ntion during inference. For the first type, LLMs,\\nsuch as LLaMA 2 (Touvron et al., 2023), undergo\\nextensive training cycles with high-fidelity data\\nsources like Wikipedia to bolster factual consis-\\ntency in pre-training. Zhou et al. (2023) alleviate\\nhallucination during instruction fine-tuning, whichadopts high quality manually annotated content to\\nregulate hallucination. Some studies (Ouyang et al.,\\n2022; Touvron et al., 2023) also introduce penalties\\nfor nonfactual responses to alleviate hallucination\\nin RLHF. However, all these methods need extra\\ntraining and annotations.\\nOn the other hand, researchers try to deal\\nwith the hallucination challenge during inference.\\nInference-Time-Intervention (Li et al., 2023b)\\nmitigates hallucination by shifting model acti-\\nvations along these factuality-related directions\\nduring inference. Retrieval-Augmented Genera-\\ntion (RAG) (Lewis et al., 2020) has become a preva-\\nlent technique in alleviating hallucination by re-\\ntrieving reliable documents before generation (Yu,\\n2022). While these methods still generate halluci-\\nnations due to the lack of post-hoc verification and\\nthey are unable to provide citations for verification.\\n2.2 Citation Augmented LLMs\\nIn the realm of LLMs, retrieval technology has be-\\ncome a crucial component (Zhang et al., 2023b;\\nGao et al., 2023b), as it provides related knowledge\\nin generating more reliable results (also mitigates\\nthe occurrence of hallucinations). Previous studies\\npoint out that citation, generated by retrieval mod-\\nels, is key to building responsible and accountable\\nLLMs (Huang and Chang, 2023).\\nExisting citation augmented strategies can be\\ndivided into two types: parametric and non-\\nparametric. Parametric methods (Taylor et al.,\\n2022) refer to information internalized from the\\ntraining data, often leading to inaccurate annotated\\ndocuments, as the annotation process itself can\\ngive rise to hallucinations. Non-parametric meth-\\nods (Gao et al., 2023a; Menick et al., 2022; Izac-\\nard and Grave, 2021) involve querying relevant in-\\nformation and seamlessly integrating the retrieved', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 1}), Document(page_content='ods (Gao et al., 2023a; Menick et al., 2022; Izac-\\nard and Grave, 2021) involve querying relevant in-\\nformation and seamlessly integrating the retrieved\\ncontent from outside corpus, which provides more\\nreliable citations. Thus, most previous studies are\\nnon-parametric, but they are pre-hoc based. For ex-\\nample, Gao et al. (2023a) adopt retrieval processes\\nto facilitate the annotation of documents within\\nmodel-generated outputs. Nevertheless, their pre-\\nhoc annotation strategy inadvertently escalates the\\ncomplexity of a QA task by converting it into a dual\\nchallenge of generating a response coupled with\\nsimultaneous annotation. Different from existing\\ncitation augmented studies, we propose a differ-\\nent strategy to utilize retrieval models to generate\\ncitations in a post-hoc way.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 1}), Document(page_content='Figure 2: An overview of our CEG framework. [R1] and [R2] denote segments. [D1] and [D2] represent retrieved\\ndocuments for each segment. [L1] and [L2] are labels (Factual/Nonfactual) generated by the NLI module.\\n3 Approach\\n3.1 Overview\\nFirstly, we give an overview of our proposed CEG\\nframework. Illustrated in Figure 2, CEG has sev-\\neral critical modules: 1) Retrieval augmentation\\nmodule, designed to search for documents Djrel-\\nevant to the original response R. In cases where\\nresponses are excessively lengthy, they can be bro-\\nken down into sub-claims R={R1, R2, ..., R n}.\\n2) Citation generation module, which assesses if\\nthe retrieved documents Djsubstantiate the { Ri}\\nin response or not. 3) Regeneration module, tasked\\nwith creating a new prompt that integrates the orig-\\ninal user query and key retrieved information for\\nthe LLM Mto get a more reliable response R′.\\nIt is important to note that our method is a post-\\nhoc framework and is highly adaptable across dif-\\nferent LLMs, as it does not require any additionaltraining or fine-tuning. Consequently, we do not\\nspecify a particular LLM here.\\n3.2 Retrieval Augmentation Module\\nRetrieval augmentation has been shown to have\\npowerful abilities in previous hallucination-related\\nstudies (Gao et al., 2023a; Zhao et al., 2023). Dif-\\nferent from these studies that aim to retrieve doc-\\numents as evidence before response generation\\n(questions are queries), we propose to conduct re-\\ntrieval augmentation in a post-hoc way to verify\\nthe correctness of the generated claim Ri(claims\\nare queries). As there are various existing studies\\non how to retrieve the most related document, we\\nuse a simple but effective dense retrieval strategy\\nto verify the performance of our CEG framework,\\nand we believe stronger retrieval will bring further\\nimprovements.\\nQuery : For the response R, it will be seg-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 2}), Document(page_content='mented into several claims if necessary, resulting\\ninR={R1, R2, ..., R n}. Here, we adhere to\\nprevious work (Chen et al., 2023) and employ a\\nheuristic algorithm for segmentation. Utilizing the\\nNLTK (Bird et al., 2009) sentence tokenizer, we\\nsplitRto obtain reasonable results that align with\\nuser reading habits to get the claims Ri. Then, Ri\\nis adopted as the query one by one.\\nCorpus (Candidate Documents) : The choice of\\nCorpus decides the scope of applications, and there\\nare multiple candidates. In this study, our focus lies\\npredominantly in the domain of knowledge-based\\nquestion answering, necessitating the employment\\nof a curated corpus. To this end, we leverage a\\nprocessed snapshot of Wikipedia from October 20,\\n20231, segmented into approximately 100-word\\ncandidate documents, each demarcated by a period\\nor newline character. Note that you can replace\\nit with any other corpus, and we use it as most\\nhallucination benchmarks are based on Wikipedia.\\nRetriever : Dense vector based retrieval tech-\\nnologies have demonstrated powerful perfor-\\nmances in recent years, which are also widely used\\nin existing RAG models. Here, we adopt the Sim-\\nCSE BERT2(Gao et al., 2021; Wang et al., 2023d)\\nas the query and document encoder for its promis-\\ning efficiency in previous studies (Wang et al.,\\n2023c,d). Then, candidate documents are ranked\\nbased on cosine similarity scores calculated by the\\nfollowing equation:\\nSim(Ri, dj) =e(Ri)·e(dj)\\n∥e(Ri)∥ · ∥e(dj)∥,\\nWhere e()is the SimCSE BERT encoder, djis a\\ncandidate document in the corpus. As more doc-\\numents need more calculation in further modules,\\nthe top- kretrieved documents with higher similar-\\nity are selected to form the reference document set\\nDi={di,1, di,2, ..., d i,k}. We add an extra thresh-\\noldtto filter out the retrieved documents that have\\nlow cosine similarity. Apart from the top- 1docu-\\nment, if the Sim( Ri, dj) <t,djwill not be included\\ninDi. These documents are subsequently concate-\\nnated to construct the final retrieved content Difor\\nfurther calculation.\\n3.3 Citation Generation Module\\nAfter getting the reference document Difor each\\nresponse segment Ri, the next step involves gen-\\n1https://dumps.wikimedia.org/enwiki/\\n2https://huggingface.co/princeton-nlp/sup-simcse-bert-\\nbase-uncased/tree/mainerating labels and citations to verify the correct-\\nness of Ri. We propose to adopt an NLI method\\nto determine the relationship between each claim-\\ndocument pair ( Ri,Di). In general, the relationship\\ncan be categorized into three types: support, inde-\\npendence, and contradiction. But in hallucination-\\nrelated scenarios, to adhere to previous studies, we\\nonly utilize two categories: (1) Factual , where Di\\nserves as a reference for Ri, thereby substantiat-\\ning the claim. (2) Nonfactual , which means Di\\npresents a opposite claim to Ri.\\nAlthough numerous models (Honovich et al.,\\n2022; Raffel et al., 2020) are capable of the NLI\\nmethod, our CEG framework seeks to fully lever-\\nage the language comprehension capabilities of\\nLLMs. Therefore, we prefer to utilize an LLM,\\nequipped with predefined prompts, to serve as the\\nNLI method. An illustrative prompt is provided\\nbelow:\\nInstruction : I will show you a question, a re-\\nsponse segment of this question, and a reference\\ndocument. Your task is to assess whether the\\ngiven response segment contains factual errors or\\nnot with the help of the reference document. ...\\nWhen the LLM output indicates “factual”, the\\ndocument Diis identified as a valid reference for\\nthe claim Ri. Consequently, this citation can be\\nadded to the original response. If none of the re-\\ntrieved top-k documents substantiate the claim Ri\\nor if there are documents opposing the claim, we\\nwill label this claim as nonfactual (potential hallu-\\ncination) to remind users to keep carefully reading.\\nBased on the introduced two modules, we can de-\\ntect whether there are hallucinations in responses.\\n3.4 Response Regeneration Module\\nIn previous modules, our framework offers a post-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 3}), Document(page_content='Based on the introduced two modules, we can de-\\ntect whether there are hallucinations in responses.\\n3.4 Response Regeneration Module\\nIn previous modules, our framework offers a post-\\nhoc method to conduct citation-enhanced verifica-\\ntion for responses, where reliable responses are\\nincorporated with citations. However, a new chal-\\nlenge is how to deal with potential hallucinations.\\nSo we propose a response regeneration module.\\nAssuming LLM Mgenerates the original re-\\nsponse R, our framework will provide a new\\nprompt for regeneration. The prompt not only con-\\ntains the original query, but is also incorporated\\nwith retrieved documents and the annotated nonfac-\\ntual segments. Here we provide a brief illustration\\nof the prompt (a full prompt is shown in appendix):', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 3}), Document(page_content='User : Question; Chatbot : Response; User : [There\\nwere some factual errors in your output: ( Nonfac-\\ntual Claims ). Please generate a new response based\\non relevant docs: ( Relevant Docs ).]\\nUpon receipt of the regenerated response, we can\\ninitiate a new citation-enhanced generation process.\\nIf the response is adjudged to be free of factual\\nerrors, it becomes the final response and will be\\nshown to users. However, if the new response still\\ncontains hallucinations, the regeneration cycle will\\nbe repeated. To conserve API resources and reduce\\nthe waiting time, a predefined parameter Tcan be\\nset to constrain the max regeneration attempts.\\n4 Experimental Settings\\n4.1 Overview\\nTo verify the effectiveness of our framework, we\\nadopt four hallucination-related datasets in our ex-\\nperiments: WikiBio GPT-3 (Manakul et al., 2023),\\nFELM (Chen et al., 2023), HaluEval (Li et al.,\\n2023a), and WikiRetr. WikiBio GPT-3 and FELM\\nare hallucination detection benchmarks. HaluEval\\nis a hallucination generation benchmark. Besides,\\nwe construct a new dataset named WikiRetr, which\\nis to evaluate the retrieval and citation annotation\\nperformance. Due to the tasks and baselines are\\ndistinct in various datasets, we will introduce each\\ndataset and corresponding settings one by one.\\nWe use GPT models as the LLM backbones, and\\nthe version involved in different datasets is distinct\\nfor fair comparison with existing baselines. Unless\\notherwise specified, “ChatGPT” refers to GPT-3.5-\\nTurbo-1106, and “GPT-4” refers to GPT-4-0613.\\nWe set the decoding temperature as 0 to maintain\\nthe reproducibility of the responses generated by\\nLLMs. All prompts are listed in Appendix A, and\\nmore dataset information is shown in Appendix B.\\n4.2 WikiBio GPT-3 Dataset\\nWikiBio GPT-3 dataset is constructed to evalu-\\nate the hallucination of LLMs. Researchers ran-\\ndomly select 238 biographical articles from Wik-\\niBio dataset (Lebret et al., 2016), and utilize the\\ntext-davinci-003 to generate new passages. The\\npassages are split into 1,908 sentences, and then\\nmanually annotated into three categories: Major In-\\naccurate ,Minor Inaccurate , and Accurate . Follow-\\ning previous studies, Major Inaccurate andMinor\\nInaccurate are categorized as Nonfactual (poten-tially with hallucinations, 1,392 segments), and\\nAccurate is treated as Factual (516 segments).\\nBaselines : 1) HalluDetector (Wang et al., 2023b)\\nutilizes external knowledge sources, a specific\\nclassification model and a Naive Bayes classifier\\nto detect hallucination. 2) Focus (Zhang et al.,\\n2023a) adopts a multi-stage decision-making pro-\\ncess, where both pre-retrieval and task specific\\nclassifiers are adopted. 3) SelfCheckGPT3, three\\nvariants of which are utilized: w/BERTScore,\\nw/Prompt, and w/NLI (Manakul et al., 2023). Self-\\nCheckGPT w/BERTScore is based on the inher-\\nent uncertainty of LLM, while SelfCheckGPT\\nw/Prompt and w/NLI draw upon external knowl-\\nedge sources. The Area Under the Precision-\\nRecall Curve (AUC-PR) and Balanced_Accuracy\\nare adopted as evaluation metrics.\\n4.3 FELM Dataset\\nFELM dataset is designed to evaluate hallucination\\ndetection ability. Researchers assemble prompts\\nfrom diverse scenarios, and use them to instruct\\nGPT-3.5-Turbo-0301 to generate responses. These\\nresponses are manually annotated as nonfactual and\\nfactual, along with supporting documents. Our ex-\\nperiments are conducted on the WorldKnowledge\\nsubset of FELM as it is based on Wikipedia corpus.\\nBaselines : Following settings in FELM, we\\nadopt four strategies with ChatGPT, GPT-4, and\\nVicuna-33B as the backbone LLM (Zheng et al.,\\n2023): 1) Vanilla prompts. 2) Prompts augmented\\nwith Chain-of-Thought (CoT) reasoning (Kojima\\net al., 2022). 3) Prompts augmented with hy-\\nperlinks to reference documents and 4) Prompts\\naugmented with human-annotated reference doc-\\numents (Chen et al., 2023). Experiments are con-\\nducted at the individual response level. The metrics\\nare accuracy of nonfactual, factual, and balanced.\\n4.4 HaluEval Dataset', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 4}), Document(page_content='uments (Chen et al., 2023). Experiments are con-\\nducted at the individual response level. The metrics\\nare accuracy of nonfactual, factual, and balanced.\\n4.4 HaluEval Dataset\\nHaluEval dataset is a benchmark for assessing the\\nability of LLMs to discern hallucinations. Each in-\\nstance comprises a question, a correct answer, and\\na hallucinated answer (multiple answers are auto-\\nmatically generated, and the most confusing one is\\nselected by ChatGPT). The QA subset of HaluEval\\nis adopted as it is constructed by Wikipedia corpus,\\nand 2,000 samples of which are randomly sampled.\\nBaselines : We adopt several models building\\nupon the updated version of ChatGPT as previ-\\n3The latest version in https://arxiv.org/pdf/2303.08896.pdf.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 4}), Document(page_content='MethodAUC-PR (%)Balanced_Acc (%)Nonfactual Factual\\nHalluDetectorCM= 14,CFA= 24 82.42 57.01 70.54\\nCM= 28,CFA= 96 86.45 61.96 74.82\\nFocusLLaMA −30Bfocus 89.79 65.69 73.64\\nLLaMA −65Bfocus 89.94 64.90 74.08\\nSelfCheckGPTw/BERTScore 81.96 44.23 59.31\\nw/NLI 92.50 58.47 70.55\\nw/Prompt 91.16 68.37 72.64\\nCEG top- k=6 92.31 70.24 77.59\\nTable 1: Experimental results of our method powered by GPT-3.5-Turbo-Instruct on WikiBio GPT-3. The Self-\\nCheckGPT with Prompt is also powered by GPT-3.5-Turbo-Instruct because GPT-3.5-Turbo-0613 is deprecated.\\nous studies (Li et al., 2023a): 1) Vanilla prompts.\\n2) Prompts augmented with CoT reasoning. 3)\\nPrompts with Pre-RAG, where a strong and fine-\\ntuned retriever, All-mpnet-base-v24, is used. Accu-\\nracy is chosen as the evaluation metric.\\n4.5 WikiRetr Datasets\\nWikiRetr datasets are designed to conduct further\\nanalyses on our CEG framework, which is cre-\\nated based on the October 20, 2023 snapshot of\\nWikipedia. We randomly select 1,000 passages,\\nand apply text-davinci-003 and GPT-4 to rewrite\\nthem as new claims, separately. So that each\\nrewritten claim is accompanied by an original pas-\\nsage. The constructed datasets are named WikiRetr-\\nGPT3 and WikiRetr-GPT4, respectively.\\nTo analyze the retrieval module, we utilize\\nvarious retrievers, including: 1) SimCSE BERT,\\nwhich is employed in our CEG framework; 2)\\nSentence BERT (Reimers and Gurevych, 2019),\\na retriever trained with siamese networks; and 3)\\nAll-mpnet-base-v2. Recall@ kis the metric to ver-\\nify if the original document is retrieved in top- k.\\nPrecision@ kis the metric to verify if the claim is\\nsupported by a doc in top- kdocs with NLI method.\\nFor NLI methods in the citation generation mod-\\nule, we randomly select 100 instances from each\\ndataset for evaluation. We conduct manual anno-\\ntation to assess whether the original passages can\\nsupport the rewritten claims by three annotators.\\nLabeling results show that 90% and 94% gener-\\nated claims are supported by original documents,\\nwhich is the ground truth of NLI models. Then, we\\nuse 1) True-9B5(Honovich et al., 2022) model and\\n4https://huggingface.co/sentence-transformers/all-mpnet-\\nbase-v2.\\n5https://huggingface.co/google/t5_xxl_true_nli_mixture.2) GPT models as NLI methods in analyses. We\\nchoose the consistency between manual and model\\nannotations as the evaluation metric.\\n5 Experimental Results and Analyses\\n5.1 Performance on Hallucination Detection\\nTo verify the effectiveness of our method, we utilize\\nour retrieval augmentation and citation generation\\nmodules for hallucination detection on WikiBio\\nGPT-3 and FELM datasets.\\nOverall performances in WikiBio GPT-3 dataset\\nare shown in Table 1, and we have the following ob-\\nservations: 1) Our CEG framework outperforms all\\nbaseline methods in Balanced_ACC and AUC-PR\\nof Factual segments, achieving the second perfor-\\nmance in AUC-PR of nonfactual segments. These\\nresults indicate the strength of our method. 2) Pre-\\nvious pre-retrieval augmented generation models,\\nSelfCheckGPT w/NLI and w/prompt, also get good\\nperformances in AUC-PR. While suffering from\\nthe lengthy text, they cannot achieve comparable\\nperformance of our model in all metrics. 3) Our\\nmodel performs slightly worse than w/NLI in the\\nAUC-PR of nonfactual segments, the reason can be\\nthe NLI module of SeftCheckGPT undergoes ad-\\nditional training on detecting nonfactual segments\\n(but achieve poor results in factual).\\nExperimental results in FELM dataset are sum-\\nmarized in Table 2. Firstly, our model achieves the\\nbest result in balanced accuracy with GPT-4, indi-\\ncating its effectiveness. Most baseline models are\\nbiased in classifying a single type. Then, CEG with\\nChatGPT beats other ChatGPT baselines, showing\\nthe flexibility of our model. Thirdly, CEG outper-\\nforms all pre-retrieval baselines, which shows the\\nstrength of the proposed post-hoc segment-level\\nretrieval module in hallucination detection.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 5}), Document(page_content='MethodAccuracy (%)\\nNonfact Factual Balanced\\nVanillaVicuna-33B 72.8 34.0 53.4\\nChatGPT 3.4 96.1 49.8\\nGPT-4 21.8 95.3 58.5\\nCoTVicuna-33B 40.8 62.3 51.6\\nChatGPT 2.7 96.9 49.8\\nGPT-4 42.9 94.0 68.4\\nLinkVicuna-33B 70.7 29.9 50.3\\nChatGPT 11.6 94.3 52.9\\nGPT-4 35.4 93.2 64.3\\nDocVicuna-33B 81.6 17.9 49.8\\nChatGPT 34.7 73.2 54.0\\nGPT-4 88.3 40.8 64.6\\nCEGChatGPT 40.1 79.2 59.7\\nGPT-4 54.4 85.5 69.9\\nTable 2: Experimental results of our method powered by\\nChatGPT and GPT-4 on FELM worldknowledge subset.\\nBaselines use the same GPT versions as our CEG, so\\nthe performances may vary from their original papers.\\nTo summarize, CEG outperforms various SOTA\\nbaselines in two benchmarks with distinct LLM\\nbackbones, indicating that post-hoc retrieval with\\nNLI is powerful for hallucination detection.\\n5.2 Results on Hallucination Regeneration\\nMethod Accuracy (%)\\nBaselinesVanilla 63.40\\nw/CoT 68.55\\nw/Pre-Retrieval 61.35\\nCEGw/ChatGPT 69.00\\nw/GPT-3.5-Turbo-Instruct 69.45\\nTable 3: Experimental results powered by ChatGPT on\\nthe HaluEval QA subset. We employ two GPT models\\nas the NLI method in our citation generation module.\\nOn the HaluEval dataset, we adopt the full frame-\\nwork of CEG to further evaluate the regeneration\\nmodule. If the doc is helpful in solving the problem\\nand any of the response segments are classified into\\nnonfactual, our method will generate a new prompt\\nfor regeneration as introduced in Section 3.4. Be-\\nsides, a 2018 Wikipedia snapshot is adopted as the\\ncorpus (Gao et al., 2023a) in this experiment due\\nto the inconsistency between this dataset and the\\nprevious corpus.\\nExperimental results are presented in Table 3.\\nFirstly, our CEG framework with GPT-3.5-Turbo-\\nInstruct achieves the best performance (69.45% in\\naccuracy), which achieves 8.10% improvements\\ncompared to the pre-hoc retrieval strategy. CEG\\nwith ChatGPT also outperforms the pre-retrievalstrategy, so these results demonstrate our post-hoc\\nmethod is robust. Secondly, pre-hoc retrieval strat-\\negy even performs worse than the baseline with\\nCoT (Li et al., 2023a), which indicates the retrieved\\ndocuments are not always helpful. Thirdly, consis-\\ntent with our experiments related to NLI models in\\nTable 5, when using GPT-3.5-Turbo-Instruct as the\\nNLI model, the results are superior to ChatGPT. We\\nalso conduct case studies to show our regeneration\\nresults, and some cases are shown in Appendix C.\\n5.3 Further Analyses\\n5.3.1 Ablation Study\\nVariantsAccuracy (%)\\nNonfact Factual Balanced\\nChatGPTw/o RA 17.7 90.1 53.9\\nw/o Threshold 40.1 78.2 59.2\\nALL 40.1 79.2 59.7\\nGPT-4w/o RA 30.6 93.3 61.9\\nw/o Threshold 50.3 83.6 67.0\\nALL 54.4 85.5 69.9\\nTable 4: Ablation results of CEG on the subset of FELM.\\n‘RA’ means the Retrieval Augmentation module.\\nWe conduct ablation experiments on the FELM\\ndataset, where the retrieval augmentation ( k= 4)\\nand the document selection threshold are involved\\n(threshold = 0.5). As shown in Table 4, the re-\\ntrieval augmentation module plays an important\\nrole in providing better results on different back-\\nbone LLMs (ChatGPT and GPT-4). Furthermore,\\nthe threshold of retrieved documents is necessary,\\nwhich can filter out irrelevant documents in citation\\ngeneration. Thus, all designed modules contribute\\nto improvements in the CEG framework.\\n5.3.2 Retrieval Models\\nFigure 3: Performances of different retrievers on the\\nWikiRetr-GPT3 dataset.\\nThe choice of retrieval model significantly impacts\\nthe performance of our retrieval augmentation mod-', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 6}), Document(page_content='ule, as illustrated in Figure 3. Experimental results\\nof three different retrieval models show that Sim-\\nCSE BERT has better performances in post-hoc re-\\ntrieval tasks (76.8% when using top 10 documents),\\nwhere 64 million documents are used as candidates.\\nBesides, although a larger value of kcan improve\\nrecall, it also requires more resources for further\\ncomputation. For a balance between efficiency and\\neffectiveness, the value of k is set between 4 and 6\\nin our experiments.\\n5.3.3 NLI Models in Citation Generation\\nWikiRetr-GPT3 WikiRetr-GPT4\\nTrue-9B 84 84\\nChatGPT 66 77\\nGPT-3.5-Turbo-Instruct 86 91\\nGPT-4 Turbo 83 90\\nGPT-4 83 96\\nTable 5: Agreement rate (%) of different NLI models\\nwith human annotated instances on WikiRetr datasets.\\nThe performance of different NLI models in the\\ncitation generation module is illustrated in Table 5,\\nand there are two main observations we can make:\\n1) Despite being a state-of-the-art task-specific NLI\\napproach, True-9B performs worse than best LLMs\\nin this scenario. LLMs, such as GPT-3.5-Turbo-\\nInstruct and GPT-4, are capable of playing the NLI\\nmodel in our citation generation module, as they\\nachieve high agreement rates with human annota-\\ntors. 2) LLMs show better performance on data\\nthey generate, which is consistent with previous\\nstudies (Wang et al., 2023a; Zheng et al., 2023).\\nMetric Precision (%) Recall (%)\\nNLI model True GPT-4 -\\nTop-k 5 10 5 10 5 10\\nWikiRetr-GPT3 71.2 74.2 75.2 78.2 73.6 76.8\\nWikiRetr-GPT4 58.1 62.6 69.7 75.7 57.0 60.9\\nTable 6: NLI Precision of True-9B and GPT-4 on\\nWikiRetr Datasets.\\nTable 6 shows experimental results of the cita-\\ntion generation module with distinct NLI models\\nwhen the retriever is Simcse BERT, which indi-\\ncate: 1) Even on the corpus with over 64 million\\ncandidates, our citation generation module exhibits\\noutstanding performance, achieving 78.2 and 75.7\\non the precision of the two datasets, respectively.\\n2) Compared to WikiRetr-GPT3, WikiRetr-GPT4\\nconstitutes a more challenging and higher-quality\\ndataset. The reason is that WikiRetr-GPT4 demon-\\nstrates lower recall, suggesting a lower semanticsimilarity between the original text and the gen-\\nerated claim. While its precision surpasses recall,\\nindicating the generated claims are high quality.\\n5.3.4 Hyper-parameter Analysis\\nFigure 4: The variation of Balanced Accuracy with the\\nchange of kon the FELM dataset.\\nDue to the space limit, we only show the hyper-\\nparameter analysis of kon FELM datasets in Fig-\\nures 4. We have the following observations: 1) Ex-\\nperimental results on both FELM and WikiBio\\nGPT-3 datasets demonstrate that more documents\\n(larger value of k) do not always provide better\\nperformance. The reason can be more documents\\nlead to longer input, attention becomes dispersed,\\nand simultaneously, the relevance of the retrieved\\ndocuments decreases. 2) Less or no retrieved docu-\\nments also contribute to worse performance, which\\nindicates the usefulness of the retrieval augmen-\\ntation module in CEG. 3) The best performance\\nachieved when knear 5 (Top-4 for FELM dataset\\nand Top-6 for WikiBio GPT-3 dataset).\\n6 Conclusion\\nIn this study, we propose a novel post-hoc citation-\\nenhanced generation framework to reduce hallu-\\ncinations in LLMs, which involves retrieval aug-\\nmentation and natural language inference tech-\\nnologies. Different from previous hallucination\\nstudies, our framework is post-hoc and flexible,\\nwhich can be applied to distinct LLMs without addi-\\ntional training or annotations, making it hold signif-\\nicant practical implications. Experiments on three\\nhallucination-related benchmarks and our datasets\\ndemonstrate that our CEG framework achieves\\nstate-of-the-art performance in hallucination detec-\\ntion and regeneration. Further analyses show the\\neffectiveness of our proposed modules and adopted\\nmodels. In the future, we plan to further expand\\nthe corpus to support more scenarios.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 7}), Document(page_content='Limitations\\nOur study has several limitations: 1) Restricted\\nretriever and corpus: In our experiments, we do\\nnot employ a fine-tuned specific retriever for post-\\nhoc methods, and our method utilizes only the\\nWikipedia corpus, limiting the applicability of our\\nframework to general knowledge-based question-\\nanswering scenarios but only demonstrate the effec-\\ntiveness of our model. 2) Our experiments are con-\\nducted on existing benchmarks and manual annota-\\ntions, where we do not propose new QA datasets\\nfor the verification of regeneration performance. 3)\\nThe adopted NLI method in the citation genera-\\ntion module inherently relies on the LLM’s world\\nknowledge. More powerful NLI methods can con-\\ntribute to better performance.\\nReferences\\nSteven Bird, Ewan Klein, and Edward Loper. 2009. Nat-\\nural language processing with Python: analyzing text\\nwith the natural language toolkit . \" O’Reilly Media,\\nInc.\".\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\\ncan, George Bm Van Den Driessche, Jean-Baptiste\\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\\nImproving language models by retrieving from tril-\\nlions of tokens. In International conference on ma-\\nchine learning , pages 2206–2240. PMLR.\\nShiqi Chen, Yiran Zhao, Jinghan Zhang, I Chern, Siyang\\nGao, Pengfei Liu, Junxian He, et al. 2023. Felm:\\nBenchmarking factuality evaluation of large language\\nmodels. arXiv preprint arXiv:2310.00741 .\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\\nSimCSE: Simple contrastive learning of sentence em-\\nbeddings. In Proceedings of the 2021 Conference\\non Empirical Methods in Natural Language Process-\\ning, pages 6894–6910, Online and Punta Cana, Do-\\nminican Republic. Association for Computational\\nLinguistics.\\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\\n2023a. Enabling large language models to generate\\ntext with citations. In Proceedings of the 2023 Con-\\nference on Empirical Methods in Natural Language\\nProcessing , pages 6465–6488, Singapore. Associa-\\ntion for Computational Linguistics.\\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\\nWang. 2023b. Retrieval-augmented generation for\\nlarge language models: A survey. arXiv preprint\\narXiv:2312.10997 .\\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\\nTaitelbaum, Doron Kukliansy, Vered Cohen, ThomasScialom, Idan Szpektor, Avinatan Hassidim, and\\nYossi Matias. 2022. TRUE: Re-evaluating factual\\nconsistency evaluation. In Proceedings of the 2022\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies , pages 3905–3920, Seattle,\\nUnited States. Association for Computational Lin-\\nguistics.\\nJie Huang and Kevin Chen-Chuan Chang. 2023. Ci-\\ntation: A key to building responsible and ac-\\ncountable large language models. arXiv preprint\\narXiv:2307.02185 .\\nGautier Izacard and Edouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open do-\\nmain question answering. In Proceedings of the 16th\\nConference of the European Chapter of the Associ-\\nation for Computational Linguistics: Main Volume ,\\npages 874–880, Online. Association for Computa-\\ntional Linguistics.\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\\nEdouard Grave. 2022. Few-shot learning with re-\\ntrieval augmented language models. arXiv preprint\\narXiv:2208.03299 .\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\\nMadotto, and Pascale Fung. 2023. Survey of halluci-\\nnation in natural language generation. ACM Comput-\\ning Surveys , 55(12):1–38.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\\nguage models are zero-shot reasoners. Advances in\\nneural information processing systems , 35:22199–\\n22213.\\nRémi Lebret, David Grangier, and Michael Auli. 2016.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 8}), Document(page_content='guage models are zero-shot reasoners. Advances in\\nneural information processing systems , 35:22199–\\n22213.\\nRémi Lebret, David Grangier, and Michael Auli. 2016.\\nNeural text generation from structured data with ap-\\nplication to the biography domain. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Natu-\\nral Language Processing , pages 1203–1213, Austin,\\nTexas. Association for Computational Linguistics.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. 2020. Retrieval-augmented generation\\nfor knowledge-intensive nlp tasks. Advances in Neu-\\nral Information Processing Systems , 33:9459–9474.\\nJunyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and\\nJi-Rong Wen. 2023a. HaluEval: A large-scale hal-\\nlucination evaluation benchmark for large language\\nmodels. In Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing ,\\npages 6449–6464, Singapore. Association for Com-\\nputational Linguistics.\\nKenneth Li, Oam Patel, Fernanda Viégas, Hanspeter\\nPfister, and Martin Wattenberg. 2023b. Inference-\\ntime intervention: Eliciting truthful answers from a\\nlanguage model. arXiv preprint arXiv:2306.03341 .', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 8}), Document(page_content='Potsawee Manakul, Adian Liusie, and Mark Gales. 2023.\\nSelfCheckGPT: Zero-resource black-box hallucina-\\ntion detection for generative large language models.\\nInProceedings of the 2023 Conference on Empiri-\\ncal Methods in Natural Language Processing , pages\\n9004–9017, Singapore. Association for Computa-\\ntional Linguistics.\\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\\nJohn Aslanides, Francis Song, Martin Chadwick,\\nMia Glaese, Susannah Young, Lucy Campbell-\\nGillingham, Geoffrey Irving, et al. 2022. Teaching\\nlanguage models to support answers with verified\\nquotes. arXiv preprint arXiv:2203.11147 .\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow instruc-\\ntions with human feedback. Advances in Neural\\nInformation Processing Systems , 35:27730–27744.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2020. Exploring the limits\\nof transfer learning with a unified text-to-text trans-\\nformer. The Journal of Machine Learning Research ,\\n21(1):5485–5551.\\nNils Reimers and Iryna Gurevych. 2019. Sentence-\\nBERT: Sentence embeddings using Siamese BERT-\\nnetworks. In Proceedings of the 2019 Conference on\\nEmpirical Methods in Natural Language Processing\\nand the 9th International Joint Conference on Natu-\\nral Language Processing (EMNLP-IJCNLP) , pages\\n3982–3992, Hong Kong, China. Association for Com-\\nputational Linguistics.\\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\\nEllery Wulczyn, Le Hou, Kevin Clark, Stephen\\nPfohl, Heather Cole-Lewis, Darlene Neal, et al.\\n2023. Towards expert-level medical question an-\\nswering with large language models. arXiv preprint\\narXiv:2305.09617 .\\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\\nGalactica: A large language model for science. arXiv\\npreprint arXiv:2211.09085 .\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, et al. 2023. Llama 2: Open founda-\\ntion and fine-tuned chat models. arXiv preprint\\narXiv:2307.09288 .\\nGuan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li,\\nSen Song, and Yang Liu. 2023a. Openchat: Advanc-\\ning open-source language models with mixed-quality\\ndata. arXiv preprint arXiv:2309.11235 .\\nXiaohua Wang, Yuliang Yan, Longtao Huang, Xiaoqing\\nZheng, and Xuanjing Huang. 2023b. Hallucinationdetection for generative large language models by\\nBayesian sequential estimation. In Proceedings of\\nthe 2023 Conference on Empirical Methods in Natu-\\nral Language Processing , pages 15361–15371, Sin-\\ngapore. Association for Computational Linguistics.\\nYile Wang, Peng Li, Maosong Sun, and Yang Liu.\\n2023c. Self-knowledge guided retrieval augmenta-\\ntion for large language models. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2023 , pages 10303–10315, Singapore. Association\\nfor Computational Linguistics.\\nYuxia Wang, Shimin Tao, Ning Xie, Hao Yang, Tim-\\nothy Baldwin, and Karin Verspoor. 2023d. Collec-\\ntive human opinions in semantic textual similarity.\\nTransactions of the Association for Computational\\nLinguistics , 11:997–1013.\\nWenhao Yu. 2022. Retrieval-augmented generation\\nacross heterogeneous knowledge. In Proceedings\\nof the 2022 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies: Student Research\\nWorkshop , pages 52–58, Hybrid: Seattle, Washington\\n+ Online. Association for Computational Linguistics.\\nShengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li,\\nChenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao,\\nSong Yun, Wei Lin, et al. 2023. Disc-lawllm: Fine-\\ntuning large language models for intelligent legal\\nservices. arXiv preprint arXiv:2309.11325 .\\nTianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng,', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 9}), Document(page_content='Song Yun, Wei Lin, et al. 2023. Disc-lawllm: Fine-\\ntuning large language models for intelligent legal\\nservices. arXiv preprint arXiv:2309.11325 .\\nTianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng,\\nYue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing\\nWang, and Luoyi Fu. 2023a. Enhancing uncertainty-\\nbased hallucination detection with stronger focus.\\nInProceedings of the 2023 Conference on Empiri-\\ncal Methods in Natural Language Processing , pages\\n915–932, Singapore. Association for Computational\\nLinguistics.\\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,\\nTingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,\\nYulong Chen, et al. 2023b. Siren’s song in the ai\\nocean: a survey on hallucination in large language\\nmodels. arXiv preprint arXiv:2309.01219 .\\nRuochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei\\nQin, and Lidong Bing. 2023. Verify-and-edit: A\\nknowledge-enhanced chain-of-thought framework.\\nInProceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers) , pages 5823–5840, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\\nZhuohan Li, Dacheng Li, Eric Xing, et al. 2023.\\nJudging llm-as-a-judge with mt-bench and chatbot\\narena. arXiv preprint arXiv:2306.05685 .\\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\\nLili Yu, et al. 2023. Lima: Less is more for alignment.\\narXiv preprint arXiv:2305.11206 .', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 9}), Document(page_content='Appendix\\nA Prompts Used in Our Experiments\\nThe prompt used in the evaluation of CEG and w/o\\nthreshold variants of CEG on the FELM dataset, as\\nwell as in the citation generation experiments on\\nthe HaluEval dataset, is presented in Table 11. The\\nprompt for w/o retrieval augmentation variants of\\nCEG in the FELM dataset evaluation is presented\\nin Table 12.\\nThe prompt to regenerate a new response on the\\nHaluEval dataset is presented in Table 13.\\nContext :\\n{Retrieved passages }\\nSentence :\\n{Sentence to be verified }\\nIs the sentence supported by the context above?\\nAnswer Yes or No:\\nThe NLI prompt in experiments on WikiBio\\ndataset is presented above.\\npremise : {Passage retrieved }hypothesis : {Hy-\\npothesis to be verified }\\nThe NLI prompt employed by True-9B in experi-\\nments on WikiRetr datasets is presented above. We\\nutilize the same prompt6as in Gao et al., 2023a.\\nThe agreement rates with human annotaters of True-\\n9B in Table 5 and the precision of True-9B in Ta-\\nble 6 are based on this prompt.\\nPremise : {Passage retrieved }\\nHypothesis : {Hypothesis to be verified }\\nTask : Determine the logical relationship between\\npremise and hypothesis.\\nOutput format : If you believe the relationship is\\nEntailment, output Entailment; for Contradiction,\\noutput Contradiction; for Neutral, output Neutral.\\nThe NLI prompt employed by GPT series mod-\\nels in experiments on WikiRetr datasets is pre-\\nsented above. The agreement rates with human\\nannotaters of GPT models in Table 5 and the pre-\\ncision of GPT models in Table 6 are based on this\\nprompt.\\n6You can find the original prompt at\\nhttps://github.com/princeton-nlp/ALCE/blob/main/eval.pyB Detailed Information About Adopted\\nDatasets\\nMore Statistics of WikiBio GPT-3 dataset are\\nshown in Table 7, and statistics of WorldKnowl-\\nedge subset in FELM are summarized in Table 8.\\n#Passages #Sentences #Factual #Nonfactual\\n238 1,908 516 1,392\\nTable 7: Statistics of WikiBio GPT-3 dataset.\\n#Response Error rate (%) agreement rate (%)\\nStatistics 184 46.2 81.5\\n#Segment #Factual #Nonfactual\\nStatistics 532 385 147\\nTable 8: Statistics of WorldKnowledge subset in FELM.\\nThe “Error rate” indicates the proportion of responses\\ncontaining factual errors. The agreement rate is agree-\\nment rate between the two annotators during the annota-\\ntion process.\\nIn Table 9, we provide an example of HaluEval\\ndataset.\\n#Knowledge#: The nine-mile byway starts south of\\nMorehead, Kentucky and can be accessed by U.S. High-\\nway 60. Morehead is a home rule-class city located\\nalong US 60 (the historic Midland Trail) and Interstate\\n64 in Rowan County, Kentucky, in the United States.\\n#Question#: What U.S Highway gives access to Zilpo\\nRoad, and is also known as Midland Trail?\\n#Right Answer#: U.S. Highway 60\\n#Hallucinated Answer#: U.S. Highway 70\\nTable 9: An example of HaluEval dataset.\\nThe utilized modules in different datasets are\\nsummarized in Table 10.\\nDatasets Retrieval Citation Regeneration\\nWikiBio GPT-3 ✓ ✓ -\\nFELM ✓ ✓ -\\nHaluEval ✓ ✓ ✓\\nWikiRetr ✓ ✓ -\\nTable 10: Modules of our framework used in four exper-\\nimental datasets. \" ✓\" indicates this module is adopted\\nin the experiment, and \"-\" indicates not used.\\nC Case Studies on HaluEval Dataset\\nThere are two cases in evaluation on HaluEval\\ndataset, presented in Table 14 and Table 15, re-\\nspectively.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 10}), Document(page_content='Instruction : I will show you a question, a response segment of this question, and a reference doc. Your\\ntask is to assess whether the given response segment contains factual errors or not with the help of the\\nreference doc. If you believe the segment contains factual errors, your answer should be “Nonfactual”; if\\nthere is no factual error in this segment, your answer should be “Factual”. This means that the answer\\nis “Nonfactual” only if there are some factual errors in the response segment. When there is no factual\\njudgment in the response segment or the response segment has no clear meaning, your answer should be\\n“Factual”. Think it step by step. Give your reasoning first and then output the Answer.\\nQuestion :\\n{Question }\\nResponse segment :\\n{Response segment to be verified }\\nReference doc :\\n{Top-k docs concatenated with newline characters and numbers. }\\nOutput:\\nTable 11: Prompt used for the evaluation of CEG and w/o threshold variants of CEG on the FELM dataset, as well\\nas in the NLI experiments on the HaluEval dataset.\\nInstruction : I will show you a question, a response segment of this question. Your task is to assess\\nwhether the given response segment contains factual errors or not. If you believe the segment contains\\nfactual errors, your answer should be “Nonfactual”; if there is no factual error in this segment, your answer\\nshould be “Factual”. This means that the answer is “Nonfactual” only if there are some factual errors in\\nthe response segment.When there is no factual judgment in the response segment or the response segment\\nhas no clear meaning, your answer should be “Factual”. Think it step by step. Give your reasoning first\\nand then output the Answer.\\nQuestion :\\n{Question }\\nResponse segment :\\n{Response segment to be verified }\\nReference doc :\\nOutput :\\nTable 12: Prompt used for the evaluation of w/o RA variant of our method on the FELM dataset.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 11}), Document(page_content='User :\\nInstruction: I want you act as an answer judge. Given a question, two answers, your objective is to select\\nthe best and correct answer without hallucination and nonfactual information. You should try your best to\\nselect the best and correct answer. If the two answers are the same, you can randomly choose one. If both\\nanswers are incorrect, choose the better one. You MUST select an answer from the provided two answers.\\nThink it step by step. Give your reasoning first and then output your choice. Output in the following\\nformat, “#Reasoning#:Your Reasoning\\\\n#Choice#:Your Choice”. Your choice should be “Answer 1” or\\n“Answer 2”.\\n#Question#: { Question }\\n#Answer 1#: { Right_answer }\\n#Answer 2#: { Hallucinated_answer }\\nChatbot :\\nReasoning and Answer\\nUser :\\nDocuments:\\n{Docs retrieved for the nonfactual segments }\\nIn your previous response, there are factual inaccuracies in the following passages:\\n{Nonfactual segments }\\nPlease re-answer the previous question with the help of documents. If the documents are unrelated to the\\nissue, please ignore them. Output in the following format, “#Reasoning#:Your Reasoning\\\\n#Choice#:Your\\nChoice”. Your choice should be “Answer 1” or “Answer 2”.\\nTable 13: Prompt used for regeneration of our method on the HaluEval dataset.\\n#Question# : What film was written by the newspaperman born in 1880?\\n#Right_Answer# : The Lemon Drop Kid\\n#Hallucinated_Answer# : The Great American Novel written by Damon Runyon.\\n#Reasoning1# : Damon Runyon was a newspaperman born in 1880 and is known for writing The Great\\nAmerican Novel.\\n#Choice1# : Answer 2: The Great American Novel written by Damon Runyon.\\n#Segments# : 1.Damon Runyon was a newspaperman born in 1880 and is known for writing The Great\\nAmerican Novel.\\n#NLI Results# : [0]\\n#Doc Retrieved for Segment1# : Damon Runyon Alfred Damon Runyon (October 4, 1880 – December\\n10, 1946) was an American newspaperman and short-story writer. He was best known for his short stories\\ncelebrating the world of Broadway in New York City that grew out of the Prohibition era. To New Yorkers\\nof his generation, a “Damon Runyon character” evoked a distinctive social type from the Brooklyn or\\nMidtown demi-monde. The adjective “Runyonesque” refers to this type of character as well as to the\\ntype of situations and dialog that Runyon depicted. He spun humorous and sentimental tales of gamblers,\\nhustlers, actors, and gangsters, few of\\n#Reasoning2# : Damon Runyon was known for his short stories celebrating the world of Broadway in\\nNew York City, not for writing The Great American Novel.\\n#Choice2# : Answer 1: The Lemon Drop Kid\\nTable 14: HaluEval Example 1. The redpart is the content generated for the first time and annotated with citations.\\nThe green part is the content of the regeneration.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 12}), Document(page_content='#Question# : Which low-cost airline is headquartered in Sydney?\\n#Right_Answer# : Jetstar Airways\\n#Hallucinated_Answer# : Tiger Airways is headquartered in Sydney.\\n#Reasoning1# : Jetstar Airways is a well-known low-cost airline and is headquartered in Melbourne, not\\nSydney. Tiger Airways used to be headquartered in Singapore, not Sydney.\\n#Choice1# : Answer 2: Tiger Airways is not headquartered in Sydney.\\n#Segments# : 1. Jetstar Airways is a well-known low-cost airline and is headquartered in Melbourne, not\\nSydney. 2. Tiger Airways used to be headquartered in Singapore, not Sydney\\n#NLI Results# : [0,0]\\n#Doc Retrieved for Segment1# : Jetstar Airways Jetstar Airways Pty Ltd, trading as Jetstar, is an\\nAustralian low-cost airline (self-described as \\\\\"value based\\\\\") headquartered in Melbourne. It is a wholly\\nowned subsidiary of Qantas, created in response to the threat posed by low-cost airline Virgin Blue. Jetstar\\nis part of Qantas’ two brand strategy of having Qantas Airways for the premium full-service market and\\nJetstar for the low-cost market. Jetstar carries 8.5% of all passengers travelling in and out of Australia.\\nThe airline operates an extensive domestic network as well as regional and international services from its\\nmain base at Melbourne Airport, using a mixed\\n#Doc Retrieved for Segment2# : Australia Asia Airlines Australia Asia Airlines\\\\“Àoyà Hángk ¯ong\\nG¯ongs ¯ı\\\\”) was a wholly owned subsidiary of Qantas set up to operate services between Australia and\\nTaiwan (Republic of China). The subsidiary was established due to the People’s Republic of China\\nobjection to national carriers of countries with which it had diplomatic relations flying to a territory that it\\nregarded as a breakaway province. The airline operated two Boeing 747SPs and a Boeing 767 aircraft\\nseconded from the Qantas fleet, repainted in a modified livery, which did not display the Flag of Australia,\\nor the kangaroo logo, which was replaced by\\n#Reasoning2# : According to the provided document, Jetstar Airways is headquartered in Melbourne, not\\nSydney. There is no mention of Tiger Airways being headquartered in Sydney.\\n#Choice2# : Answer 1: Jetstar Airways\\nTable 15: HaluEval Example 2. The redpart is the content generated for the first time and annotated with citations.\\nThe green part is the content of the regeneration.', metadata={'source': '/Users/shanghongsim/GRoVER/ml/data/citation enhanced generation for llm chatbots.pdf', 'page': 13})]\n"
          ]
        }
      ],
      "source": [
        "# load pdf and chunk it\n",
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "documents = []\n",
        "pdf_dir = os.path.join(os.getcwd(),\"data\")\n",
        "for filename in os.listdir(pdf_dir):\n",
        "    if filename.endswith('.pdf'):\n",
        "      loader = PyPDFLoader(os.path.join(pdf_dir, filename))\n",
        "      pages = loader.load_and_split()\n",
        "      documents.extend(pages)\n",
        "print(\"Number of document chunks:\", documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gcB7AawKyNPM"
      },
      "outputs": [],
      "source": [
        "# create vector index\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "vector = FAISS.from_documents(documents, OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HaitmuaEyOf6"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\"\"\")\n",
        "\n",
        "document_chain = create_stuff_documents_chain(ChatOpenAI(), prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WiAy-OqlyRnc"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retriever = vector.as_retriever()\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q9pp_LyyZvJ",
        "outputId": "60ef7115-67bb-4573-9e23-e827ebacf786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Automatic LLMs’ Citation Evaluation (ALCE) is a benchmark proposed in the '\n",
            " 'provided context that collects a diverse set of questions and retrieval '\n",
            " 'corpora and requires building end-to-end systems to retrieve supporting '\n",
            " 'evidence and generate answers with citations. ALCE aims to evaluate the '\n",
            " 'fluency, correctness, and citation quality of text generated by Large '\n",
            " 'Language Models (LLMs) automatically, using metrics that have been '\n",
            " 'demonstrated to have strong correlation with human judgments.')\n"
          ]
        }
      ],
      "source": [
        "question = \"what is Automatic LLMs’ Citation Evaluation?\"\n",
        "response = retrieval_chain.invoke({\"input\": question})\n",
        "pprint.pp(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63L87x79yoEQ",
        "outputId": "9e4555ca-d687-4b22-d4e0-23cb51f766b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1]: alce.pdf, pg 0\n",
            "[2]: attribute firtst then generate.pdf, pg 4\n",
            "[3]: alce.pdf, pg 8\n",
            "[4]: training language models to generate text with citations via fine grained rewards.pdf, pg 0\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "count = 1\n",
        "for doc in response[\"context\"]:\n",
        "  path = f'{doc.metadata[\"source\"]}'\n",
        "  # Regex pattern\n",
        "  pattern = r'[^/]+$'\n",
        "\n",
        "  # Extract the last part after the last slash\n",
        "  filename = re.search(pattern, path)\n",
        "\n",
        "  if filename:\n",
        "    print(f'[{count}]: {filename.group()}, pg {doc.metadata[\"page\"]}')\n",
        "  count+=1\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
